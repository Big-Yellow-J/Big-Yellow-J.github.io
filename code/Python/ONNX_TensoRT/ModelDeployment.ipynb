{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd9a8de6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 模型部署————ONNX以及TensRT使用教程  \n",
    "> 本代码设备基本配置如下：  \n",
    "> **显卡**：TITAN Xp 12G CUDA Version: 13.0  \n",
    "> **torch版本**：2.3.0+cu121  \n",
    "> **系统**： Ubuntu 22.04.3 LTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947d33fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorrt\n",
    "!pip install onnx onnxruntime pycuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aae6e5-398a-460a-862e-9d7c61277978",
   "metadata": {},
   "source": [
    "## 导出ONNX模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6990b0bc-abf6-4a2e-b5eb-1383d1c8b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import onnxruntime as ort\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=torch.jit.TracerWarning)\n",
    "\n",
    " \n",
    "class ImgModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ImgModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        image_features = self.model.get_image_features(pixel_values=pixel_values)\n",
    "        return image_features\n",
    "\n",
    "class TxtModelWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(TxtModelWrapper, self).__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        text_features = self.model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        return text_features\n",
    "\n",
    "def trans_clip_onnx(clip_model_name, image_path, text= [\"a photo of a cat\"]):\n",
    "    # 加载模型\n",
    "    model = CLIPModel.from_pretrained(clip_model_name,\n",
    "                                    use_safetensors=True)\n",
    "    processor = CLIPProcessor.from_pretrained(clip_model_name,\n",
    "                                            use_safetensors=True)\n",
    "    \n",
    "    # 处理输入\n",
    "    image = Image.open(image_path) \n",
    "    inputs = processor(text= text, images=image, return_tensors=\"pt\", padding='max_length')\n",
    "    \n",
    "    # 转换ONNX\n",
    "    img_model = ImgModelWrapper(model)\n",
    "    txt_model = TxtModelWrapper(model)\n",
    "    \n",
    "    torch.onnx.export(img_model,\n",
    "                    (inputs.pixel_values),\n",
    "                    \"clip_img.onnx\",\n",
    "                    opset_version=17,\n",
    "                    do_constant_folding=True,\n",
    "                    input_names=['pixel_values'],\n",
    "                    )\n",
    "    torch.onnx.export(txt_model,\n",
    "                    (inputs.input_ids, inputs.attention_mask),\n",
    "                    \"clip_txt.onnx\",\n",
    "                    opset_version=17,\n",
    "                    do_constant_folding=True,\n",
    "                    input_names=['input_ids', 'attention_mask'],\n",
    "                    dynamic_axes={'input_ids': {0: 'batch', 1: 'seq'}, \n",
    "                                    'attention_mask': {0: 'batch', 1: 'seq'}},\n",
    "                    )\n",
    "\n",
    "def test_model_pt(clip_model_name, image_path):\n",
    "    model = CLIPModel.from_pretrained(clip_model_name, use_safetensors=True)\n",
    "    processor = CLIPProcessor.from_pretrained(clip_model_name, use_safetensors=True)\n",
    "    model.eval()\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    s_pt_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        pt_features = model.get_image_features(pixel_values=inputs.pixel_values)\n",
    "    pt_features = pt_features.cpu().numpy()\n",
    "    print(f\"原始推理使用时间：{time.time()- s_pt_time:.2f} 秒\")\n",
    "\n",
    "    s_onnx_time = time.time()\n",
    "    ort_session = ort.InferenceSession(\"clip_img.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "    ort_inputs = {\"pixel_values\": inputs.pixel_values.cpu().numpy()}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "    print(f\"ONNX 推理使用时间：{time.time()- s_onnx_time:.2f} 秒\")\n",
    "\n",
    "def classify_image_and_compare(clip_model_name, image_path, candidate_labels):\n",
    "    model = CLIPModel.from_pretrained(clip_model_name, use_safetensors=True)\n",
    "    processor = CLIPProcessor.from_pretrained(clip_model_name, use_safetensors=True)\n",
    "    model.eval()\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    inputs = processor(text=candidate_labels, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # ---------------- PyTorch 推理 ----------------\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad():\n",
    "        pt_image_features = model.get_image_features(pixel_values=inputs.pixel_values)   # shape (1, D)\n",
    "        pt_text_features  = model.get_text_features(input_ids=inputs.input_ids, attention_mask=inputs.attention_mask)  # shape (N, D)\n",
    "    pt_time = time.time() - t0\n",
    "\n",
    "    # 转 numpy 并做 L2 归一化\n",
    "    pt_image = pt_image_features.cpu().numpy()\n",
    "    pt_text  = pt_text_features.cpu().numpy()\n",
    "    pt_image = pt_image / np.linalg.norm(pt_image, axis=-1, keepdims=True)\n",
    "    pt_text  = pt_text  / np.linalg.norm(pt_text, axis=-1, keepdims=True)\n",
    "\n",
    "    # 计算相似度（image @ text.T），得到每个候选标签的分数\n",
    "    pt_sim = (pt_image @ pt_text.T).squeeze(0)\n",
    "    pt_best_idx = int(np.argmax(pt_sim))\n",
    "    pt_best_label = candidate_labels[pt_best_idx]\n",
    "    pt_best_score = float(pt_sim[pt_best_idx])\n",
    "\n",
    "    # ---------------- ONNX 推理 ----------------\n",
    "    ort_img_sess = ort.InferenceSession(\"clip_img.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "    ort_txt_sess = ort.InferenceSession(\"clip_txt.onnx\", providers=[\"CPUExecutionProvider\"])\n",
    "\n",
    "    ort_inputs_img = {\"pixel_values\": inputs.pixel_values.cpu().numpy().astype(np.float32)}\n",
    "    ort_inputs_txt = {\n",
    "        \"input_ids\": inputs.input_ids.cpu().numpy().astype(np.int64),\n",
    "        \"attention_mask\": inputs.attention_mask.cpu().numpy().astype(np.int64)\n",
    "    }\n",
    "\n",
    "    t1 = time.time()\n",
    "    ort_img_out = ort_img_sess.run(None, ort_inputs_img)\n",
    "    ort_txt_out = ort_txt_sess.run(None, ort_inputs_txt)\n",
    "    onnx_time = time.time() - t1\n",
    "\n",
    "    onnx_image = ort_img_out[0]\n",
    "    onnx_text  = ort_txt_out[0] \n",
    "    onnx_image = onnx_image / np.linalg.norm(onnx_image, axis=-1, keepdims=True)\n",
    "    onnx_text  = onnx_text  / np.linalg.norm(onnx_text, axis=-1, keepdims=True)\n",
    "\n",
    "    onnx_sim = (onnx_image @ onnx_text.T).squeeze(0)\n",
    "    onnx_best_idx = int(np.argmax(onnx_sim))\n",
    "    onnx_best_label = candidate_labels[onnx_best_idx]\n",
    "    onnx_best_score = float(onnx_sim[onnx_best_idx])\n",
    "\n",
    "    # ---------------- 输出对比 ----------------\n",
    "    print(\"=== PyTorch 结果 ===\")\n",
    "    print(f\"预测标签: {pt_best_label}\")\n",
    "    print(f\"相似度(score): {pt_best_score:.6f}\")\n",
    "    print(f\"推理时间: {pt_time:.6f} 秒 (包含 image & text 推理)\")\n",
    "\n",
    "    print(\"\\n=== ONNX 结果 ===\")\n",
    "    print(f\"预测标签: {onnx_best_label}\")\n",
    "    print(f\"相似度(score): {onnx_best_score:.6f}\")\n",
    "    print(f\"推理时间: {onnx_time:.6f} 秒 (包含 image & text 推理)\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    clip_model = \"openai/clip-vit-base-patch32\"\n",
    "    image_path = \"./test_2.jpg\"\n",
    "\n",
    "    # trans_clip_onnx(clip_model, image_path)\n",
    "    candidate_labels = [\n",
    "        \"a photo of a cat\",\n",
    "        \"a photo of a dog\",\n",
    "        \"a photo of a car\",\n",
    "        \"a photo of a building\",\n",
    "        \"a photo of a person\"\n",
    "    ]\n",
    "    classify_image_and_compare(clip_model, image_path, candidate_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2763db49",
   "metadata": {},
   "source": [
    "## TensoRT推理  \n",
    "### Linux下安装TensoRT\n",
    "在安装TensorRT前，**首先确保已经安装了CUDA和cuDNN**。**安装方式一**：`pip install tensorrt`。**安装方式二**：  \n",
    "**首先**，下载`TensoRT`。直接去访问[网站](https://developer.nvidia.com/tensorrt#)然后直接去下载`TensorRT`（Linux直接去获取window上的下载地址然后直接到终端 `wget -c 地址`，就不需要再去上传到服务器）\n",
    "> 比如说下载链接：`https://developer.download.nvidia.com/compute/machine-learning/tensorrt/10.13.3/tars/TensorRT-10.13.3.9.Linux.x86_64-gnu.cuda-12.9.tar.gz?t=eyJscyI6ImdzZW8iLCJsc2QiOiJodHRwczovL3d3dy5nb29nbGUuY29tLyJ9` 那么就可以直接：`wget -c \"https://developer.download.nvidia.com/compute/machine-learning/tensorrt/10.13.3/tars/TensorRT-10.13.3.9.Linux.x86_64-gnu.cuda-12.9.tar.gz?t=eyJscyI6ImdzZW8iLCJsc2QiOiJodHRwczovL3d3dy5nb29nbGUuY29tLyJ9\"      -O TensorRT-10.13.3.9.Linux.x86_64-gnu.cuda-12.9.tar.gz`\n",
    "\n",
    "![](https://s2.loli.net/2025/10/16/dATZbpzSWYl1wyg.png)\n",
    "**安装**，安装`TensoRT`  \n",
    "*第一步、解压安装包*：\n",
    "```bash\n",
    "tar -xzvf TensorRT-10.13.3.9.Linux.x86_64-gnu.cuda-12.9.tar.gz\n",
    "cd TensorRT-10.13.3.9\n",
    "```\n",
    "*第二步、添加环境变量*  \n",
    "```bash\n",
    "vim ~/.bashrc\n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/hy-tmp/TensorRT-10.13.3.9/lib\n",
    "export PATH=$PATH:/hy-tmp/TensorRT-10.13.3.9/bin\n",
    "source ~/.bashrc\n",
    "```  \n",
    "*第三步、安装python包*  \n",
    "```bash\n",
    "python --version # Python 3.11.8\n",
    "cd python\n",
    "pip install tensorrt-10.13.3.9-cp311-none-linux_x86_64.whl \n",
    "```\n",
    "> 在python中一般都有 `tensorrt-8.x.x.x-cp3x-none-linux_x86_64.wh` 其中 `cp`后面表示的是 python版本，tensorrt后面表示的是tensorrt版本  \n",
    "\n",
    "### TensoRT推理\n",
    "TensorRT有自己的一套推理流程，我们在使用PyTorch或TensorFlow导出模型权重后，需要进一步转换。TensorRT最终需要的是一个TensorRT Engine，这个Engine是由TensorRT的Builder构建，而Builder需要一个TensorRT Network。TensorRT Network是由TensorRT Parser解析的ONNX模型构建的。\n",
    "![](https://s2.loli.net/2025/10/17/VGwpv8hWS3XIfem.png)\n",
    "**第一步、转换为TensoRT Engine**。需要完成操作：1、转换并填充Network对象；2、编写构建配置对象（Config）；3、编写优化配置对象（Optimization Profile）整个过程参考下面的`build_engine_from_onnx`函数，对于其中更加具体的细节描述如下：\n",
    "> 对于该函数主要3个核心内容：1、创建我的3部分核心内容：`builder`、`config`、`parser`。构建、配置、解析。\n",
    "\n",
    "**1、使用显式批处理**：显式批处理（explicit batch）和隐式批处理（implicit batch）。假设网络的输入张量形状为`(n, c, h, w)`，在隐式批处理模式下，网络只需要指定输入形状为`(c, h, w)`，批次维度是隐式的，并且在运行时可动态指定；在显式批处理模式下，批量维度需要网络显示定义，甚至可以不用批量维度，对于动态批量大小的需求，可以使用Optimization Profile配置动态形状\n",
    ">`network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))`  \n",
    "  \n",
    "**2、动态形状**：直接通过`set_shape`去设定尺寸。一般而言其参数为：输入节点名称，可接受的最小输入尺寸，最优的输入尺寸，可接受的最大输入尺寸\n",
    "\n",
    "\n",
    "**第二步、使用TensoRT进行推理**。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
