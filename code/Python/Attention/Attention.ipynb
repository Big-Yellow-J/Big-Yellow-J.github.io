{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3db3079",
   "metadata": {},
   "source": [
    "# MHA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e931e9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状: torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: float, num_heads: int, dropout: float= 0.2, bias: bool=False):\n",
    "        super().__init__()\n",
    "        assert embed_dim% num_heads== 0\n",
    "        self.head_dim = embed_dim// num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.m_attn = nn.Linear(embed_dim, 3* embed_dim,\n",
    "                                bias= bias)\n",
    "        self.m_proj = nn.Linear(embed_dim, embed_dim, bias= bias)\n",
    "        \n",
    "        self.atten_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "\n",
    "    def forward(self, x: torch.tensor, attention_mask: torch.tensor=None):\n",
    "        B, T, C = x.shape\n",
    "        qkv = self.m_attn(x)\n",
    "        qkv = qkv.reshape(B, T, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        if attention_mask is not None:\n",
    "            # Expand to shape: (B, 1, 1, T) to broadcast\n",
    "            attention_mask = attention_mask[:, None, None, :]  # (B, 1, 1, T)\n",
    "            # Convert mask to float with -inf where masked\n",
    "            attention_mask = attention_mask.masked_fill(attention_mask == 0, float(\"-inf\"))\n",
    "            attention_mask = attention_mask.masked_fill(attention_mask == 1, 0.0)\n",
    "        if self.flash:\n",
    "            try:\n",
    "                from flash_attn import flash_attn_func\n",
    "                y = flash_attn_func(q, k, v, \n",
    "                                    dropout_p= self.dropout if self.training else 0,\n",
    "                                    causal= True)\n",
    "            except Exception:\n",
    "                y = F.scaled_dot_product_attention(q, k, v, attn_mask=attention_mask,\n",
    "                                                   dropout_p= self.dropout if self.training else 0,\n",
    "                                                   is_causal= False)\n",
    "        else:\n",
    "            att = (q@ k.transpose(-2, -1))* (1/ torch.sqrt(k.size(-1)))\n",
    "            if attention_mask is not None:\n",
    "                att = att.masked_fill(attention_mask == 0, float('-inf'))\n",
    "            att = self.atten_dropout(F.softmax(att, dim=-1))\n",
    "            y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.m_proj(y))\n",
    "        return y\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    B, T, C = 2, 5, 16\n",
    "    num_heads = 4\n",
    "    x = torch.randn(B, T, C).to(device)\n",
    "    attention_mask = torch.tensor([\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 0, 0]\n",
    "    ], dtype=torch.float32).to(device)  # shape: (B, T)\n",
    "    mha = MultiHeadAttention(embed_dim=C, num_heads=num_heads, dropout=0.1)\n",
    "    mha = mha.to(device)\n",
    "    output = mha(x, attention_mask)\n",
    "    print(\"输出形状:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85d34b9",
   "metadata": {},
   "source": [
    "# MQA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3ab477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状: torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiQueryAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, dropout: float = 0.2, bias: bool = False):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # MQA: 单独定义 Q 的线性层，K 和 V 共享单头\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)  # 多头 Q\n",
    "        self.k_proj = nn.Linear(embed_dim, self.head_dim, bias=bias)  # 单头 K\n",
    "        self.v_proj = nn.Linear(embed_dim, self.head_dim, bias=bias)  # 单头 V\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.atten_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "\n",
    "    def forward(self, x: torch.Tensor, attention_mask: torch.Tensor = None):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # 计算 Q, K, V\n",
    "        q = self.q_proj(x)  # (B, T, embed_dim)\n",
    "        k = self.k_proj(x)  # (B, T, head_dim)\n",
    "        v = self.v_proj(x)  # (B, T, head_dim)\n",
    "\n",
    "        # 重塑 Q 为多头形式，K 和 V 保持单头\n",
    "        q = q.reshape(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # (B, num_heads, T, head_dim)\n",
    "        k = k.unsqueeze(1)  # (B, 1, T, head_dim)\n",
    "        v = v.unsqueeze(1)  # (B, 1, T, head_dim)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            # 扩展为 (B, 1, 1, T) 以广播\n",
    "            attention_mask = attention_mask[:, None, None, :]  # (B, 1, 1, T)\n",
    "            attention_mask = attention_mask.masked_fill(attention_mask == 0, float(\"-inf\"))\n",
    "            attention_mask = attention_mask.masked_fill(attention_mask == 1, 0.0)\n",
    "\n",
    "        if self.flash:\n",
    "            try:\n",
    "                from flash_attn import flash_attn_func\n",
    "                y = flash_attn_func(q, k, v, \n",
    "                                  dropout_p=self.dropout if self.training else 0,\n",
    "                                  causal=True)\n",
    "            except Exception:\n",
    "                y = F.scaled_dot_product_attention(q, k, v, attn_mask=attention_mask,\n",
    "                                                 dropout_p=self.dropout if self.training else 0,\n",
    "                                                 is_causal=False)\n",
    "        else:\n",
    "            # 手动计算注意力\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32)))\n",
    "            if attention_mask is not None:\n",
    "                att = att + attention_mask  # 直接加，因为 mask 已转为 -inf/0\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.atten_dropout(att)\n",
    "            y = att @ v  # (B, num_heads, T, head_dim)\n",
    "\n",
    "        # 重塑输出并投影\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, self.embed_dim)  # (B, T, embed_dim)\n",
    "        y = self.resid_dropout(self.out_proj(y))\n",
    "        return y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    B, T, C = 2, 5, 16\n",
    "    num_heads = 4\n",
    "    x = torch.randn(B, T, C).to(device)\n",
    "    attention_mask = torch.tensor([\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 0, 0]\n",
    "    ], dtype=torch.float32).to(device)  # shape: (B, T)\n",
    "    mqa = MultiQueryAttention(embed_dim=C, num_heads=num_heads, dropout=0.1)\n",
    "    mqa = mqa.to(device)\n",
    "    output = mqa(x, attention_mask)\n",
    "    print(\"输出形状:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf82e69",
   "metadata": {},
   "source": [
    "# GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21cbbd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输出形状: torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, embed_dim: int, num_heads: int, num_kv_heads: int, dropout: float = 0.1, bias: bool = False):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        assert num_heads % num_kv_heads == 0\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_kv_heads = num_kv_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.group_size = num_heads // num_kv_heads\n",
    "\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)                                # (B, T, H * D)\n",
    "        self.kv_proj = nn.Linear(embed_dim, 2 * self.head_dim * num_kv_heads, bias=bias)         # (B, T, 2 * h_kv * D)\n",
    "\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        self.atten_dropout = nn.Dropout(dropout)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        self.dropout = dropout\n",
    "        self.flash = hasattr(F, 'scaled_dot_product_attention')\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        q = self.q_proj(x).reshape(B, T, self.num_heads, self.head_dim).permute(0, 2, 1, 3)  # (B, nh, T, hd)\n",
    "        kv = self.kv_proj(x).reshape(B, T, self.num_kv_heads, 2, self.head_dim).permute(3, 0, 2, 1, 4)\n",
    "        k, v = kv[0], kv[1]  # (B, n_kv, T, hd)\n",
    "\n",
    "        # 将 KV 扩展为每组 query 使用\n",
    "        k = k.repeat_interleave(self.group_size, dim=1)  # (B, nh, T, hd)\n",
    "        v = v.repeat_interleave(self.group_size, dim=1)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = attention_mask[:, None, None, :]  # (B, 1, 1, T)\n",
    "            attention_mask = attention_mask.masked_fill(attention_mask == 0, float(\"-inf\"))\n",
    "            attention_mask = attention_mask.masked_fill(attention_mask == 1, 0.0)\n",
    "\n",
    "        if self.flash:\n",
    "            try:\n",
    "                from flash_attn import flash_attn_func\n",
    "                y = flash_attn_func(q, k, v, dropout_p=self.dropout if self.training else 0.0, causal=True)\n",
    "            except Exception:\n",
    "                y = F.scaled_dot_product_attention(q, k, v, attn_mask=attention_mask,\n",
    "                                                   dropout_p=self.dropout if self.training else 0.0,\n",
    "                                                   is_causal=False)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "            if attention_mask is not None:\n",
    "                att = att.masked_fill(attention_mask == float('-inf'), float('-inf'))\n",
    "            att = self.atten_dropout(F.softmax(att, dim=-1))\n",
    "            y = att @ v\n",
    "\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.out_proj(y))\n",
    "        return y\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    B, T, C = 2, 5, 16\n",
    "    num_heads = 4\n",
    "    num_kv_heads = 2\n",
    "    x = torch.randn(B, T, C).to(device)\n",
    "    attention_mask = torch.tensor([\n",
    "        [1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 0, 0]\n",
    "    ], dtype=torch.float32).to(device)  # shape: (B, T)\n",
    "    mha = GroupedQueryAttention(embed_dim=C, num_heads=num_heads, num_kv_heads= 2, dropout=0.1)\n",
    "    mha = mha.to(device)\n",
    "    output = mha(x, attention_mask)\n",
    "    print(\"输出形状:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a988d08",
   "metadata": {},
   "source": [
    "# Sparse Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb490f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAttention(nn.Module):\n",
    "    def __init__(self, d_model=512, num_heads=8, window_size=256):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads) \n",
    "        self.window_size = window_size\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, N, _ = x.shape\n",
    "        # 创建局部mask（对角线窗口）\n",
    "        local_mask = torch.ones(B, N, N, device=x.device)\n",
    "        for i in range(N):\n",
    "            local_mask[:, i, max(0, i-self.window_size):min(N, i+self.window_size+1)] = 0\n",
    "        if mask is not None:\n",
    "            local_mask = local_mask | (mask == 0)\n",
    "        return self.mha(x, local_mask == 0)   # 1表示可attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35ee44c",
   "metadata": {},
   "source": [
    "# MLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80ef851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class MultiHeadLatentAttention(nn.Module):\n",
    "    def __init__(self, d_model=4096, num_heads=32, q_latent_dim=512, kv_latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.q_latent_dim = q_latent_dim\n",
    "        self.kv_latent_dim = kv_latent_dim\n",
    "\n",
    "        # 1. 输入 → 低维潜空间\n",
    "        self.Wq_d = nn.Linear(d_model, q_latent_dim)      # Query latent\n",
    "        self.Wkv_d = nn.Linear(d_model, kv_latent_dim)    # KV latent（共享！）\n",
    "\n",
    "        # 2. 潜空间中做注意力\n",
    "        self.W_qk = nn.Linear(q_latent_dim, num_heads * kv_latent_dim)  # 多头展开\n",
    "        self.Wv_u = nn.Linear(kv_latent_dim, num_heads * self.head_dim) # 恢复V\n",
    "\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, _ = x.shape\n",
    "\n",
    "        C_q = self.Wq_d(x)                                      # (B, N, q_latent)\n",
    "        C_kv = self.Wkv_d(x)                                    # (B, N, kv_latent) ← 关键压缩\n",
    "\n",
    "        # 多头分数计算（在潜空间）\n",
    "        scores = torch.matmul(\n",
    "            self.W_qk(C_q).view(B, N, self.num_heads, self.kv_latent_dim).transpose(1, 2),\n",
    "            C_kv.transpose(-2, -1)[:, None, ...]\n",
    "        ) / math.sqrt(self.kv_latent_dim)\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # 恢复V并计算输出\n",
    "        V = self.Wv_u(C_kv).view(B, N, self.num_heads, self.head_dim)\n",
    "        out = torch.matmul(attn, V.transpose(1, 2)).transpose(1, 2).reshape(B, N, -1)\n",
    "        return self.out(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8371c29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docparse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
