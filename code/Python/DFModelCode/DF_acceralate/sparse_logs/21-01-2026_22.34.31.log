2026-01-21 22:34:31.719 | INFO     | llmcompressor.metrics.logger:_create_default_logger:356 - Logging all LLM Compressor modifier-level logs to sparse_logs/21-01-2026_22.34.31.log
2026-01-21 22:34:31.720 | DEBUG    | llmcompressor.core.lifecycle:initialize:92 - Initializing compression lifecycle
2026-01-21 22:34:31.720 | INFO     | llmcompressor.recipe.recipe:from_modifiers:68 - Creating recipe from modifiers
2026-01-21 22:58:33.649 | DEBUG    | llmcompressor.transformers.finetune.data.base:load_dataset:187 - Loading dataset garage-bAInd/Open-Platypus
2026-01-21 22:58:42.624 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:91 - Raw dataset: ['input', 'output', 'instruction', 'data_source']
2026-01-21 22:58:42.893 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:103 - Dataset after preprocessing: ['input', 'output', 'instruction', 'data_source', 'text', 'prompt']
2026-01-21 22:58:42.894 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:107 - Dataset after column renaming: ['input', 'output', 'instruction', 'data_source', 'text', 'prompt']
2026-01-21 22:58:42.894 | DEBUG    | llmcompressor.transformers.finetune.data.base:filter_tokenizer_args:240 - Found processor args `{'return_special_tokens_mask', 'text_pair_target', 'verbose', 'return_offsets_mapping', 'return_attention_mask', 'text', 'is_split_into_words', 'add_special_tokens', 'pad_to_multiple_of', 'padding', 'return_tensors', 'text_pair', 'stride', 'text_target', 'max_length', 'truncation', 'return_overflowing_tokens', 'return_token_type_ids', 'padding_side', 'return_length'}`. Removing all other columns
2026-01-21 22:58:42.896 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:115 - Tokenizer args after filtering: ['text', 'prompt']
2026-01-21 22:59:11.432 | DEBUG    | llmcompressor.transformers.finetune.data.base:load_dataset:187 - Loading dataset garage-bAInd/Open-Platypus
2026-01-21 22:59:19.924 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:91 - Raw dataset: ['input', 'output', 'instruction', 'data_source']
2026-01-21 22:59:20.157 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:103 - Dataset after preprocessing: ['input', 'output', 'instruction', 'data_source', 'text', 'prompt']
2026-01-21 22:59:20.158 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:107 - Dataset after column renaming: ['input', 'output', 'instruction', 'data_source', 'text', 'prompt']
2026-01-21 22:59:20.158 | DEBUG    | llmcompressor.transformers.finetune.data.base:filter_tokenizer_args:240 - Found processor args `{'return_special_tokens_mask', 'text_pair_target', 'verbose', 'return_offsets_mapping', 'return_attention_mask', 'text', 'is_split_into_words', 'add_special_tokens', 'pad_to_multiple_of', 'padding', 'return_tensors', 'text_pair', 'stride', 'text_target', 'max_length', 'truncation', 'return_overflowing_tokens', 'return_token_type_ids', 'padding_side', 'return_length'}`. Removing all other columns
2026-01-21 22:59:20.160 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:115 - Tokenizer args after filtering: ['text', 'prompt']
2026-01-21 23:20:16.639 | DEBUG    | llmcompressor.transformers.finetune.data.base:load_dataset:187 - Loading dataset garage-bAInd/Open-Platypus
2026-01-21 23:20:22.001 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:91 - Raw dataset: ['input', 'output', 'instruction', 'data_source']
2026-01-21 23:20:22.234 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:103 - Dataset after preprocessing: ['input', 'output', 'instruction', 'data_source', 'text', 'prompt']
2026-01-21 23:20:22.235 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:107 - Dataset after column renaming: ['input', 'output', 'instruction', 'data_source', 'text', 'prompt']
2026-01-21 23:20:22.235 | DEBUG    | llmcompressor.transformers.finetune.data.base:filter_tokenizer_args:240 - Found processor args `{'return_special_tokens_mask', 'text_pair_target', 'verbose', 'return_offsets_mapping', 'return_attention_mask', 'text', 'is_split_into_words', 'add_special_tokens', 'pad_to_multiple_of', 'padding', 'return_tensors', 'text_pair', 'stride', 'text_target', 'max_length', 'truncation', 'return_overflowing_tokens', 'return_token_type_ids', 'padding_side', 'return_length'}`. Removing all other columns
2026-01-21 23:20:22.237 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:115 - Tokenizer args after filtering: ['text', 'prompt']
2026-01-21 23:23:54.878 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:91 - Raw dataset: ['image', 'height', 'width', 'prefix', 'suffix', 'prompt', 'image', 'height', 'width', 'prefix', 'suffix', 'prompt']
2026-01-21 23:23:54.878 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:107 - Dataset after column renaming: ['image', 'height', 'width', 'prefix', 'suffix', 'prompt', 'image', 'height', 'width', 'prefix', 'suffix', 'prompt']
2026-01-21 23:23:54.879 | DEBUG    | llmcompressor.transformers.finetune.data.base:filter_tokenizer_args:240 - Found processor args `{'return_special_tokens_mask', 'text_pair_target', 'verbose', 'return_offsets_mapping', 'return_attention_mask', 'text', 'is_split_into_words', 'add_special_tokens', 'pad_to_multiple_of', 'padding', 'return_tensors', 'text_pair', 'stride', 'text_target', 'max_length', 'truncation', 'return_overflowing_tokens', 'return_token_type_ids', 'padding_side', 'return_length'}`. Removing all other columns
2026-01-21 23:23:54.882 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:115 - Tokenizer args after filtering: ['prompt', 'prompt']
2026-01-21 23:27:38.147 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:91 - Raw dataset: ['image', 'height', 'width', 'prefix', 'suffix', 'prompt', 'image', 'height', 'width', 'prefix', 'suffix', 'prompt']
2026-01-21 23:27:38.148 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:107 - Dataset after column renaming: ['image', 'height', 'width', 'prefix', 'suffix', 'prompt', 'image', 'height', 'width', 'prefix', 'suffix', 'prompt']
2026-01-21 23:27:38.148 | DEBUG    | llmcompressor.transformers.finetune.data.base:filter_tokenizer_args:240 - Found processor args `{'return_special_tokens_mask', 'text_pair_target', 'verbose', 'return_offsets_mapping', 'return_attention_mask', 'text', 'is_split_into_words', 'add_special_tokens', 'pad_to_multiple_of', 'padding', 'return_tensors', 'text_pair', 'stride', 'text_target', 'max_length', 'truncation', 'return_overflowing_tokens', 'return_token_type_ids', 'padding_side', 'return_length'}`. Removing all other columns
2026-01-21 23:27:38.151 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:115 - Tokenizer args after filtering: ['prompt', 'prompt']
2026-01-21 23:29:39.902 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:91 - Raw dataset: ['image', 'height', 'width', 'prefix', 'suffix', 'prompt', 'image', 'height', 'width', 'prefix', 'suffix', 'prompt']
2026-01-21 23:29:39.902 | DEBUG    | llmcompressor.transformers.finetune.data.base:rename_columns:227 - Renaming column `prompt` to `text`
2026-01-21 23:29:39.905 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:107 - Dataset after column renaming: ['image', 'height', 'width', 'prefix', 'suffix', 'text', 'image', 'height', 'width', 'prefix', 'suffix', 'text']
2026-01-21 23:29:39.906 | DEBUG    | llmcompressor.transformers.finetune.data.base:filter_tokenizer_args:240 - Found processor args `{'return_special_tokens_mask', 'text_pair_target', 'verbose', 'return_offsets_mapping', 'return_attention_mask', 'text', 'is_split_into_words', 'add_special_tokens', 'pad_to_multiple_of', 'padding', 'return_tensors', 'text_pair', 'stride', 'text_target', 'max_length', 'truncation', 'return_overflowing_tokens', 'return_token_type_ids', 'padding_side', 'return_length'}`. Removing all other columns
2026-01-21 23:29:39.908 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:115 - Tokenizer args after filtering: ['text', 'text']
2026-01-21 23:29:40.430 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:130 - Model kwargs after tokenizing: ['input_ids', 'attention_mask', 'input_ids', 'attention_mask']
2026-01-21 23:29:40.431 | DEBUG    | llmcompressor.transformers.finetune.data.base:__call__:159 - Model kwargs after postprocessing: ['input_ids', 'attention_mask', 'input_ids', 'attention_mask']
2026-01-21 23:29:40.431 | WARNING  | llmcompressor.datasets.utils:format_calibration_data:132 - Requested 512 calibration samples but the provided dataset only has 3. 
2026-01-21 23:29:40.438 | DEBUG    | llmcompressor.core.lifecycle:reset:59 - Resetting compression lifecycle
2026-01-21 23:29:40.438 | INFO     | llmcompressor.core.lifecycle:reset:71 - Compression lifecycle reset
2026-01-21 23:29:40.439 | DEBUG    | llmcompressor.core.state:update:182 - Updating state with provided parameters: {'model': Qwen2ForCausalLM(
  (model): Qwen2Model(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2DecoderLayer(
        (self_attn): Qwen2Attention(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2RotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
), 'teacher_model': None, 'optimizer': None, 'attach_optim_callbacks': True, 'train_data': None, 'val_data': None, 'test_data': None, 'calib_data': <torch.utils.data.dataloader.DataLoader object at 0x7f8ed53f9010>, 'copy_data': True, 'start': -1, 'steps_per_epoch': None, 'batches_per_step': None, 'loggers': None, 'model_log_cadence': None, 'kwargs': {}}
2026-01-21 23:29:40.443 | DEBUG    | llmcompressor.core.lifecycle:initialize:92 - Initializing compression lifecycle
2026-01-21 23:29:40.443 | INFO     | llmcompressor.recipe.recipe:from_modifiers:68 - Creating recipe from modifiers
2026-01-21 23:29:40.444 | INFO     | llmcompressor.modifiers.smoothquant.base:_infer_mappings_from_model:188 - No SmoothQuantModifier.mappings provided, inferring from model...
2026-01-21 23:29:41.722 | DEBUG    | llmcompressor.core.lifecycle:initialize:105 - Initialized modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=False ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-21 23:29:41.770 | DEBUG    | llmcompressor.core.lifecycle:initialize:105 - Initialized modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=False ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-21 23:29:41.771 | INFO     | llmcompressor.core.lifecycle:initialize:110 - Compression lifecycle initialized for 2 modifiers
2026-01-21 23:29:41.773 | INFO     | llmcompressor.pipelines.independent.pipeline:IndependentPipeline:43 - Inferred `SequentialPipeline` for `SmoothQuantModifier`
2026-01-21 23:29:42.260 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-21 23:29:42.261 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(input_ids, inputs_embeds):
    if (input_ids is None) ^ (inputs_embeds is not None):
        raise ValueError('You must specify exactly one of input_ids or inputs_embeds')
    return ()
2026-01-21 23:29:42.261 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-21 23:29:42.261 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-21 23:29:42.262 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_1(input_ids, inputs_embeds):
    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)
    return (inputs_embeds,)
2026-01-21 23:29:42.262 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-21 23:29:42.262 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-21 23:29:42.262 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_2(past_key_values, use_cache):
    if use_cache and past_key_values is None:
        past_key_values = DynamicCache()
    return (past_key_values,)
2026-01-21 23:29:42.263 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-21 23:29:42.263 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-21 23:29:42.264 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_3(cache_position, inputs_embeds, past_key_values, *, past_seen_tokens=None):
    if cache_position is None:
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        cache_position = torch.arange(past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device)
    return (cache_position, past_seen_tokens)
2026-01-21 23:29:42.264 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-21 23:29:42.265 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-21 23:29:42.265 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_4(cache_position, position_ids):
    if position_ids is None:
        position_ids = cache_position.unsqueeze(0)
    return (position_ids,)
2026-01-21 23:29:42.265 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-21 23:29:42.266 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-21 23:29:42.266 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_5(attention_mask, cache_position, inputs_embeds, past_key_values, position_ids, *, causal_mask_mapping=None, mask_kwargs=None):
    if not isinstance((causal_mask_mapping := attention_mask), dict):
        mask_kwargs = {'config': self.config, 'input_embeds': inputs_embeds, 'attention_mask': attention_mask, 'cache_position': cache_position, 'past_key_values': past_key_values, 'position_ids': position_ids}
        causal_mask_mapping = {'full_attention': create_causal_mask(**mask_kwargs)}
        if self.has_sliding_layers:
            causal_mask_mapping['sliding_attention'] = create_sliding_window_causal_mask(**mask_kwargs)
    return (causal_mask_mapping, mask_kwargs)
2026-01-21 23:29:42.267 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-21 23:29:42.272 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-21 23:29:42.273 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(kwargs, labels, logits, loss):
    if labels is not None:
        loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
    return (loss,)
2026-01-21 23:29:42.273 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-21 23:29:42.459 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.CALIBRATION_EPOCH_START
2026-01-21 23:29:42.460 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ed4523470>
2026-01-21 23:29:42.460 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ed4503890>
2026-01-21 23:29:42.461 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea083e0>
2026-01-21 23:29:42.461 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea084d0>
2026-01-21 23:29:42.461 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea08560>
2026-01-21 23:29:42.461 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea085f0>
2026-01-21 23:29:42.462 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ed4521a30>
2026-01-21 23:29:42.462 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea086e0>
2026-01-21 23:29:42.462 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea087a0>
2026-01-21 23:29:42.462 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea08800>
2026-01-21 23:29:42.463 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea08890>
2026-01-21 23:29:42.463 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea08920>
2026-01-21 23:29:42.463 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea089b0>
2026-01-21 23:29:42.463 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea08a40>
2026-01-21 23:29:42.464 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea08b00>
2026-01-21 23:29:42.464 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea08b60>
2026-01-21 23:29:42.464 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea08bf0>
2026-01-21 23:29:42.464 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea08c80>
2026-01-21 23:29:42.465 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea08d10>
2026-01-21 23:29:42.465 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea08da0>
2026-01-21 23:29:42.465 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea082c0>
2026-01-21 23:29:42.465 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea08e60>
2026-01-21 23:29:42.466 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea08f20>
2026-01-21 23:29:42.466 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea08fb0>
2026-01-21 23:29:42.466 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea09040>
2026-01-21 23:29:42.466 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea09100>
2026-01-21 23:29:42.467 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea091f0>
2026-01-21 23:29:42.467 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea092e0>
2026-01-21 23:29:42.467 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea093d0>
2026-01-21 23:29:42.467 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea094c0>
2026-01-21 23:29:42.468 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea095b0>
2026-01-21 23:29:42.468 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea096a0>
2026-01-21 23:29:42.468 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea09760>
2026-01-21 23:29:42.468 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea09820>
2026-01-21 23:29:42.469 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea09910>
2026-01-21 23:29:42.469 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea099d0>
2026-01-21 23:29:42.469 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea09ac0>
2026-01-21 23:29:42.469 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea09bb0>
2026-01-21 23:29:42.470 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea09ca0>
2026-01-21 23:29:42.470 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea09d60>
2026-01-21 23:29:42.470 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea09e50>
2026-01-21 23:29:42.470 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea09f40>
2026-01-21 23:29:42.471 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea0a000>
2026-01-21 23:29:42.471 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea0a0c0>
2026-01-21 23:29:42.471 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea0a1b0>
2026-01-21 23:29:42.471 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea0a240>
2026-01-21 23:29:42.471 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea0a2d0>
2026-01-21 23:29:42.472 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea0a390>
2026-01-21 23:29:42.472 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea0a480>
2026-01-21 23:29:42.472 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea0a540>
2026-01-21 23:29:42.472 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea0a600>
2026-01-21 23:29:42.473 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea0a6f0>
2026-01-21 23:29:42.473 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea0a720>
2026-01-21 23:29:42.473 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea0a840>
2026-01-21 23:29:42.473 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea0a900>
2026-01-21 23:29:42.474 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f8ecea0a9c0>
2026-01-21 23:29:42.474 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-21 23:29:43.288 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-21 23:29:43.289 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.0.input_layernorm
2026-01-21 23:29:43.304 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.0.post_attention_layernorm
2026-01-21 23:29:43.307 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
