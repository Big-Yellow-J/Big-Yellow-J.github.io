{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca4d5c76",
   "metadata": {},
   "source": [
    "# 模型量化处理\n",
    "| 文件名称 | 文件描述 | 文件地址 |\n",
    "| --- | --- | --- |\n",
    "|`LLM_Quant.py`  | 使用llmcompressor对LLM模型进行量化处理 | [LLM_Quant.py](code/Python/DFModelCode/DF_acceralate/code/LLM_Quant.py) |\n",
    "|`VLLM_Quant.py` | 使用llmcompressor对vllm模型进行量化处理 | [VLLM_Quant.py](code/Python/DFModelCode/DF_acceralate/code/VLLM_Quant.py) |\n",
    "|`model_generate.py`| 使用llmcompressor使用量化后的模型 | [model_generate.py](code/Python/DFModelCode/DF_acceralate/code/model_generate.py) |\n",
    "\n",
    "**简单总结**：1、如果使用llmcompressor去优化模型之后，对于模型加载推理更加推荐使用vlm进行模型加载，使用hf上的模型加载方式在生成生速度会慢很多；\n",
    "\n",
    "| 量化方式 | 准确率 |\n",
    "| --- | --- |\n",
    "|`raw model`        | 60.27% |\n",
    "|`GPTQ-W4A16`       | 49.54% |\n",
    "|`AWQ-W4A16`        | 56.16% |\n",
    "|`GPTQ-W8A8`        | 60.14% |\n",
    "|`GPTQ-FP8_DYNAMIC` | 59.35% |\n",
    "\n",
    "AWQ 高度依赖激活分布统计，如果校准数据是英文、通用对话、或与目标任务（中文多科目知识）差异很大，AWQ 学到的 per-channel scale 会严重偏离，导致保护错通道，精度反而崩得比 GPTQ 更厉害。GPTQ 只看权重 Hessian，不依赖激活，所以对校准数据敏感度低。\n",
    "\n",
    "* 数据选择\n",
    "\n",
    "![](https://s2.loli.net/2026/01/21/aXEqLHontgUS1Pd.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa24808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7165cc4a",
   "metadata": {},
   "source": [
    "## 使用llama.cpp量化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea3f3b",
   "metadata": {},
   "source": [
    "安装方式：\n",
    "```bash\n",
    "git clone https://github.com/ggml-org/llama.cpp\n",
    "cd llama.cpp\n",
    "cmake -B build\n",
    "cmake --build build --config Release -j 8\n",
    "\n",
    "# 检查环境版本然后安装 llama-cpp\n",
    "python --version # 3.12.12\n",
    "pip show torch # 2.8.0+cu126\n",
    "nvidia-smi # 12.6 \n",
    "wget https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.16-cu124/llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl\n",
    "pip install llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl\n",
    "```\n",
    "最后结果都会存储在`./build/bin/`\n",
    "### 基本原理解释\n",
    "#### 1、量化精度\n",
    "\n",
    "#### 2、量化范式\n",
    "### 1、量化大语言模型\n",
    "> https://qwen.readthedocs.io/zh-cn/latest/quantization/llama.cpp.html\n",
    "\n",
    "为了实现GGUF格式转换，按照下面步骤进行\n",
    "#### 1、无校准量化GGUF\n",
    "```python\n",
    "# 首先创建GGUF文件  转换文件\n",
    "python convert-hf-to-gguf.py 模型名称/地址 --outtype bf16 --outfile Qwen3-1.5B-BF16.gguf \n",
    "# 其中的 outtype后面的数据类型可以是：bf16 f32等\n",
    "```\n",
    "在执行上面代码之后直接将模型量化到`8 bit`：\n",
    "```python\n",
    "./build/bin/llama-quantize --quantize --model Qwen3-1.5B-F16.gguf --output Qwen3-1.5B-Q8_0.gguf --type Q8_0\n",
    "```\n",
    "`Q8_0`是一个量化预设的代号，在此之前可以使用`python convert_hf_to_gguf.py --help`了解一些这个脚本都有哪些参数，`python convert_hf_to_gguf.py --print-supported-models`直接看一下支持哪些模型进行量化，一般而言模型名称就和`config.json`中是一致的比如`Qwen2ForCausalLM`。量化开始：**首先**，在下载得到模型权重之后，在`cache_dir`里面就会有模型权重，比如说上面过程中模型权重就会保存为：`/Model/models--Qwen--Qwen2.5-1.5B-Instruct`，那么在创建以及转换文件过程中就需要将其中的 **模型名称/地址**进行替换，那么**完整带脚本**就是：`python convert_hf_to_gguf.py /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/Model/models--Qwen--Qwen2.5-1.5B-Instruct --outfile /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-BF16.gguf --outtype bf16 --verbose`\n",
    "**不过**，上面代码还是会出错，因为实际模型文件在`code/Python/DFModelCode/DF_acceralate/Model/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306`因此需要将上面的文件路径改为：`/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/Model/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306`最后输出：\n",
    "```bash\n",
    "...\n",
    "INFO:hf-to-gguf:Model successfully exported to /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-BF16.gguf\n",
    "```\n",
    "就表示成功了，而后可以通过`./build/bin/llama-quantize --help`去检查支持哪些参数，**并且进行量化**：`./build/bin/llama-quantize /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-BF16.gguf /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-Q8_0.gguf q8_0`。不过还有一点有意思的，比如说我的模型一般都会用lora进行微调，我需要对lora微调模型进行处理，类似的：\n",
    "```bash\n",
    "python convert_lora_to_gguf.py /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/test-lora/ --outfile /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Lora-Qwen3-1.5B-BF16.gguf --outtype bf16 --verbose\n",
    "\n",
    "./build/bin/llama-quantize /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Lora-Qwen3-1.5B-BF16.gguf /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Lora-Qwen3-1.5B-Q8_0.gguf q8_0\n",
    "```\n",
    "> **总结上面过程代码**：\n",
    "> `python convert_hf_to_gguf.py /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/Model/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306 --outfile /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-BF16.gguf --outtype bf16 --verbose`\n",
    "> `./build/bin/llama-quantize /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-BF16.gguf /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-Q8_0.gguf q8_0`\n",
    "\n",
    "#### 2、使用重要性矩阵量化GGUF\n",
    "```python\n",
    "# 首先\n",
    "./llama-imatrix -m Qwen3-1.5B-F16.gguf -f calibration-text.txt --chunk 512 -o Qwen3-1.5B-imatrix.dat -ngl 80\n",
    "# \n",
    "./llama-quantize --imatrix Qwen3-1.5B-imatrix.dat Qwen3-1.5B-F16.gguf Qwen3-1.5B-Q4_K_M.gguf Q4_K_M\n",
    "```\n",
    "\n",
    "### 使用GGUF量化模型\n",
    "```bash\n",
    "pip install llama-cpp-python\n",
    "# 直接去 https://github.com/abetlen/llama-cpp-python/releases 下载对应的版本 和flash-attn一样\n",
    "pip install llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl\n",
    "```\n",
    "这样一来就安装好了就可以直接加载GGUF模型权重了（和非量化模型推理10.8s，**量化后推理：3.5s**）：\n",
    "```python\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-Q8_0.gguf\",\n",
    "    # lora_path= \n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=-1,\n",
    "    chat_format=\"chatml\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个有帮助的助手，用简洁的中文回答。\"},\n",
    "    {\"role\": \"user\",   \"content\": \"晚上睡不着怎么办\"}\n",
    "]\n",
    "\n",
    "# 生成\n",
    "response = llm.create_chat_completion(\n",
    "    messages,\n",
    "    max_tokens=300,\n",
    "    temperature=0.75,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.1,\n",
    "    stream=False  # 改成 True 即可流式输出\n",
    ")\n",
    "\n",
    "print(\"AI 回答：\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "```\n",
    "\n",
    "#### 2、量化范式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b5a8878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "\n",
    "def load_model():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "                                                cache_dir= '/root/autodl-tmp/Model/', \n",
    "                                                mirror='https://hf-mirror.com')\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "                                                cache_dir= '/root/autodl-tmp/Model/', \n",
    "                                                mirror='https://hf-mirror.com')\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_GGUF(lora_path=None):\n",
    "    from llama_cpp import Llama\n",
    "\n",
    "    llm = Llama(\n",
    "        model_path=\"/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-Q8_0.gguf\",\n",
    "        lora_path= lora_path,\n",
    "        n_ctx=8192,\n",
    "        n_gpu_layers=-1,\n",
    "        chat_format=\"chatml\",\n",
    "        verbose=False\n",
    "    )\n",
    "    return llm\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个有帮助的助手，用简洁的中文回答。\"},\n",
    "    {\"role\": \"user\",   \"content\": \"晚上睡不着怎么办?\"}\n",
    "]\n",
    "model, tokenizer = load_model()\n",
    "model_gguf = load_GGUF()\n",
    "model_gguf_lora = load_GGUF(lora_path= '/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Lora-Qwen3-1.5B-BF16.gguf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da151b3",
   "metadata": {},
   "source": [
    "## 使用llmcompressor量化\n",
    "> `llmcompressor==0.7.1、transformers==4.55.2、torch==2.8.0+cu126、trl==0.22.2、vllm==0.11.0` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d05bbb",
   "metadata": {},
   "source": [
    "### 原理及官方文档总结"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82746fc",
   "metadata": {},
   "source": [
    "| 方法 | 描述 | 准确性恢复 vs. 时间 |\n",
    "|---|---|---|\n",
    "| GPTQ | 利用二阶逐层优化来优先处理重要的权重/激活，并允许更新剩余权重 | 准确性恢复高，但运行成本更高/更慢 |\n",
    "| AWQ | 使用通道级缩放更好地保留权重和激活中的重要异常值 | 比GPTQ具有更好的准确性恢复和更快的运行时间 |\n",
    "| SmoothQuant | 通过将异常值折叠到权重中来平滑激活中的异常值，确保权重和激活量化模型具有更好的准确性 | 准确性恢复良好，校准时间最短；可与其他方法组合使用 |\n",
    "| 就近取整(RTN) | 简单的量化技术，将每个值四舍五入到目标精度中最接近的可表示级别 | 在大多数情况下提供中等准确性恢复。计算成本低且实现速度快，适用于实时或资源受限的环境 |\n",
    "| AutoRound | 通过符号梯度下降优化舍入和裁剪范围 | 与GPTQ/AWQ相比，提供领先的4位和优异的低于4位精度，运行时间比GPTQ快，与AWQ相当 |\n",
    "\n",
    "直接对[官方](https://docs.vllm.ai/projects/llm-compressor/en/0.7.1/getting-started/compress/#apply-the-recipe)中描述进行总结，收其提供的压缩方式进行操作，直接使用llmcompressor中的压缩方式使用起来很简单，可以直接使用`oneshot`进行量化，比如说：\n",
    "```python\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor import oneshot\n",
    "\n",
    "recipe = [\n",
    "    SmoothQuantModifier(smoothing_strength=0.8),\n",
    "    GPTQModifier(scheme=\"W8A8\", targets=\"Linear\", ignore=[\"lm_head\"]),\n",
    "]\n",
    "oneshot(\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    dataset=\"open_platypus\",\n",
    "    recipe=recipe,\n",
    "    output_dir=\"TinyLlama-1.1B-Chat-v1.0-INT8\",\n",
    "    max_seq_length=2048,\n",
    "    num_calibration_samples=512,\n",
    ")\n",
    "```\n",
    "**值得注意的是**，在使用其进行模型压缩过程中对于显存的估计大概是：`mem(1B parameters) ~= (1B parameters) * (2 bytes / parameter) = 2B bytes ~= 2Gb`，除此之外压缩模型过程中对于文本解码层（LLM部分）以及视觉编码层（Vision Layer部分）两部分压缩过程中处理存在差异，**前者**会动态地将模型逐层加载到GPU进行计算，其余部分仍保留在CPU内存中。**后者**会一次性全部加载到GPU。与文本模型不同，视觉编码层在加载到GPU前不会被拆分为独立层。对于视觉塔层规模大于文本层的模型，这种机制可能导致GPU内存瓶颈。目前（**测试的是0.7.1版本实际对于最新的也是如此**） LLM Compressor 暂不对视觉编码器进行量化处理，因量化技术在延迟/吞吐量与精度损失间的权衡通常得不偿失。比如说：\n",
    "**1、四舍五入量化（Round To Nearest RTN）**  \n",
    "![](https://s2.loli.net/2026/01/20/JiXhKPoFfpNdv7e.png)  \n",
    "**2、GPTQ**  \n",
    "![](https://s2.loli.net/2026/01/20/m2IqEYWsFKUPbkT.png)  \n",
    "在llm compressor中支持[量化精度](https://github.com/vllm-project/llm-compressor/blob/main/docs/guides/compression_schemes.md?plain=1)以及量化方式：`GPTQModifier`、`AWQModifier`、``：  \n",
    "\n",
    "| Scheme | Description | Data required? | vLLM Hardware Compatibility |\n",
    "|--------|-------------|----------------|-----------------------------|\n",
    "| **[W8A8-FP8](../../examples/quantization_w8a8_fp8/README.md)** | 8位浮点（FP8）量化用于权重和激活值，通过8位算术运算实现约2倍的权重压缩。采用逐通道（channel-wise）量化将权重压缩至8位，并采用动态逐token或静态逐张量（per-tensor）量化将激活值压缩至8位。权重的缩放因子既可以按通道生成，也可以按张量生成。其中，逐通道权重量化配合动态逐token激活量化是性能最优的选项。W8A8-FP8无需校准数据集，激活值的量化在vLLM推理过程中实时进行。该方案在通用性能和压缩率方面表现良好，尤其适用于服务器端和批处理推理场景。 | 无需校准数据集，除非你采用静态逐张量激活量化。 | 支持最新的 NVIDIA GPU（Hopper 架构及之后）和最新的 AMD GPU。推荐用于计算能力 >=8.9 的 NVIDIA GPU（包括 Hopper、Ada Lovelace 和 Blackwell 架构）。 |\n",
    "| **[W8A8-INT8](../../examples/quantization_w8a8_int8/README.md)** | 8位整数（INT8）量化用于权重和激活值，通过8位算术运算实现约2倍的权重压缩。使用GPTQ进行逐通道（channel-wise）量化将权重压缩至8位，并采用动态逐token量化将激活值压缩至8位。INT8权重量化既支持逐张量（per-tensor）也支持逐通道（per-channel）方式。W8A8-INT8在通用性能和压缩率方面表现良好，尤其适用于服务器端和批处理推理。激活值的量化在vLLM推理过程中执行，可选择静态或动态方式；此外，INT8激活值还支持非对称量化。W8A8-INT8有助于在高QPS场景或使用vLLM进行离线服务时提升推理速度。 | 权重量化以及静态逐张量激活量化需要校准数据集 | 支持所有NVIDIA GPU、AMD GPU、TPU、CPU及其他加速器。推荐用于计算能力 <8.9 的NVIDIA GPU（如Ampere、Turing、Volta、Pascal或更早架构）。 |\n",
    "| **[W4A16](../../examples/quantization_w4a16/README.md)** | 仅将权重量化为4位整数（INT4）精度，激活值保持16位浮点（FP16）精度。W4A16可实现约3.7倍的权重压缩，但仍需使用16位算术运算。W4A16还支持非对称权重量化。该方案在内存受限、对延迟敏感的应用中提供最大压缩率，并在低QPS场景下通过更高的权重压缩带来显著加速。所附示例采用GPTQ算法以降低量化损失，但也可使用其他算法，如[AWQ](../../examples/awq/llama_example.py) 进行| 需要数据集 | 所有设备 |\n",
    "| **W8A16** | 将模型权重编码为8位整数，激活值编码为16位整数。W8A16压缩相比FP32可生成更小的模型输出体积，并在具备原生8位整数计算单元的硬件上实现更快的推理速度。相比浮点运算，其功耗和内存带宽需求更低| 需要数据集 | 所有设备 |\n",
    "| **NVFP4** | NVFP4 是随 NVIDIA Blackwell GPU 架构推出的4位浮点编码格式。它通过高精度缩放编码和两级微块（micro-block）缩放策略，在张量值的宽动态范围内保持数值精度。NVFP4 压缩为每个张量生成一个全局缩放因子，并为每16个元素组成的组生成局部量化缩放因子。权重和激活值均使用全局缩放因子和局部量化缩放因子进行量化。组大小固定，不可更改。 | 需要数据集 | 仅支持 NVIDIA Blackwell 架构及后续 GPU |\n",
    "| [**W8A8-FP8_BLOCK**](../../examples/quantization_w8a8_fp8/fp8_block_example.py)| 采用分块量化（通常为 128×128 的块）将权重压缩至 FP8，并对激活值使用动态逐 token 组（每组 128 个 token）量化。无需校准数据集，激活量化在 vLLM 推理过程中实时执行 | 不需要数据集 | 支持最新的 NVIDIA GPU（Hopper 及之后）和最新的 AMD GPU。推荐用于计算能力 ≥8.9 的 NVIDIA GPU（包括 Hopper、Ada Lovelace 和 Blackwell 架构） |\n",
    "| **[2:4 Semi-structured Sparsity](../../examples/sparse_2of4_quantization_fp8/README.md)** | U采用半结构化稀疏（如 SparseGPT）：在张量中每连续 4 个权重中，将其中 2 个置零。使用权重逐通道量化压缩至 8 位，并对激活值采用动态逐 token 量化压缩至 8 位。相比标准 W8A8-FP8，可实现更优推理性能，且评估分数几乎无下降，参考[evaluation score](https://neuralmagic.com/blog/24-sparse-llama-fp8-sota-performance-for-nvidia-hopper-gpus/). 但小型模型可能因剩余非零权重不足以还原原始分布而出现精度下降。| 需要数据集 |  支持所有 NVIDIA GPU、AMD GPU、TPU、CPU 及其他加速器。推荐用于计算能力 ≥9.0 的 GPU（Hopper 和 Blackwell 架构） |\n",
    "| **Unstructured Sparsity** | 非结构化稀疏量化以无固定模式的方式将模型中的个别权重置零。与分块或通道剪枝不同，它在模型中任意位置移除贡献最小的权重，从而生成细粒度的稀疏矩阵。 |不需要数据集 | 支持所有 NVIDIA GPU、AMD GPU、TPU、CPU 及其他加速器 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335bd059",
   "metadata": {},
   "source": [
    "### 关键参数解释\n",
    "介绍其支持量化范式之后直接看代码操作\n",
    "```python\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor import oneshot\n",
    "\n",
    "recipe = [\n",
    "    SmoothQuantModifier(smoothing_strength=0.8),\n",
    "    GPTQModifier(scheme=\"W8A8\", targets=\"Linear\", ignore=[\"lm_head\"]),\n",
    "]\n",
    "oneshot(\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    dataset=\"open_platypus\",\n",
    "    recipe=recipe,\n",
    "    output_dir=\"TinyLlama-1.1B-Chat-v1.0-INT8\",\n",
    "    max_seq_length=2048,\n",
    "    num_calibration_samples=512,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795146d3",
   "metadata": {},
   "source": [
    "\n",
    "#### 1、oneshot参数解释\n",
    "**首先**看`oneshot`中参数操作，在代码`llmcompressor/entrypoints/oneshot.py`以及[文档](https://docs.vllm.ai/projects/llm-compressor/en/0.7.1/reference/llmcompressor/entrypoints/oneshot/?h=oneshot)中参数，着重了解：  \n",
    "* `model`：直接使用hf上模型名称/本地名称，对于本地需要具体到config.json位置比如说：`models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa30/`  \n",
    "* `cache_dir`：缓存路径：str\n",
    "* `output_dir`：保存路径：str\n",
    "* `log_dir`: 日志路径：str\n",
    "* `text_column`：指定数据的columns（也就是用数据集中那一个标签的数据量化处理）\n",
    "* `dataset`：使用的数据集，str, Dataset, DatasetDict，如果在直接使用hf上数据集那么直接用名称就行，如open_platypus。（具体代码见下）对于后面两项可以直接用 `datasets`中load方式加载数据即可\n",
    "\n",
    "此处简单介绍其中**数据处理逻辑**，在`llmcompressor`中数据的处理逻辑和`datasets`库（该库一般就是两个核心操作load、process，前者加载数据后者直接编码数据）处理逻辑一致，datasets中**加载本地数据**[具体逻辑](https://huggingface.co/docs/datasets/loading)如下：**1、如果就只有一个文件**（比如说我的所有数据都存储在一个jsonl中），那么可以直接使用`load_dataset('json', data_files= './test_datasets.jsonl',streaming=True)`;**2、如果有多个文件**，比较简单方式直接将`data_files`替换为`tmp/TestDatasets-Json/*.jsonl`其中用 * 通配符去匹配所有的jsonl文件，除此之外还可以**用一个字典去表示**，比如说：`{'train': [xxx, ..., xxx]}`\n",
    "> 上面代码中用 steaming=True 流式处理去处理数据较大情况，访问数据可以直接`next(iter(json_one_dataset[\"train\"]))`\n",
    "\n",
    "> **处理数据**，将数据丢给llm/vllm进行处理之前，需要将数据进行变换，比如说转化为对应的数据格式/对数据进行tokenizer进行处理，这里处理方式在`datasets`中可以直接使用map处理即可（具体处理看下面例子）。回到`oneshot`中的数据处理过程中，其内部逻辑和`datasets`中处理逻辑一致，处理本地文件逻辑如下：`get_processed_dataset`(llmcompressor/datasets/utils.py)判断数据是不是提前被tokenized处理，**如果是**那么直接返回即可，**如果不是**直接使用`TextGenerationDataset.load_from_registry`(llmcompressor/transformers/finetune/data/base.py)处理数据  \n",
    "==>`TextGenerationDataset`处理数据逻辑，加载数据-->process数据（两部分数据和hf库datasets的处理逻辑是一致的），对于第一部分**加载数据处理如下**：\n",
    "1、对于hf上数据直接加载==>返回  \n",
    "2、对于自定义的数据（也支持使用下载好的hf本地数据），使用`get_custom_datasets_from_path`(llmcompressor/transformers/finetune/data/data_helpers.py)，在这个函数中默认两个参数：path、ext（文件类型）。对于path格式要求  \n",
    "![](https://s2.loli.net/2026/01/20/my5v1aBonNuh9IZ.png)  \n",
    "最后都统一返回。**process数据处理过程**和`datasets`中处理是一致的，再代码中`TextGenerationDataset`(llmcompressor/transformers/finetune/data/base.py)中直接使用`self.map()`处理数据而后再去通过`format_calibration_data`（llmcompressor/datasets/utils.py）使用DataLoader进行处理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad8be73",
   "metadata": {},
   "source": [
    "#### 2、量化方式参数解释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a486b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff86ccc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': '../data/OmniDoc/images/eastmoney_1885ca41425d245551f3482457304f78b48186bff625fa91e675eaf6bba5229f.pdf_0.jpg', 'height': 2338, 'width': 1654, 'prefix': 'QwenVL HTML', 'suffix': '<body><h2 data-bbox=\"67 227 1138 289\">索通发展 Sunstone Development（603612 CH）</h2><h2 data-bbox=\"65 297 1566 355\">首次覆盖：扩张阳极产能稳固龙头地位，布局负极培育第二成长曲线</h2><h2 data-bbox=\"64 364 1597 449\">Expand Anode Production Capacity to Stabilize the Leading Position, and Lay out the Second Growth Curve of Anode Cultivation: Initiation</h2><p data-bbox=\"197 484 517 516\">观点聚焦 Investment Focus</p><h2 data-bbox=\"90 536 691 570\">首次覆盖优于大市 Initiate with OUTPERFORM</h2><div class=\"table\" data-bbox=\"69 571 703 885\"><table><tr><td>评级</td><td>优于大市OUTPERFORM</td></tr><tr><td>现价</td><td>Rmb18.38</td></tr><tr><td>目标价</td><td>Rmb24.50</td></tr><tr><td>市值</td><td>Rmb9.94bn/US$1.36bn</td></tr><tr><td>日交易额（3个月均值）</td><td>US$15.46mn</td></tr><tr><td>发行股票数目</td><td>540.85mn</td></tr><tr><td>自由流通股（%）</td><td>50%</td></tr><tr><td>1年股价最高最低值</td><td>Rmb33.62-Rmb15.99</td></tr></table></div><p data-bbox=\"75 895 484 918\">注：现价Rmb18.38为2023年9月21日收盘价</p><div class=\"image\" data-bbox=\"108 950 676 1352\"></div><p data-bbox=\"78 1356 216 1376\">资料来源：Factset</p><div class=\"table\" data-bbox=\"74 1414 701 1532\"><table><tr><td></td><td>1mth</td><td>3mth</td><td>12mth</td></tr><tr><td>绝对值</td><td>5.8%</td><td>9.7%</td><td>-40.8%</td></tr><tr><td>绝对值(美元)</td><td>5.7%</td><td>7.9%</td><td>-43.1%</td></tr><tr><td>相对 MSCI China</td><td>5.8%</td><td>9.7%</td><td>-40.8%</td></tr></table></div><div class=\"table\" data-bbox=\"75 1581 701 1878\"><table><tr><td>(Rmb mn)</td><td>Dec-22A</td><td>Dec-23E</td><td>Dec-24E</td><td>Dec-25E</td></tr><tr><td>营业收入</td><td>19,401</td><td>17,097</td><td>22,311</td><td>28,205</td></tr><tr><td>(+/-)</td><td>105%</td><td>-12%</td><td>31%</td><td>26%</td></tr><tr><td>净利润</td><td>905</td><td>189</td><td>948</td><td>1,391</td></tr><tr><td>(+/-)</td><td>46%</td><td>-79%</td><td>401%</td><td>47%</td></tr><tr><td>全面摊薄EPS (Rmb)</td><td>1.67</td><td>0.35</td><td>1.75</td><td>2.57</td></tr><tr><td>毛利率</td><td>13.2%</td><td>10.7%</td><td>14.4%</td><td>14.9%</td></tr><tr><td>净资产收益率</td><td>16.6%</td><td>2.7%</td><td>11.8%</td><td>14.7%</td></tr><tr><td>市盈率</td><td>10.75</td><td>51.38</td><td>10.27</td><td>7.00</td></tr></table></div><p data-bbox=\"76 1880 263 1907\">资料来源：公司信息，HTI</p><p data-bbox=\"737 531 1252 562\">(Please see APPENDIX 1 for English summary)</p><p data-bbox=\"734 597 1599 919\">- 预焙阳极行业龙头，盈利能力逐年提升。索通发展拥有国内最大的预焙阳极产能，占据着预焙阳极行业龙头地位，目前在产产能282万吨。公司的主营业务为预焙阳极的研发、生产及销售业务。公司营业收入增长迅猛，2022年实现营业收入 194.01 亿元，同比增长 105.12% ，三年 CAGR 达 64% ；归母净利持续提升，2022年实现归母净利9.05亿元，同比增长 45.99% ，三年 CAGR 达 119% 。2023年上半年实现营业收入81.30亿元，同比增长 2.09% ；实现归母净利 - 4.09亿元，同比减少 171.97% ；实现扣非归母净利 - 4.12亿元，同比减少 172.43%。</p><p data-bbox=\"732 953 1599 1205\">- 预焙阳极：下游需求端稳步回升，公司市占率不断提高。电解铝增产带动预焙阳极需求快速提升，而区域间供需错配仍然严重。在这种情况下，公司积极抓住北铝南移机遇，持续扩张西南地区预焙阳极产能，公司预计2025年阳极总产能将达到约500万吨，公司市占率有望快速提升。通过优化上游原材料布局，公司综合成本优势日益显著，预焙阳极单吨毛利从2019年的 407.6元／吨，提高至2022年的866.6元/吨。 866.6元／吨。</p><p data-bbox=\"732 1239 1599 1562\">- 负极：进军锂电负极行业，加速布局第二成长曲线。锂电池行业发展空间广阔，对负极材料需求较大。公司通过收购欣源股份快速切入锂电负极行业，进而构建以风光储氢一体化的绿色能源供应为基础的“预焙阳极+锂电负极”碳材料产业，打造公司的第二增长曲线。负极产能正处于加速扩建阶段。内蒙欣源新建4万吨石墨化项目与盛源负极项目首期一步2.5万吨石墨化产能投产， 770MW光伏发电项目已主体完工，公司预计2023年下半年通电。可以预见，未来公司负极材料业务的活力将得到大幅度增强，第二增长曲线将为公司的成长和发展提供源源不竭的能量。</p><p data-bbox=\"736 1598 1598 1811\">- 盈利预测与估值：我们预计2023-2025年公司收入分别为170.97、 223.11、282.05亿元，同比增长- 11.9%、30.5%、26.42%我们预计公司2023-2025年EPS分别为0.35、1.75、2.57元，根据可比公司估值，考虑到公司预焙阳极产能密集投产，切入锂电负极带来协同效应，我们给予公司2024年14倍PE，对应合理价值24.50元，首次覆盖给予“优于大市”评级。</p><p data-bbox=\"736 1848 1598 1915\">- 风险提示：原材料价格波动、下游需求不及预期、产能建设及释放不及预期。</p><div class=\"header\" data-bbox=\"1336 109 1538 203\">海通國际 HAITONG</div><div class=\"header\" data-bbox=\"63 134 311 204\">研究报告 Research Report 24 Sep 2023</div><div class=\"footer\" data-bbox=\"64 2066 266 2125\">吴旖婕 Yijie Wu lisa.yj.wu@htisec.com</div><div class=\"footer\" data-bbox=\"21 2156 1632 2306\">本研究报告由海通国际分销，海通国际是由海通国际研究有限公司，海通证券印度私人有限公司，海通国际株式会社和海通国际证券集团其他各成员单位的证券研究团队所组成的全球品牌，海通国际证券集团各成员分别在其许可的司法管辖区内从事证券活动。关于海通国际的分析师证明，重要披露声明和免责声明，请参阅附录。(Please see appendix for English translation of the disclaimer)</div></body>'} \n",
      " {'image': '../data/OmniDoc/images/eastmoney_d2bb183f5254ede5760932ac3118524aa7eddad551935713b435541f1525c1ee.pdf_3.jpg', 'height': 2339, 'width': 1654, 'prefix': 'QwenVL HTML', 'suffix': '<body><p data-bbox=\"125 239 612 272\">图表2：全国R&D经费投入强度（%）</p><div class=\"image\" data-bbox=\"115 286 1286 689\"></div><p data-bbox=\"119 704 461 737\">来源：国家统计局、农银国际证券</p><h2 data-bbox=\"192 845 1273 890\">四、专利密集型产业步入快速增长轨道，成为拉动经济增长的重要因素</h2><p data-bbox=\"122 938 1539 1374\">要将研发成果商业化，推动经济增长，关键因素是专利保护。专利是企业业务的一条护城河，专利也是研发活动的成果。有了专利保护，商业化价值就会提升。据国家统计局的数据，2022年专利密集型产业增加值占GDP的比重为 12.71% ，而2018年为 11.6% 。2018年至2022年，专利密集型产业增加值年复合增长率为 9.4% ，同期名义GDP值年复合增长率为 7.1% 。专利密集型产业经济的增长速度快于整体经济的增长速度。专利密集型产业虽然取得了较快的经济增长速度，但其在整体经济中的比重仍然较小，约为 13% 。由此看来，持续新质生产力发展的工作对于推动研发、实现创新、获得专利保护、提升商业价值、拉动经济增长具有重要意义。</p><p data-bbox=\"124 1414 559 1451\">图表3：专利密集型产业增加值</p><p data-bbox=\"122 1458 863 1495\">新装备制造业、新材料制造业和医药医疗产业增速较慢</p><div class=\"table\" data-bbox=\"121 1493 1299 1992\"><table><tr><td>(亿元)</td><td>2018</td><td>2020</td><td>2022</td><td>2018-2022 复合年增长率</td></tr><tr><td>专利密集型产业增加值</td><td>107,090</td><td>121,289</td><td>153,176</td><td>9.4%</td></tr><tr><td>新装备制造业</td><td>32,833</td><td>34,194</td><td>41,643</td><td>6.1%</td></tr><tr><td>信息通信技术服务业</td><td>19,472</td><td>26,415</td><td>33,888</td><td>14.9%</td></tr><tr><td>信息通信技术制造业</td><td>21,551</td><td>24,177</td><td>31,818</td><td>10.2%</td></tr><tr><td>新材料制造业</td><td>14,130</td><td>14,064</td><td>18,575</td><td>7.1%</td></tr><tr><td>医药医疗产业</td><td>9,465</td><td>10,984</td><td>12,880</td><td>8.0%</td></tr><tr><td>研发、设计和技术服务业</td><td>7,215</td><td>8,708</td><td>10,953</td><td>11.0%</td></tr><tr><td>环保产业</td><td>2,424</td><td>2,748</td><td>3,419</td><td>9.0%</td></tr></table></div><p data-bbox=\"123 1998 459 2029\">来源：国家统计局、农银国际证券</p><div class=\"header\" data-bbox=\"1123 54 1449 165\">農銀國際 ABC INTERNATIONAL ABCI SECURITIES COMPANY LIMITED</div><div class=\"footer\" data-bbox=\"816 2123 836 2149\">4</div></body>'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# hf_dataset = load_dataset(\"garage-bAInd/Open-Platypus\", \n",
    "#                        split=\"train\",\n",
    "#                        cache_dir= '/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/')\n",
    "# print(f\"{hf_dataset[0]}\")\n",
    "\n",
    "path_dir = './tmp/TestDatasets-Json/'\n",
    "json_dir_dataset = load_dataset('json', \n",
    "                                 data_files= {'train': [os.path.join(path_dir, f) for f in os.listdir(path_dir)]},)\n",
    "\n",
    "json_one_dataset = load_dataset('json', \n",
    "                                 data_files= './test_datasets.jsonl',\n",
    "                                 streaming=True)\n",
    "json_dict_dataset = load_dataset('json', \n",
    "                                 data_files= {'train': [os.path.join(path_dir, f) for f in os.listdir(path_dir)],\n",
    "                                              'test': [os.path.join(path_dir, f) for f in os.listdir(path_dir)]})\n",
    "\n",
    "# print(json_one_dataset, '\\n', json_dir_dataset, '\\n', json_dict_dataset)\n",
    "print(next(iter(json_one_dataset[\"train\"])), '\\n', json_dict_dataset['train'][0])\n",
    "\n",
    "# 处理数据\n",
    "def mulitmodel_data_process(data):\n",
    "    '''处理多模态数据'''\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\"content\": [{\"type\": \"text\", \"text\": \"You are a helpful multimodal assistant.\"}]},\n",
    "        {\"role\": \"user\",\"content\": [\n",
    "            {\"type\": \"image\", \"image\": data['image']},\n",
    "            {\"type\": \"text\", \"text\": data['prefix']},],},\n",
    "        {\"role\": \"assistant\",\"content\": [\n",
    "            {\"type\": \"text\", \"text\": data['suffix']}],},]\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "json_dict_dataset_map = json_dict_dataset.map(mulitmodel_data_process)\n",
    "json_dict_dataset_map['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270244b6",
   "metadata": {},
   "source": [
    "## 模型微调\n",
    "> https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb\n",
    "\n",
    "https://github.com/shangxiaaabb/ProjectCode/blob/main/code/Python/DFModelCode/DF_acceralate/GRPO_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0282f618",
   "metadata": {},
   "source": [
    "## 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "\n",
    "def load_model():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "                                                cache_dir= '/root/autodl-tmp/Model/', \n",
    "                                                mirror='https://hf-mirror.com')\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "                                                cache_dir= '/root/autodl-tmp/Model/', \n",
    "                                                mirror='https://hf-mirror.com')\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_GGUF(lora_path=None):\n",
    "    from llama_cpp import Llama\n",
    "\n",
    "    llm = Llama(\n",
    "        model_path=\"/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-Q8_0.gguf\",\n",
    "        lora_path= lora_path,\n",
    "        n_ctx=8192,\n",
    "        n_gpu_layers=-1,\n",
    "        chat_format=\"chatml\",\n",
    "        verbose=False\n",
    "    )\n",
    "    return llm\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个有帮助的助手，用简洁的中文回答。\"},\n",
    "    {\"role\": \"user\",   \"content\": \"晚上睡不着怎么办?\"}\n",
    "]\n",
    "model, tokenizer = load_model()\n",
    "model_gguf = load_GGUF()\n",
    "model_gguf_lora = load_GGUF(lora_path= '/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Lora-Qwen3-1.5B-BF16.gguf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb75de67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可以试试深呼吸、听轻音乐、喝杯热牛奶或泡个热水澡等方法。如果经常失眠，建议咨询医生。<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adea9d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI 回答：\n",
      "可尝试用热水泡脚，听听轻音乐，或者看看电视。\n"
     ]
    }
   ],
   "source": [
    "response = model_gguf.create_chat_completion(\n",
    "    messages,\n",
    "    max_tokens=300,\n",
    "    temperature=0.75,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.1,\n",
    "    stream=False  # 改成 True 即可流式输出\n",
    ")\n",
    "\n",
    "print(\"AI 回答：\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docparse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
