2026-01-22 23:15:48.829 | INFO     | llmcompressor.metrics.logger:_create_default_logger:356 - Logging all LLM Compressor modifier-level logs to sparse_logs/22-01-2026_23.15.48.log
2026-01-22 23:15:48.829 | DEBUG    | llmcompressor.core.lifecycle:initialize:92 - Initializing compression lifecycle
2026-01-22 23:15:48.829 | INFO     | llmcompressor.recipe.recipe:from_modifiers:68 - Creating recipe from modifiers
2026-01-22 23:15:48.830 | INFO     | llmcompressor.modifiers.smoothquant.base:_infer_mappings_from_model:188 - No SmoothQuantModifier.mappings provided, inferring from model...
2026-01-22 23:15:48.830 | INFO     | llmcompressor.modifiers.smoothquant.utils:get_layer_mappings_from_architecture:93 - Architecture Qwen2_5_VLForConditionalGeneration not found in mappings. Using default mappings: [LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')]
2026-01-22 23:15:53.131 | DEBUG    | llmcompressor.core.lifecycle:initialize:105 - Initialized modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=False ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:15:53.216 | DEBUG    | llmcompressor.core.lifecycle:initialize:105 - Initialized modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=False ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:15:53.216 | INFO     | llmcompressor.core.lifecycle:initialize:110 - Compression lifecycle initialized for 2 modifiers
2026-01-22 23:15:53.217 | INFO     | llmcompressor.pipelines.independent.pipeline:IndependentPipeline:43 - Inferred `SequentialPipeline` for `SmoothQuantModifier`
2026-01-22 23:16:00.357 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:00.358 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(labels, logits, loss):
    if labels is not None:
        loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size)
    return (loss,)
2026-01-22 23:16:00.358 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:00.365 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:00.366 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(input_ids, inputs_embeds):
    if inputs_embeds is None:
        inputs_embeds = self.get_input_embeddings()(input_ids)
    return (inputs_embeds,)
2026-01-22 23:16:00.366 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:00.366 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:00.367 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_1(image_grid_thw, input_ids, inputs_embeds, pixel_values, *, _=None, image_embeds=None, image_mask=None):
    if pixel_values is not None:
        image_embeds = self.get_image_features(pixel_values, image_grid_thw)
        image_embeds = torch.cat(image_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)
        image_mask, _ = self.get_placeholder_mask(input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds)
        inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)
    return (_, image_embeds, image_mask, inputs_embeds)
2026-01-22 23:16:00.367 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:00.367 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:00.368 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_2(_, input_ids, inputs_embeds, pixel_values_videos, video_grid_thw, *, video_embeds=None, video_mask=None):
    if pixel_values_videos is not None:
        video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)
        video_embeds = torch.cat(video_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)
        _, video_mask = self.get_placeholder_mask(input_ids, inputs_embeds=inputs_embeds, video_features=video_embeds)
        inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)
    return (_, inputs_embeds, video_embeds, video_mask)
2026-01-22 23:16:00.368 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:00.369 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:00.371 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_3(_, attention_mask, cache_position, image_grid_thw, input_ids, inputs_embeds, past_key_values, position_ids, rope_deltas, second_per_grid_ts, video_grid_thw, *, batch_size=None, delta=None, prefill_compiled_stage=None, prefill_noncompiled_stage=None, seq_length=None):
    if position_ids is None:
        prefill_compiled_stage = is_torchdynamo_compiling() and (input_ids is not None and input_ids.shape[1] != 1 or (inputs_embeds is not None and inputs_embeds.shape[1] != 1))
        prefill_noncompiled_stage = not is_torchdynamo_compiling() and (cache_position is not None and cache_position[0] == 0 or (past_key_values is None or past_key_values.get_seq_length() == 0))
        if (prefill_compiled_stage or prefill_noncompiled_stage) or self.rope_deltas is None:
            position_ids, rope_deltas = self.get_rope_index(input_ids, image_grid_thw, video_grid_thw, second_per_grid_ts=second_per_grid_ts, attention_mask=attention_mask)
            self.rope_deltas = rope_deltas
        else:
            batch_size, seq_length, _ = inputs_embeds.shape
            position_ids = torch.arange(seq_length, device=inputs_embeds.device)
            position_ids = position_ids.view(1, 1, -1).expand(3, batch_size, -1)
            if cache_position is not None:
                delta = (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)
            else:
                delta = torch.zeros((batch_size, seq_length), device=inputs_embeds.device)
            delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=1)
            position_ids += delta.to(position_ids.device)
    return (_, batch_size, delta, position_ids, prefill_compiled_stage, prefill_noncompiled_stage, rope_deltas, seq_length)
2026-01-22 23:16:00.371 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:00.380 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:00.381 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(input_ids, inputs_embeds):
    if (input_ids is None) ^ (inputs_embeds is not None):
        raise ValueError('You must specify exactly one of input_ids or inputs_embeds')
    return ()
2026-01-22 23:16:00.381 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:00.381 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:00.381 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_1(use_cache):
    if use_cache:
        logger.warning_once('`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...')
        use_cache = False
    return (use_cache,)
2026-01-22 23:16:00.381 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:00.382 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:00.382 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_2(past_key_values, use_cache):
    if use_cache and past_key_values is None and (not torch.jit.is_tracing()):
        past_key_values = DynamicCache()
    return (past_key_values,)
2026-01-22 23:16:00.382 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:00.382 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:00.383 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_3(input_ids, inputs_embeds):
    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)
    return (inputs_embeds,)
2026-01-22 23:16:00.383 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:00.383 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:00.383 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_4(cache_position, inputs_embeds, past_key_values, *, past_seen_tokens=None):
    if cache_position is None:
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        cache_position = torch.arange(past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device)
    return (cache_position, past_seen_tokens)
2026-01-22 23:16:00.383 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:00.384 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:00.384 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_5(cache_position, inputs_embeds, position_ids):
    if position_ids is None:
        position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)
    elif position_ids.ndim == 2:
        position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)
    return (position_ids,)
2026-01-22 23:16:00.385 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:00.385 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:00.385 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_6(position_ids):
    if position_ids.ndim == 3 and position_ids.shape[0] == 4:
        text_position_ids = position_ids[0]
        position_ids = position_ids[1:]
    else:
        text_position_ids = position_ids[0]
    return (position_ids, text_position_ids)
2026-01-22 23:16:00.385 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:00.386 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:00.386 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_7(attention_mask, cache_position, inputs_embeds, past_key_values, text_position_ids, *, causal_mask_mapping=None, mask_kwargs=None):
    if not isinstance((causal_mask_mapping := attention_mask), dict):
        mask_kwargs = {'config': self.config, 'input_embeds': inputs_embeds, 'attention_mask': attention_mask, 'cache_position': cache_position, 'past_key_values': past_key_values, 'position_ids': text_position_ids}
        causal_mask_mapping = {'full_attention': create_causal_mask(**mask_kwargs)}
        if self.has_sliding_layers:
            causal_mask_mapping['sliding_attention'] = create_sliding_window_causal_mask(**mask_kwargs)
    return (causal_mask_mapping, mask_kwargs)
2026-01-22 23:16:00.387 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:00.387 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:00.387 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_8(all_hidden_states, hidden_states, output_hidden_states):
    if output_hidden_states:
        all_hidden_states += (hidden_states,)
    return (all_hidden_states,)
2026-01-22 23:16:00.387 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:00.388 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:00.388 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_9(all_self_attns, layer_outputs, output_attentions):
    if output_attentions:
        all_self_attns += (layer_outputs[1],)
    return (all_self_attns,)
2026-01-22 23:16:00.388 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:00.388 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:00.388 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_10(all_hidden_states, hidden_states, output_hidden_states):
    if output_hidden_states:
        all_hidden_states += (hidden_states,)
    return (all_hidden_states,)
2026-01-22 23:16:00.389 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:00.913 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.CALIBRATION_EPOCH_START
2026-01-22 23:16:00.913 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4a5610>
2026-01-22 23:16:00.913 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bd340>
2026-01-22 23:16:00.913 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bd3d0>
2026-01-22 23:16:00.913 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bd490>
2026-01-22 23:16:00.914 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bd4f0>
2026-01-22 23:16:00.914 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bd580>
2026-01-22 23:16:00.914 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bd610>
2026-01-22 23:16:00.914 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bd6a0>
2026-01-22 23:16:00.914 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bd730>
2026-01-22 23:16:00.914 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bd7f0>
2026-01-22 23:16:00.914 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bd880>
2026-01-22 23:16:00.914 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bd910>
2026-01-22 23:16:00.915 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bd9d0>
2026-01-22 23:16:00.915 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bda90>
2026-01-22 23:16:00.915 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bdb50>
2026-01-22 23:16:00.915 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bdc10>
2026-01-22 23:16:00.915 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bdcd0>
2026-01-22 23:16:00.915 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a3470>
2026-01-22 23:16:00.915 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f268c10b3b0>
2026-01-22 23:16:00.915 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f268c10b230>
2026-01-22 23:16:00.915 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bdeb0>
2026-01-22 23:16:00.916 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bdf40>
2026-01-22 23:16:00.916 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4be000>
2026-01-22 23:16:00.916 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4be090>
2026-01-22 23:16:00.916 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4be150>
2026-01-22 23:16:00.916 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4be240>
2026-01-22 23:16:00.916 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4be330>
2026-01-22 23:16:00.916 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4be3f0>
2026-01-22 23:16:00.916 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4be4b0>
2026-01-22 23:16:00.917 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4be570>
2026-01-22 23:16:00.917 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4be660>
2026-01-22 23:16:00.917 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4be750>
2026-01-22 23:16:00.917 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4be810>
2026-01-22 23:16:00.917 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4be900>
2026-01-22 23:16:00.917 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4be9f0>
2026-01-22 23:16:00.917 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4beae0>
2026-01-22 23:16:00.917 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bebd0>
2026-01-22 23:16:00.918 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bec90>
2026-01-22 23:16:00.918 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bed50>
2026-01-22 23:16:00.918 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f268d3f0cb0>
2026-01-22 23:16:00.918 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4beea0>
2026-01-22 23:16:00.918 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bef60>
2026-01-22 23:16:00.918 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bf050>
2026-01-22 23:16:00.918 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bf140>
2026-01-22 23:16:00.918 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bf230>
2026-01-22 23:16:00.918 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bf320>
2026-01-22 23:16:00.919 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bf3e0>
2026-01-22 23:16:00.919 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bf4d0>
2026-01-22 23:16:00.919 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bf5c0>
2026-01-22 23:16:00.919 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bf6b0>
2026-01-22 23:16:00.919 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bf7a0>
2026-01-22 23:16:00.919 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bf890>
2026-01-22 23:16:00.919 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bf950>
2026-01-22 23:16:00.919 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bfa10>
2026-01-22 23:16:00.919 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bfad0>
2026-01-22 23:16:00.920 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bfbc0>
2026-01-22 23:16:00.920 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bfcb0>
2026-01-22 23:16:00.920 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bfda0>
2026-01-22 23:16:00.920 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bfe90>
2026-01-22 23:16:00.920 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bff80>
2026-01-22 23:16:00.920 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bfe00>
2026-01-22 23:16:00.920 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265ee30140>
2026-01-22 23:16:00.920 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265ee30170>
2026-01-22 23:16:00.920 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265ee30290>
2026-01-22 23:16:00.921 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265ee30380>
2026-01-22 23:16:00.921 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265ee303b0>
2026-01-22 23:16:00.921 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265ee304d0>
2026-01-22 23:16:00.921 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265ee30590>
2026-01-22 23:16:00.921 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265ee30650>
2026-01-22 23:16:00.921 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265ee30740>
2026-01-22 23:16:00.921 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265ee30830>
2026-01-22 23:16:00.921 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f265ee308f0>
2026-01-22 23:16:00.921 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:24.739 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:24.739 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.0.input_layernorm
2026-01-22 23:16:24.776 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.0.post_attention_layernorm
2026-01-22 23:16:24.777 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:28.880 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:28.880 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.1.input_layernorm
2026-01-22 23:16:28.906 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.1.post_attention_layernorm
2026-01-22 23:16:28.908 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:29.429 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:29.429 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.2.input_layernorm
2026-01-22 23:16:29.455 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.2.post_attention_layernorm
2026-01-22 23:16:29.457 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:29.970 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:29.971 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.3.input_layernorm
2026-01-22 23:16:29.997 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.3.post_attention_layernorm
2026-01-22 23:16:29.998 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:30.514 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:30.514 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.4.input_layernorm
2026-01-22 23:16:30.541 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.4.post_attention_layernorm
2026-01-22 23:16:30.542 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:31.048 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:31.048 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.5.input_layernorm
2026-01-22 23:16:31.074 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.5.post_attention_layernorm
2026-01-22 23:16:31.076 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:31.577 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:31.577 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.6.input_layernorm
2026-01-22 23:16:31.604 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.6.post_attention_layernorm
2026-01-22 23:16:31.605 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:32.116 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:32.116 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.7.input_layernorm
2026-01-22 23:16:32.143 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.7.post_attention_layernorm
2026-01-22 23:16:32.144 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:32.648 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:32.649 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.8.input_layernorm
2026-01-22 23:16:32.675 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.8.post_attention_layernorm
2026-01-22 23:16:32.677 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:33.177 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:33.178 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.9.input_layernorm
2026-01-22 23:16:33.204 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.9.post_attention_layernorm
2026-01-22 23:16:33.205 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:33.722 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:33.722 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.10.input_layernorm
2026-01-22 23:16:33.749 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.10.post_attention_layernorm
2026-01-22 23:16:33.750 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:34.252 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:34.252 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.11.input_layernorm
2026-01-22 23:16:34.279 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.11.post_attention_layernorm
2026-01-22 23:16:34.280 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:34.785 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:34.785 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.12.input_layernorm
2026-01-22 23:16:34.812 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.12.post_attention_layernorm
2026-01-22 23:16:34.813 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:35.325 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:35.325 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.13.input_layernorm
2026-01-22 23:16:35.352 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.13.post_attention_layernorm
2026-01-22 23:16:35.353 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:35.852 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:35.852 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.14.input_layernorm
2026-01-22 23:16:35.879 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.14.post_attention_layernorm
2026-01-22 23:16:35.880 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:36.379 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:36.379 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.15.input_layernorm
2026-01-22 23:16:36.405 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.15.post_attention_layernorm
2026-01-22 23:16:36.407 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:36.901 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:36.902 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.16.input_layernorm
2026-01-22 23:16:36.928 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.16.post_attention_layernorm
2026-01-22 23:16:36.930 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:37.445 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:37.445 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.17.input_layernorm
2026-01-22 23:16:37.472 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.17.post_attention_layernorm
2026-01-22 23:16:37.473 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:37.989 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:37.989 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.18.input_layernorm
2026-01-22 23:16:38.016 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.18.post_attention_layernorm
2026-01-22 23:16:38.017 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:38.526 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:38.526 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.19.input_layernorm
2026-01-22 23:16:38.552 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.19.post_attention_layernorm
2026-01-22 23:16:38.554 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:39.071 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:39.072 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.20.input_layernorm
2026-01-22 23:16:39.098 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.20.post_attention_layernorm
2026-01-22 23:16:39.100 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:39.604 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:39.604 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.21.input_layernorm
2026-01-22 23:16:39.631 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.21.post_attention_layernorm
2026-01-22 23:16:39.632 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:40.154 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:40.155 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.22.input_layernorm
2026-01-22 23:16:40.182 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.22.post_attention_layernorm
2026-01-22 23:16:40.183 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:40.683 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:40.683 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.23.input_layernorm
2026-01-22 23:16:40.710 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.23.post_attention_layernorm
2026-01-22 23:16:40.711 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:41.217 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:41.217 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.24.input_layernorm
2026-01-22 23:16:41.244 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.24.post_attention_layernorm
2026-01-22 23:16:41.245 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:41.745 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:41.745 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.25.input_layernorm
2026-01-22 23:16:41.772 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.25.post_attention_layernorm
2026-01-22 23:16:41.774 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:42.273 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:42.273 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.26.input_layernorm
2026-01-22 23:16:42.300 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.26.post_attention_layernorm
2026-01-22 23:16:42.301 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:42.805 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:42.805 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.27.input_layernorm
2026-01-22 23:16:42.831 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.27.post_attention_layernorm
2026-01-22 23:16:42.833 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:43.337 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:43.337 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.28.input_layernorm
2026-01-22 23:16:43.364 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.28.post_attention_layernorm
2026-01-22 23:16:43.365 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:43.870 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:43.870 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.29.input_layernorm
2026-01-22 23:16:43.896 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.29.post_attention_layernorm
2026-01-22 23:16:43.898 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:44.400 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:44.400 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.30.input_layernorm
2026-01-22 23:16:44.427 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.30.post_attention_layernorm
2026-01-22 23:16:44.428 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:44.930 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:44.930 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.31.input_layernorm
2026-01-22 23:16:44.957 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.31.post_attention_layernorm
2026-01-22 23:16:44.958 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:45.507 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:45.507 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.32.input_layernorm
2026-01-22 23:16:45.533 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.32.post_attention_layernorm
2026-01-22 23:16:45.535 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:46.043 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:46.044 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.33.input_layernorm
2026-01-22 23:16:46.070 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.33.post_attention_layernorm
2026-01-22 23:16:46.071 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:46.577 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:46.577 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.34.input_layernorm
2026-01-22 23:16:46.603 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.34.post_attention_layernorm
2026-01-22 23:16:46.604 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:47.120 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:47.120 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.35.input_layernorm
2026-01-22 23:16:47.147 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.language_model.layers.35.post_attention_layernorm
2026-01-22 23:16:47.148 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:47.915 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:16:47.915 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:49.085 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.CALIBRATION_EPOCH_END
2026-01-22 23:16:49.085 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=True smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:16:49.093 | INFO     | llmcompressor.pipelines.independent.pipeline:IndependentPipeline:43 - Inferred `SequentialPipeline` for `GPTQModifier`
2026-01-22 23:16:56.720 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:56.721 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(labels, logits, loss):
    if labels is not None:
        loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size)
    return (loss,)
2026-01-22 23:16:56.721 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:56.728 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:56.728 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(input_ids, inputs_embeds):
    if inputs_embeds is None:
        inputs_embeds = self.get_input_embeddings()(input_ids)
    return (inputs_embeds,)
2026-01-22 23:16:56.728 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:56.729 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:56.729 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_1(image_grid_thw, input_ids, inputs_embeds, pixel_values, *, _=None, image_embeds=None, image_mask=None):
    if pixel_values is not None:
        image_embeds = self.get_image_features(pixel_values, image_grid_thw)
        image_embeds = torch.cat(image_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)
        image_mask, _ = self.get_placeholder_mask(input_ids, inputs_embeds=inputs_embeds, image_features=image_embeds)
        inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)
    return (_, image_embeds, image_mask, inputs_embeds)
2026-01-22 23:16:56.729 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:56.730 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:56.730 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_2(_, input_ids, inputs_embeds, pixel_values_videos, video_grid_thw, *, video_embeds=None, video_mask=None):
    if pixel_values_videos is not None:
        video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)
        video_embeds = torch.cat(video_embeds, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)
        _, video_mask = self.get_placeholder_mask(input_ids, inputs_embeds=inputs_embeds, video_features=video_embeds)
        inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)
    return (_, inputs_embeds, video_embeds, video_mask)
2026-01-22 23:16:56.730 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:56.732 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:56.733 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_3(_, attention_mask, cache_position, image_grid_thw, input_ids, inputs_embeds, past_key_values, position_ids, rope_deltas, second_per_grid_ts, video_grid_thw, *, batch_size=None, delta=None, prefill_compiled_stage=None, prefill_noncompiled_stage=None, seq_length=None):
    if position_ids is None:
        prefill_compiled_stage = is_torchdynamo_compiling() and (input_ids is not None and input_ids.shape[1] != 1 or (inputs_embeds is not None and inputs_embeds.shape[1] != 1))
        prefill_noncompiled_stage = not is_torchdynamo_compiling() and (cache_position is not None and cache_position[0] == 0 or (past_key_values is None or past_key_values.get_seq_length() == 0))
        if (prefill_compiled_stage or prefill_noncompiled_stage) or self.rope_deltas is None:
            position_ids, rope_deltas = self.get_rope_index(input_ids, image_grid_thw, video_grid_thw, second_per_grid_ts=second_per_grid_ts, attention_mask=attention_mask)
            self.rope_deltas = rope_deltas
        else:
            batch_size, seq_length, _ = inputs_embeds.shape
            position_ids = torch.arange(seq_length, device=inputs_embeds.device)
            position_ids = position_ids.view(1, 1, -1).expand(3, batch_size, -1)
            if cache_position is not None:
                delta = (cache_position[0] + self.rope_deltas).to(inputs_embeds.device)
            else:
                delta = torch.zeros((batch_size, seq_length), device=inputs_embeds.device)
            delta = delta.repeat_interleave(batch_size // delta.shape[0], dim=1)
            position_ids += delta.to(position_ids.device)
    return (_, batch_size, delta, position_ids, prefill_compiled_stage, prefill_noncompiled_stage, rope_deltas, seq_length)
2026-01-22 23:16:56.733 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:56.742 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:56.743 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(input_ids, inputs_embeds):
    if (input_ids is None) ^ (inputs_embeds is not None):
        raise ValueError('You must specify exactly one of input_ids or inputs_embeds')
    return ()
2026-01-22 23:16:56.743 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:56.743 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:56.743 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_1(use_cache):
    if use_cache:
        logger.warning_once('`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...')
        use_cache = False
    return (use_cache,)
2026-01-22 23:16:56.743 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:56.744 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:56.744 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_2(past_key_values, use_cache):
    if use_cache and past_key_values is None and (not torch.jit.is_tracing()):
        past_key_values = DynamicCache()
    return (past_key_values,)
2026-01-22 23:16:56.744 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:56.744 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:56.744 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_3(input_ids, inputs_embeds):
    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)
    return (inputs_embeds,)
2026-01-22 23:16:56.745 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:56.745 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:56.745 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_4(cache_position, inputs_embeds, past_key_values, *, past_seen_tokens=None):
    if cache_position is None:
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        cache_position = torch.arange(past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device)
    return (cache_position, past_seen_tokens)
2026-01-22 23:16:56.745 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:56.746 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:56.746 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_5(cache_position, inputs_embeds, position_ids):
    if position_ids is None:
        position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)
    elif position_ids.ndim == 2:
        position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)
    return (position_ids,)
2026-01-22 23:16:56.746 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:56.747 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:56.747 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_6(position_ids):
    if position_ids.ndim == 3 and position_ids.shape[0] == 4:
        text_position_ids = position_ids[0]
        position_ids = position_ids[1:]
    else:
        text_position_ids = position_ids[0]
    return (position_ids, text_position_ids)
2026-01-22 23:16:56.747 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:56.748 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:56.748 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_7(attention_mask, cache_position, inputs_embeds, past_key_values, text_position_ids, *, causal_mask_mapping=None, mask_kwargs=None):
    if not isinstance((causal_mask_mapping := attention_mask), dict):
        mask_kwargs = {'config': self.config, 'input_embeds': inputs_embeds, 'attention_mask': attention_mask, 'cache_position': cache_position, 'past_key_values': past_key_values, 'position_ids': text_position_ids}
        causal_mask_mapping = {'full_attention': create_causal_mask(**mask_kwargs)}
        if self.has_sliding_layers:
            causal_mask_mapping['sliding_attention'] = create_sliding_window_causal_mask(**mask_kwargs)
    return (causal_mask_mapping, mask_kwargs)
2026-01-22 23:16:56.748 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:56.749 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:56.749 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_8(all_hidden_states, hidden_states, output_hidden_states):
    if output_hidden_states:
        all_hidden_states += (hidden_states,)
    return (all_hidden_states,)
2026-01-22 23:16:56.749 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:56.749 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:56.750 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_9(all_self_attns, layer_outputs, output_attentions):
    if output_attentions:
        all_self_attns += (layer_outputs[1],)
    return (all_self_attns,)
2026-01-22 23:16:56.750 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:56.750 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:16:56.750 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_10(all_hidden_states, hidden_states, output_hidden_states):
    if output_hidden_states:
        all_hidden_states += (hidden_states,)
    return (all_hidden_states,)
2026-01-22 23:16:56.750 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:16:57.271 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.CALIBRATION_EPOCH_START
2026-01-22 23:16:57.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4ed0d0>
2026-01-22 23:16:57.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c154950>
2026-01-22 23:16:57.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c155340>
2026-01-22 23:16:57.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268d3f10d0>
2026-01-22 23:16:57.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee58e30>
2026-01-22 23:16:57.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee595b0>
2026-01-22 23:16:57.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4bcc80>
2026-01-22 23:16:57.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee51430>
2026-01-22 23:16:57.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee51970>
2026-01-22 23:16:57.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4a4380>
2026-01-22 23:16:57.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4a6240>
2026-01-22 23:16:57.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4a4140>
2026-01-22 23:16:57.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4a7440>
2026-01-22 23:16:57.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4a4a10>
2026-01-22 23:16:57.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4a7950>
2026-01-22 23:16:57.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4a4740>
2026-01-22 23:16:57.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4a5430>
2026-01-22 23:16:57.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4a5010>
2026-01-22 23:16:57.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4a4770>
2026-01-22 23:16:57.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4a50d0>
2026-01-22 23:16:57.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c1578c0>
2026-01-22 23:16:57.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4a4950>
2026-01-22 23:16:57.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4a5040>
2026-01-22 23:16:57.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0c140>
2026-01-22 23:16:57.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0c1d0>
2026-01-22 23:16:57.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0c320>
2026-01-22 23:16:57.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0c290>
2026-01-22 23:16:57.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0c530>
2026-01-22 23:16:57.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0c620>
2026-01-22 23:16:57.295 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0c5f0>
2026-01-22 23:16:57.295 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee30680>
2026-01-22 23:16:57.295 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0c710>
2026-01-22 23:16:57.295 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0c8f0>
2026-01-22 23:16:57.295 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0caa0>
2026-01-22 23:16:57.295 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0c9e0>
2026-01-22 23:16:57.295 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0cce0>
2026-01-22 23:16:57.296 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0cc50>
2026-01-22 23:16:57.296 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0ce00>
2026-01-22 23:16:57.296 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0c890>
2026-01-22 23:16:57.296 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0d370>
2026-01-22 23:16:57.296 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0d1f0>
2026-01-22 23:16:57.296 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0d580>
2026-01-22 23:16:57.296 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0d1c0>
2026-01-22 23:16:57.297 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0d520>
2026-01-22 23:16:57.297 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0d910>
2026-01-22 23:16:57.297 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0d8b0>
2026-01-22 23:16:57.297 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0cf50>
2026-01-22 23:16:57.297 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0dc10>
2026-01-22 23:16:57.297 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0d820>
2026-01-22 23:16:57.297 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0d010>
2026-01-22 23:16:57.298 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0db20>
2026-01-22 23:16:57.298 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0de20>
2026-01-22 23:16:57.298 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0c410>
2026-01-22 23:16:57.298 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0e480>
2026-01-22 23:16:57.298 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0e570>
2026-01-22 23:16:57.298 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0e210>
2026-01-22 23:16:57.298 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0e270>
2026-01-22 23:16:57.298 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4ef3e0>
2026-01-22 23:16:57.299 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0e510>
2026-01-22 23:16:57.299 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0e8a0>
2026-01-22 23:16:57.299 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0e750>
2026-01-22 23:16:57.299 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0e300>
2026-01-22 23:16:57.299 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0ec60>
2026-01-22 23:16:57.299 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0e0c0>
2026-01-22 23:16:57.299 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0eae0>
2026-01-22 23:16:57.300 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0ee10>
2026-01-22 23:16:57.300 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0ee70>
2026-01-22 23:16:57.300 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0fd10>
2026-01-22 23:16:57.300 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0dbe0>
2026-01-22 23:16:57.300 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0fdd0>
2026-01-22 23:16:57.300 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0ed20>
2026-01-22 23:16:57.300 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0eff0>
2026-01-22 23:16:57.300 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4a4fb0>
2026-01-22 23:16:57.301 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0ef90>
2026-01-22 23:16:57.301 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0f1a0>
2026-01-22 23:16:57.301 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0f890>
2026-01-22 23:16:57.301 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee54140>
2026-01-22 23:16:57.301 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee57830>
2026-01-22 23:16:57.301 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee0f0b0>
2026-01-22 23:16:57.301 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee54e00>
2026-01-22 23:16:57.302 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee54b00>
2026-01-22 23:16:57.302 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee54f20>
2026-01-22 23:16:57.302 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee550a0>
2026-01-22 23:16:57.302 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee55160>
2026-01-22 23:16:57.302 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee55190>
2026-01-22 23:16:57.302 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee552b0>
2026-01-22 23:16:57.302 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee55310>
2026-01-22 23:16:57.303 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee54e30>
2026-01-22 23:16:57.303 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee55250>
2026-01-22 23:16:57.303 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee553a0>
2026-01-22 23:16:57.303 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee55670>
2026-01-22 23:16:57.303 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee554f0>
2026-01-22 23:16:57.303 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee55850>
2026-01-22 23:16:57.303 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee554c0>
2026-01-22 23:16:57.304 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee55790>
2026-01-22 23:16:57.304 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee55490>
2026-01-22 23:16:57.304 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee55a90>
2026-01-22 23:16:57.304 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee55c40>
2026-01-22 23:16:57.304 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee55d00>
2026-01-22 23:16:57.304 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee55b80>
2026-01-22 23:16:57.304 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee55ee0>
2026-01-22 23:16:57.304 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee55eb0>
2026-01-22 23:16:57.305 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee55ca0>
2026-01-22 23:16:57.305 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee560f0>
2026-01-22 23:16:57.305 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee56150>
2026-01-22 23:16:57.305 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee55b20>
2026-01-22 23:16:57.305 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee562d0>
2026-01-22 23:16:57.305 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee56420>
2026-01-22 23:16:57.305 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee564e0>
2026-01-22 23:16:57.306 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee562a0>
2026-01-22 23:16:57.306 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee561e0>
2026-01-22 23:16:57.306 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee56660>
2026-01-22 23:16:57.306 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee56330>
2026-01-22 23:16:57.306 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee567e0>
2026-01-22 23:16:57.306 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee56750>
2026-01-22 23:16:57.306 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee56720>
2026-01-22 23:16:57.306 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee568a0>
2026-01-22 23:16:57.307 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee565a0>
2026-01-22 23:16:57.307 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee56cc0>
2026-01-22 23:16:57.307 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee566f0>
2026-01-22 23:16:57.307 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee56ea0>
2026-01-22 23:16:57.307 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee56b10>
2026-01-22 23:16:57.307 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4ec3b0>
2026-01-22 23:16:57.307 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee56c90>
2026-01-22 23:16:57.308 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee56ae0>
2026-01-22 23:16:57.308 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee56db0>
2026-01-22 23:16:57.308 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee56c30>
2026-01-22 23:16:57.308 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee56cf0>
2026-01-22 23:16:57.308 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee57380>
2026-01-22 23:16:57.308 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee571a0>
2026-01-22 23:16:57.308 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee57320>
2026-01-22 23:16:57.309 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee575c0>
2026-01-22 23:16:57.309 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee54860>
2026-01-22 23:16:57.309 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee57fb0>
2026-01-22 23:16:57.309 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee57da0>
2026-01-22 23:16:57.309 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee57f20>
2026-01-22 23:16:57.309 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee57620>
2026-01-22 23:16:57.309 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee57c50>
2026-01-22 23:16:57.309 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee545c0>
2026-01-22 23:16:57.310 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee54740>
2026-01-22 23:16:57.310 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee57d10>
2026-01-22 23:16:57.310 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee54560>
2026-01-22 23:16:57.310 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c156210>
2026-01-22 23:16:57.310 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee54620>
2026-01-22 23:16:57.310 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee54bf0>
2026-01-22 23:16:57.310 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee54170>
2026-01-22 23:16:57.311 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee57920>
2026-01-22 23:16:57.311 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee54410>
2026-01-22 23:16:57.311 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee54440>
2026-01-22 23:16:57.311 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee57860>
2026-01-22 23:16:57.311 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee32a80>
2026-01-22 23:16:57.311 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee57650>
2026-01-22 23:16:57.311 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee54bc0>
2026-01-22 23:16:57.312 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee543e0>
2026-01-22 23:16:57.312 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee57950>
2026-01-22 23:16:57.312 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee547d0>
2026-01-22 23:16:57.312 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee577a0>
2026-01-22 23:16:57.312 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a3260>
2026-01-22 23:16:57.312 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a2ff0>
2026-01-22 23:16:57.312 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a20f0>
2026-01-22 23:16:57.313 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a30e0>
2026-01-22 23:16:57.313 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a25a0>
2026-01-22 23:16:57.313 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a3e30>
2026-01-22 23:16:57.313 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a0ec0>
2026-01-22 23:16:57.313 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a3d40>
2026-01-22 23:16:57.313 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a13d0>
2026-01-22 23:16:57.313 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a15e0>
2026-01-22 23:16:57.313 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a1730>
2026-01-22 23:16:57.314 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a17f0>
2026-01-22 23:16:57.314 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a14f0>
2026-01-22 23:16:57.314 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a3290>
2026-01-22 23:16:57.314 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a2570>
2026-01-22 23:16:57.314 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a3c50>
2026-01-22 23:16:57.314 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a3830>
2026-01-22 23:16:57.314 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a0f20>
2026-01-22 23:16:57.315 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a2e70>
2026-01-22 23:16:57.315 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a3e60>
2026-01-22 23:16:57.315 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a1220>
2026-01-22 23:16:57.315 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a1190>
2026-01-22 23:16:57.315 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a3f80>
2026-01-22 23:16:57.315 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a27b0>
2026-01-22 23:16:57.315 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a1640>
2026-01-22 23:16:57.316 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee59b80>
2026-01-22 23:16:57.316 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a1a00>
2026-01-22 23:16:57.316 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a22a0>
2026-01-22 23:16:57.316 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a1310>
2026-01-22 23:16:57.316 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a03e0>
2026-01-22 23:16:57.316 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a0350>
2026-01-22 23:16:57.316 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee5bef0>
2026-01-22 23:16:57.317 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a0530>
2026-01-22 23:16:57.317 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a03b0>
2026-01-22 23:16:57.317 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a02f0>
2026-01-22 23:16:57.317 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a00e0>
2026-01-22 23:16:57.317 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a32c0>
2026-01-22 23:16:57.317 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a2270>
2026-01-22 23:16:57.318 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a21b0>
2026-01-22 23:16:57.318 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a3140>
2026-01-22 23:16:57.318 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a3770>
2026-01-22 23:16:57.318 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a3380>
2026-01-22 23:16:57.318 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f268c2a2330>
2026-01-22 23:16:57.318 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee64da0>
2026-01-22 23:16:57.318 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee65d60>
2026-01-22 23:16:57.319 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee64b60>
2026-01-22 23:16:57.319 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee64d70>
2026-01-22 23:16:57.319 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee66600>
2026-01-22 23:16:57.319 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee66750>
2026-01-22 23:16:57.319 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee65400>
2026-01-22 23:16:57.319 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee670b0>
2026-01-22 23:16:57.319 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee66450>
2026-01-22 23:16:57.320 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee66870>
2026-01-22 23:16:57.320 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee65b50>
2026-01-22 23:16:57.320 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee65f10>
2026-01-22 23:16:57.320 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee64f20>
2026-01-22 23:16:57.320 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee662d0>
2026-01-22 23:16:57.320 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee66e40>
2026-01-22 23:16:57.320 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee66270>
2026-01-22 23:16:57.321 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee65af0>
2026-01-22 23:16:57.321 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee66fc0>
2026-01-22 23:16:57.321 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee67260>
2026-01-22 23:16:57.321 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee65fa0>
2026-01-22 23:16:57.321 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee64b30>
2026-01-22 23:16:57.321 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265f4ec3e0>
2026-01-22 23:16:57.321 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee66c00>
2026-01-22 23:16:57.321 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee66300>
2026-01-22 23:16:57.322 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee66180>
2026-01-22 23:16:57.322 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee65940>
2026-01-22 23:16:57.322 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee64bf0>
2026-01-22 23:16:57.322 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee663f0>
2026-01-22 23:16:57.322 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee66b40>
2026-01-22 23:16:57.322 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee66c30>
2026-01-22 23:16:57.322 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee65bb0>
2026-01-22 23:16:57.323 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee66930>
2026-01-22 23:16:57.323 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee66c60>
2026-01-22 23:16:57.323 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee66e10>
2026-01-22 23:16:57.323 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee66f60>
2026-01-22 23:16:57.323 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee67020>
2026-01-22 23:16:57.323 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee665a0>
2026-01-22 23:16:57.323 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee64ce0>
2026-01-22 23:16:57.323 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee673b0>
2026-01-22 23:16:57.324 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee67440>
2026-01-22 23:16:57.324 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee67500>
2026-01-22 23:16:57.324 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee67530>
2026-01-22 23:16:57.324 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee673e0>
2026-01-22 23:16:57.324 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee67770>
2026-01-22 23:16:57.324 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee67890>
2026-01-22 23:16:57.324 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee67920>
2026-01-22 23:16:57.325 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee67a40>
2026-01-22 23:16:57.325 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee67b00>
2026-01-22 23:16:57.325 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee67b30>
2026-01-22 23:16:57.325 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee67c80>
2026-01-22 23:16:57.325 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee67d10>
2026-01-22 23:16:57.325 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f265ee67dd0>
2026-01-22 23:16:57.325 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:17:20.730 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:20.730 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.0.self_attn.q_proj using 3 samples
2026-01-22 23:17:22.435 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.70s
2026-01-22 23:17:22.437 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 11435.93
2026-01-22 23:17:22.437 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 46.69% | total memory: 25 GB
2026-01-22 23:17:22.437 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:17:22.438 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.0.self_attn.k_proj using 3 samples
2026-01-22 23:17:24.140 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.70s
2026-01-22 23:17:24.141 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1409.94
2026-01-22 23:17:24.141 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 47.96% | total memory: 25 GB
2026-01-22 23:17:24.141 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:17:24.142 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.0.self_attn.v_proj using 3 samples
2026-01-22 23:17:25.780 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.64s
2026-01-22 23:17:25.781 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 322.02
2026-01-22 23:17:25.781 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 47.96% | total memory: 25 GB
2026-01-22 23:17:25.781 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:17:25.782 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.0.self_attn.o_proj using 3 samples
2026-01-22 23:17:27.428 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.65s
2026-01-22 23:17:27.429 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 199.51
2026-01-22 23:17:27.429 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 47.96% | total memory: 25 GB
2026-01-22 23:17:27.430 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:17:27.430 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.0.mlp.gate_proj using 3 samples
2026-01-22 23:17:29.195 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.76s
2026-01-22 23:17:29.196 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 9766.46
2026-01-22 23:17:29.196 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 47.96% | total memory: 25 GB
2026-01-22 23:17:29.196 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:17:29.197 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.0.mlp.up_proj using 3 samples
2026-01-22 23:17:30.832 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.64s
2026-01-22 23:17:30.832 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 7467.80
2026-01-22 23:17:30.833 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 44.88% | total memory: 25 GB
2026-01-22 23:17:30.833 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:17:30.833 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.0.mlp.down_proj using 3 samples
2026-01-22 23:17:39.893 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 9.06s
2026-01-22 23:17:39.898 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 553.76
2026-01-22 23:17:39.898 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 46.14% | total memory: 25 GB
2026-01-22 23:17:39.898 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:17:39.898 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:17:44.293 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:44.294 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.1.self_attn.q_proj using 3 samples
2026-01-22 23:17:45.993 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.70s
2026-01-22 23:17:45.994 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 3035.79
2026-01-22 23:17:45.994 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.54% | total memory: 25 GB
2026-01-22 23:17:45.995 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:17:45.995 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.1.self_attn.k_proj using 3 samples
2026-01-22 23:17:47.622 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:17:47.623 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 850.78
2026-01-22 23:17:47.624 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.54% | total memory: 25 GB
2026-01-22 23:17:47.624 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:17:47.624 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.1.self_attn.v_proj using 3 samples
2026-01-22 23:17:49.230 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.61s
2026-01-22 23:17:49.231 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 64.05
2026-01-22 23:17:49.231 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.55% | total memory: 25 GB
2026-01-22 23:17:49.231 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:17:49.232 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.1.self_attn.o_proj using 3 samples
2026-01-22 23:17:50.926 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.69s
2026-01-22 23:17:50.927 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 77.32
2026-01-22 23:17:50.927 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.80% | total memory: 25 GB
2026-01-22 23:17:50.927 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:17:50.928 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.1.mlp.gate_proj using 3 samples
2026-01-22 23:17:52.678 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.75s
2026-01-22 23:17:52.679 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1841629.50
2026-01-22 23:17:52.679 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.80% | total memory: 25 GB
2026-01-22 23:17:52.679 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:17:52.680 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.1.mlp.up_proj using 3 samples
2026-01-22 23:17:54.441 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.76s
2026-01-22 23:17:54.442 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 862173.62
2026-01-22 23:17:54.443 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.80% | total memory: 25 GB
2026-01-22 23:17:54.443 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:17:54.443 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.1.mlp.down_proj using 3 samples
2026-01-22 23:18:03.413 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 8.97s
2026-01-22 23:18:03.417 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 81.66
2026-01-22 23:18:03.418 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.56% | total memory: 25 GB
2026-01-22 23:18:03.418 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:18:03.418 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:18:03.983 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:18:03.983 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.2.self_attn.q_proj using 3 samples
2026-01-22 23:18:05.695 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.71s
2026-01-22 23:18:05.696 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 4847.51
2026-01-22 23:18:05.696 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.90% | total memory: 25 GB
2026-01-22 23:18:05.697 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:18:05.697 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.2.self_attn.k_proj using 3 samples
2026-01-22 23:18:07.330 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:18:07.332 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1121.23
2026-01-22 23:18:07.332 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.90% | total memory: 25 GB
2026-01-22 23:18:07.332 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:18:07.332 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.2.self_attn.v_proj using 3 samples
2026-01-22 23:18:08.966 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:18:08.967 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 82.03
2026-01-22 23:18:08.967 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.90% | total memory: 25 GB
2026-01-22 23:18:08.967 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:18:08.968 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.2.self_attn.o_proj using 3 samples
2026-01-22 23:18:10.584 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.62s
2026-01-22 23:18:10.585 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 97.72
2026-01-22 23:18:10.585 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.63% | total memory: 25 GB
2026-01-22 23:18:10.586 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:18:10.586 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.2.mlp.gate_proj using 3 samples
2026-01-22 23:18:12.336 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.75s
2026-01-22 23:18:12.338 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 379965.12
2026-01-22 23:18:12.338 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:12.338 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:18:12.339 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.2.mlp.up_proj using 3 samples
2026-01-22 23:18:14.088 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.75s
2026-01-22 23:18:14.089 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 75184.04
2026-01-22 23:18:14.090 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:14.090 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:18:14.090 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.2.mlp.down_proj using 3 samples
2026-01-22 23:18:23.143 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 9.05s
2026-01-22 23:18:23.148 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 25895.52
2026-01-22 23:18:23.148 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.90% | total memory: 25 GB
2026-01-22 23:18:23.149 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:18:23.149 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:18:23.706 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:18:23.707 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.3.self_attn.q_proj using 3 samples
2026-01-22 23:18:25.413 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.71s
2026-01-22 23:18:25.414 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 9597.63
2026-01-22 23:18:25.415 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:25.415 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:18:25.415 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.3.self_attn.k_proj using 3 samples
2026-01-22 23:18:27.047 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:18:27.048 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1923.50
2026-01-22 23:18:27.048 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:27.049 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:18:27.049 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.3.self_attn.v_proj using 3 samples
2026-01-22 23:18:28.655 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.61s
2026-01-22 23:18:28.656 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 330.97
2026-01-22 23:18:28.656 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:28.656 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:18:28.657 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.3.self_attn.o_proj using 3 samples
2026-01-22 23:18:30.272 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.61s
2026-01-22 23:18:30.273 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 132.72
2026-01-22 23:18:30.273 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:30.274 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:18:30.274 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.3.mlp.gate_proj using 3 samples
2026-01-22 23:18:32.033 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.76s
2026-01-22 23:18:32.035 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 576769.94
2026-01-22 23:18:32.035 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.89% | total memory: 25 GB
2026-01-22 23:18:32.035 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:18:32.035 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.3.mlp.up_proj using 3 samples
2026-01-22 23:18:33.788 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.75s
2026-01-22 23:18:33.789 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 87178.03
2026-01-22 23:18:33.789 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.89% | total memory: 25 GB
2026-01-22 23:18:33.789 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:18:33.790 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.3.mlp.down_proj using 3 samples
2026-01-22 23:18:42.741 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 8.95s
2026-01-22 23:18:42.746 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1094.28
2026-01-22 23:18:42.746 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:42.746 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:18:42.746 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:18:43.314 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:18:43.314 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.4.self_attn.q_proj using 3 samples
2026-01-22 23:18:45.071 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.76s
2026-01-22 23:18:45.073 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 10099.12
2026-01-22 23:18:45.082 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.50% | total memory: 25 GB
2026-01-22 23:18:45.082 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:18:45.082 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.4.self_attn.k_proj using 3 samples
2026-01-22 23:18:46.711 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:18:46.713 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2565.48
2026-01-22 23:18:46.713 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.50% | total memory: 25 GB
2026-01-22 23:18:46.713 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:18:46.713 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.4.self_attn.v_proj using 3 samples
2026-01-22 23:18:48.340 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:18:48.341 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 394.59
2026-01-22 23:18:48.341 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.50% | total memory: 25 GB
2026-01-22 23:18:48.342 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:18:48.342 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.4.self_attn.o_proj using 3 samples
2026-01-22 23:18:49.983 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.64s
2026-01-22 23:18:49.984 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 258.45
2026-01-22 23:18:49.984 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.50% | total memory: 25 GB
2026-01-22 23:18:49.985 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:18:49.985 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.4.mlp.gate_proj using 3 samples
2026-01-22 23:18:51.751 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.77s
2026-01-22 23:18:51.752 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 308074.12
2026-01-22 23:18:51.752 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.25% | total memory: 25 GB
2026-01-22 23:18:51.752 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:18:51.753 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.4.mlp.up_proj using 3 samples
2026-01-22 23:18:53.502 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.75s
2026-01-22 23:18:53.503 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 81913.14
2026-01-22 23:18:53.504 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.25% | total memory: 25 GB
2026-01-22 23:18:53.504 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:18:53.504 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.4.mlp.down_proj using 3 samples
2026-01-22 23:19:02.546 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 9.04s
2026-01-22 23:19:02.550 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 368.26
2026-01-22 23:19:02.551 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.51% | total memory: 25 GB
2026-01-22 23:19:02.551 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:19:02.551 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:19:03.097 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:19:03.097 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.5.self_attn.q_proj using 3 samples
2026-01-22 23:19:04.765 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.67s
2026-01-22 23:19:04.766 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 17694.82
2026-01-22 23:19:04.766 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:04.767 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:19:04.767 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.5.self_attn.k_proj using 3 samples
2026-01-22 23:19:06.398 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:19:06.400 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 4480.79
2026-01-22 23:19:06.400 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:06.400 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:19:06.400 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.5.self_attn.v_proj using 3 samples
2026-01-22 23:19:08.019 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.62s
2026-01-22 23:19:08.020 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 414.19
2026-01-22 23:19:08.020 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:08.020 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:19:08.021 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.5.self_attn.o_proj using 3 samples
2026-01-22 23:19:09.646 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:19:09.647 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 194.65
2026-01-22 23:19:09.647 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:09.648 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:19:09.648 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.5.mlp.gate_proj using 3 samples
2026-01-22 23:19:11.464 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.82s
2026-01-22 23:19:11.465 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 295716.56
2026-01-22 23:19:11.465 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:19:11.466 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:19:11.466 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.5.mlp.up_proj using 3 samples
2026-01-22 23:19:13.226 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.76s
2026-01-22 23:19:13.228 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 85620.72
2026-01-22 23:19:13.228 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:19:13.228 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:19:13.229 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.5.mlp.down_proj using 3 samples
2026-01-22 23:19:22.153 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 8.92s
2026-01-22 23:19:22.158 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 687.11
2026-01-22 23:19:22.158 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.67% | total memory: 25 GB
2026-01-22 23:19:22.158 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:19:22.159 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:19:22.706 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:19:22.706 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.6.self_attn.q_proj using 3 samples
2026-01-22 23:19:24.467 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.76s
2026-01-22 23:19:24.468 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 13023.52
2026-01-22 23:19:24.468 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:19:24.468 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:19:24.469 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.6.self_attn.k_proj using 3 samples
2026-01-22 23:19:26.101 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:19:26.103 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2766.70
2026-01-22 23:19:26.103 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:19:26.103 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:19:26.103 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.6.self_attn.v_proj using 3 samples
2026-01-22 23:19:27.735 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:19:27.736 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1535.79
2026-01-22 23:19:27.736 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:19:27.737 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:19:27.737 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.6.self_attn.o_proj using 3 samples
2026-01-22 23:19:29.381 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.64s
2026-01-22 23:19:29.382 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 392.66
2026-01-22 23:19:29.382 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:19:29.383 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:19:29.383 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.6.mlp.gate_proj using 3 samples
2026-01-22 23:19:31.117 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.73s
2026-01-22 23:19:31.118 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 495704.62
2026-01-22 23:19:31.119 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:31.119 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:19:31.119 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.6.mlp.up_proj using 3 samples
2026-01-22 23:19:32.869 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.75s
2026-01-22 23:19:32.870 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 108000.40
2026-01-22 23:19:32.870 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:32.870 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:19:32.871 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.6.mlp.down_proj using 3 samples
2026-01-22 23:19:41.917 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 9.05s
2026-01-22 23:19:41.924 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 733.34
2026-01-22 23:19:41.924 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:19:41.925 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:19:41.925 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:19:42.478 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:19:42.478 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.7.self_attn.q_proj using 3 samples
2026-01-22 23:19:44.162 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.68s
2026-01-22 23:19:44.164 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 17116.54
2026-01-22 23:19:44.164 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:44.164 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:19:44.164 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.7.self_attn.k_proj using 3 samples
2026-01-22 23:19:45.797 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:19:45.798 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 3671.08
2026-01-22 23:19:45.798 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:45.798 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:19:45.799 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.7.self_attn.v_proj using 3 samples
2026-01-22 23:19:47.412 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.61s
2026-01-22 23:19:47.413 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 790.59
2026-01-22 23:19:47.414 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:47.414 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:19:47.414 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.7.self_attn.o_proj using 3 samples
2026-01-22 23:19:49.037 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.62s
2026-01-22 23:19:49.038 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 269.14
2026-01-22 23:19:49.038 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:49.038 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:19:49.039 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.7.mlp.gate_proj using 3 samples
2026-01-22 23:19:50.852 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.81s
2026-01-22 23:19:50.853 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 564787.00
2026-01-22 23:19:50.853 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:19:50.853 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:19:50.854 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.7.mlp.up_proj using 3 samples
2026-01-22 23:19:52.606 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.75s
2026-01-22 23:19:52.607 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 119746.55
2026-01-22 23:19:52.607 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:19:52.608 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:19:52.608 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.7.mlp.down_proj using 3 samples
2026-01-22 23:20:01.593 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 8.99s
2026-01-22 23:20:01.600 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 855.34
2026-01-22 23:20:01.600 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.68% | total memory: 25 GB
2026-01-22 23:20:01.600 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:20:01.601 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:20:02.154 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:20:02.155 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.8.self_attn.q_proj using 3 samples
2026-01-22 23:20:03.920 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.76s
2026-01-22 23:20:03.921 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 19795.72
2026-01-22 23:20:03.921 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.86% | total memory: 25 GB
2026-01-22 23:20:03.921 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:20:03.922 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.8.self_attn.k_proj using 3 samples
2026-01-22 23:20:05.556 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:20:05.557 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 3103.80
2026-01-22 23:20:05.557 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.86% | total memory: 25 GB
2026-01-22 23:20:05.558 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:20:05.558 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.8.self_attn.v_proj using 3 samples
2026-01-22 23:20:07.193 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:20:07.194 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 915.89
2026-01-22 23:20:07.194 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.86% | total memory: 25 GB
2026-01-22 23:20:07.194 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:20:07.195 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.8.self_attn.o_proj using 3 samples
2026-01-22 23:20:08.829 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:20:08.830 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 366.96
2026-01-22 23:20:08.830 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.86% | total memory: 25 GB
2026-01-22 23:20:08.830 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:20:08.831 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.8.mlp.gate_proj using 3 samples
2026-01-22 23:20:10.576 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.75s
2026-01-22 23:20:10.577 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 594264.88
2026-01-22 23:20:10.577 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:10.577 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:20:10.578 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.8.mlp.up_proj using 3 samples
2026-01-22 23:20:12.324 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.75s
2026-01-22 23:20:12.325 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 112198.06
2026-01-22 23:20:12.326 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:12.326 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:20:12.326 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.8.mlp.down_proj using 3 samples
2026-01-22 23:20:21.404 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 9.08s
2026-01-22 23:20:21.411 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1025.75
2026-01-22 23:20:21.411 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.87% | total memory: 25 GB
2026-01-22 23:20:21.412 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:20:21.412 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:20:21.971 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:20:21.971 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.9.self_attn.q_proj using 3 samples
2026-01-22 23:20:23.657 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.69s
2026-01-22 23:20:23.659 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 18633.14
2026-01-22 23:20:23.659 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:23.659 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:20:23.659 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.9.self_attn.k_proj using 3 samples
2026-01-22 23:20:25.292 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:20:25.293 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2936.09
2026-01-22 23:20:25.293 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:25.293 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:20:25.294 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.9.self_attn.v_proj using 3 samples
2026-01-22 23:20:26.899 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.61s
2026-01-22 23:20:26.900 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 680.94
2026-01-22 23:20:26.901 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:26.901 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:20:26.901 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.9.self_attn.o_proj using 3 samples
2026-01-22 23:20:28.527 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:20:28.528 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 357.27
2026-01-22 23:20:28.528 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:28.528 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:20:28.529 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.9.mlp.gate_proj using 3 samples
2026-01-22 23:20:30.343 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.81s
2026-01-22 23:20:30.344 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 678619.75
2026-01-22 23:20:30.344 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:20:30.344 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:20:30.345 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.9.mlp.up_proj using 3 samples
2026-01-22 23:20:32.094 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.75s
2026-01-22 23:20:32.095 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 97008.05
2026-01-22 23:20:32.095 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:20:32.095 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:20:32.096 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.9.mlp.down_proj using 3 samples
2026-01-22 23:20:41.068 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 8.97s
2026-01-22 23:20:41.073 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1352.24
2026-01-22 23:20:41.074 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.61% | total memory: 25 GB
2026-01-22 23:20:41.074 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:20:41.074 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:20:41.627 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:20:41.627 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.10.self_attn.q_proj using 3 samples
2026-01-22 23:20:43.389 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.76s
2026-01-22 23:20:43.390 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 24087.62
2026-01-22 23:20:43.391 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:20:43.391 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:20:43.391 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.10.self_attn.k_proj using 3 samples
2026-01-22 23:20:45.016 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.62s
2026-01-22 23:20:45.017 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 3577.09
2026-01-22 23:20:45.017 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:20:45.017 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:20:45.018 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.10.self_attn.v_proj using 3 samples
2026-01-22 23:20:46.650 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:20:46.651 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 932.39
2026-01-22 23:20:46.651 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:20:46.651 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:20:46.652 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.10.self_attn.o_proj using 3 samples
2026-01-22 23:20:48.293 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.64s
2026-01-22 23:20:48.294 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 381.87
2026-01-22 23:20:48.294 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:20:48.294 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:20:48.295 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.10.mlp.gate_proj using 3 samples
2026-01-22 23:20:50.035 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.74s
2026-01-22 23:20:50.036 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 465157.44
2026-01-22 23:20:50.038 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:50.038 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:20:50.039 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.10.mlp.up_proj using 3 samples
2026-01-22 23:20:51.798 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.76s
2026-01-22 23:20:51.799 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 62566.93
2026-01-22 23:20:51.799 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:51.800 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:20:51.800 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.10.mlp.down_proj using 3 samples
2026-01-22 23:21:00.848 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 9.05s
2026-01-22 23:21:00.853 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1917.68
2026-01-22 23:21:00.853 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.86% | total memory: 25 GB
2026-01-22 23:21:00.854 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:21:00.854 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:21:01.397 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:21:01.397 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.11.self_attn.q_proj using 3 samples
2026-01-22 23:21:03.093 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.70s
2026-01-22 23:21:03.094 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 18547.80
2026-01-22 23:21:03.094 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:03.094 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:21:03.095 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.11.self_attn.k_proj using 3 samples
2026-01-22 23:21:04.718 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.62s
2026-01-22 23:21:04.719 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2729.52
2026-01-22 23:21:04.720 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:04.720 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:21:04.720 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.11.self_attn.v_proj using 3 samples
2026-01-22 23:21:06.345 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:21:06.347 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 850.03
2026-01-22 23:21:06.347 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:06.347 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:21:06.347 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.11.self_attn.o_proj using 3 samples
2026-01-22 23:21:07.990 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.64s
2026-01-22 23:21:07.991 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 733.33
2026-01-22 23:21:07.991 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:07.991 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:21:07.992 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.11.mlp.gate_proj using 3 samples
2026-01-22 23:21:09.816 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.82s
2026-01-22 23:21:09.817 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 154906.89
2026-01-22 23:21:09.818 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:21:09.818 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:21:09.818 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.11.mlp.up_proj using 3 samples
2026-01-22 23:21:11.576 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.76s
2026-01-22 23:21:11.577 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 37422.29
2026-01-22 23:21:11.577 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:21:11.578 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:21:11.578 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.11.mlp.down_proj using 3 samples
2026-01-22 23:21:20.606 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 9.03s
2026-01-22 23:21:20.611 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2147.27
2026-01-22 23:21:20.611 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.61% | total memory: 25 GB
2026-01-22 23:21:20.611 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:21:20.612 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:21:21.167 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:21:21.167 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.12.self_attn.q_proj using 3 samples
2026-01-22 23:21:22.946 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.78s
2026-01-22 23:21:22.947 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 17806.10
2026-01-22 23:21:22.947 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:21:22.947 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:21:22.948 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.12.self_attn.k_proj using 3 samples
2026-01-22 23:21:24.583 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.64s
2026-01-22 23:21:24.585 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2481.50
2026-01-22 23:21:24.585 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:21:24.585 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:21:24.585 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.12.self_attn.v_proj using 3 samples
2026-01-22 23:21:26.211 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:21:26.212 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 690.46
2026-01-22 23:21:26.213 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:21:26.213 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:21:26.213 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.12.self_attn.o_proj using 3 samples
2026-01-22 23:21:27.838 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.62s
2026-01-22 23:21:27.839 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 668.56
2026-01-22 23:21:27.840 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 28.07% | total memory: 25 GB
2026-01-22 23:21:27.840 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:21:27.840 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.12.mlp.gate_proj using 3 samples
2026-01-22 23:21:29.585 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.74s
2026-01-22 23:21:29.587 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 151888.92
2026-01-22 23:21:29.587 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:29.587 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:21:29.587 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.12.mlp.up_proj using 3 samples
2026-01-22 23:21:31.338 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.75s
2026-01-22 23:21:31.339 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 39230.89
2026-01-22 23:21:31.340 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:31.340 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:21:31.340 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.12.mlp.down_proj using 3 samples
2026-01-22 23:21:40.435 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 9.09s
2026-01-22 23:21:40.440 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2011.25
2026-01-22 23:21:40.440 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.86% | total memory: 25 GB
2026-01-22 23:21:40.440 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:21:40.441 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:21:41.011 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:21:41.011 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.13.self_attn.q_proj using 3 samples
2026-01-22 23:21:42.719 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.71s
2026-01-22 23:21:42.720 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 15261.66
2026-01-22 23:21:42.720 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:42.722 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:21:42.722 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.13.self_attn.k_proj using 3 samples
2026-01-22 23:21:44.351 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:21:44.352 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2669.76
2026-01-22 23:21:44.352 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:44.352 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:21:44.353 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.13.self_attn.v_proj using 3 samples
2026-01-22 23:21:45.981 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:21:45.983 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 787.66
2026-01-22 23:21:45.983 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:45.983 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:21:45.983 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.13.self_attn.o_proj using 3 samples
2026-01-22 23:21:47.619 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.64s
2026-01-22 23:21:47.620 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 484.79
2026-01-22 23:21:47.620 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:47.620 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:21:47.621 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.13.mlp.gate_proj using 3 samples
2026-01-22 23:21:49.394 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.77s
2026-01-22 23:21:49.396 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 42726.45
2026-01-22 23:21:49.396 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:21:49.396 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:21:49.396 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.13.mlp.up_proj using 3 samples
2026-01-22 23:21:51.157 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.76s
2026-01-22 23:21:51.158 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 29618.33
2026-01-22 23:21:51.159 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:21:51.159 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:21:51.159 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.13.mlp.down_proj using 3 samples
2026-01-22 23:22:00.176 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 9.02s
2026-01-22 23:22:00.181 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1716.41
2026-01-22 23:22:00.181 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.61% | total memory: 25 GB
2026-01-22 23:22:00.181 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:22:00.181 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:22:00.737 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:22:00.737 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.14.self_attn.q_proj using 3 samples
2026-01-22 23:22:02.524 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.79s
2026-01-22 23:22:02.525 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 16357.46
2026-01-22 23:22:02.525 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:22:02.525 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:22:02.526 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.14.self_attn.k_proj using 3 samples
2026-01-22 23:22:04.162 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.64s
2026-01-22 23:22:04.163 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2626.38
2026-01-22 23:22:04.163 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:22:04.163 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:22:04.164 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.14.self_attn.v_proj using 3 samples
2026-01-22 23:22:05.799 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:22:05.800 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 554.77
2026-01-22 23:22:05.800 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:22:05.801 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:22:05.801 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.14.self_attn.o_proj using 3 samples
2026-01-22 23:22:07.428 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:22:07.429 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 729.97
2026-01-22 23:22:07.429 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:22:07.430 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:22:07.430 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.14.mlp.gate_proj using 3 samples
2026-01-22 23:22:09.191 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.76s
2026-01-22 23:22:09.192 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 44929.44
2026-01-22 23:22:09.193 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:22:09.193 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:22:09.193 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.14.mlp.up_proj using 3 samples
2026-01-22 23:22:10.959 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.77s
2026-01-22 23:22:10.960 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 30103.39
2026-01-22 23:22:10.960 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:22:10.960 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:22:10.961 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.14.mlp.down_proj using 3 samples
2026-01-22 23:22:20.053 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 9.09s
2026-01-22 23:22:20.057 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1703.26
2026-01-22 23:22:20.058 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.87% | total memory: 25 GB
2026-01-22 23:22:20.058 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:22:20.059 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:22:20.617 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:22:20.618 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.15.self_attn.q_proj using 3 samples
2026-01-22 23:22:22.317 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.70s
2026-01-22 23:22:22.318 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 17865.81
2026-01-22 23:22:22.319 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:22.319 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:22:22.319 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.15.self_attn.k_proj using 3 samples
2026-01-22 23:22:23.949 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:22:23.950 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2707.32
2026-01-22 23:22:23.950 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:23.951 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:22:23.951 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.15.self_attn.v_proj using 3 samples
2026-01-22 23:22:25.576 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.62s
2026-01-22 23:22:25.577 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 774.89
2026-01-22 23:22:25.577 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.67% | total memory: 25 GB
2026-01-22 23:22:25.578 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:22:25.578 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.15.self_attn.o_proj using 3 samples
2026-01-22 23:22:27.211 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:22:27.212 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 895.35
2026-01-22 23:22:27.212 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.67% | total memory: 25 GB
2026-01-22 23:22:27.212 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:22:27.213 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.15.mlp.gate_proj using 3 samples
2026-01-22 23:22:28.990 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.78s
2026-01-22 23:22:28.991 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 37818.91
2026-01-22 23:22:28.992 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:22:28.992 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:22:28.992 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.15.mlp.up_proj using 3 samples
2026-01-22 23:22:30.759 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.77s
2026-01-22 23:22:30.760 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 29950.77
2026-01-22 23:22:30.761 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:22:30.761 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:22:30.761 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.15.mlp.down_proj using 3 samples
2026-01-22 23:22:39.747 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 8.99s
2026-01-22 23:22:39.752 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1640.40
2026-01-22 23:22:39.752 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.67% | total memory: 25 GB
2026-01-22 23:22:39.752 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:22:39.752 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:22:40.311 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:22:40.311 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.16.self_attn.q_proj using 3 samples
2026-01-22 23:22:42.091 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.78s
2026-01-22 23:22:42.093 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 19898.96
2026-01-22 23:22:42.093 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:22:42.093 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:22:42.094 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.16.self_attn.k_proj using 3 samples
2026-01-22 23:22:43.734 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.64s
2026-01-22 23:22:43.735 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 3784.51
2026-01-22 23:22:43.735 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:22:43.735 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:22:43.736 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.16.self_attn.v_proj using 3 samples
2026-01-22 23:22:45.370 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:22:45.371 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 666.68
2026-01-22 23:22:45.371 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:22:45.371 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:22:45.372 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.16.self_attn.o_proj using 3 samples
2026-01-22 23:22:47.005 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:22:47.006 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 883.64
2026-01-22 23:22:47.007 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.64% | total memory: 25 GB
2026-01-22 23:22:47.007 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:22:47.007 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.16.mlp.gate_proj using 3 samples
2026-01-22 23:22:48.776 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.77s
2026-01-22 23:22:48.777 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 45878.86
2026-01-22 23:22:48.778 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:48.778 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:22:48.778 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.16.mlp.up_proj using 3 samples
2026-01-22 23:22:50.540 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.76s
2026-01-22 23:22:50.542 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 32113.49
2026-01-22 23:22:50.542 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:50.542 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:22:50.542 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.16.mlp.down_proj using 3 samples
2026-01-22 23:22:59.630 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 9.09s
2026-01-22 23:22:59.634 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1740.96
2026-01-22 23:22:59.635 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:22:59.635 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:22:59.635 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:23:00.196 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:23:00.197 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.17.self_attn.q_proj using 3 samples
2026-01-22 23:23:01.917 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.72s
2026-01-22 23:23:01.919 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 28984.50
2026-01-22 23:23:01.919 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:23:01.919 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:23:01.919 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.17.self_attn.k_proj using 3 samples
2026-01-22 23:23:03.556 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.64s
2026-01-22 23:23:03.557 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 5062.94
2026-01-22 23:23:03.558 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:23:03.558 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:23:03.558 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.17.self_attn.v_proj using 3 samples
2026-01-22 23:23:05.187 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.63s
2026-01-22 23:23:05.188 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1165.41
2026-01-22 23:23:05.188 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:23:05.188 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:23:05.189 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.17.self_attn.o_proj using 3 samples
2026-01-22 23:23:06.832 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.64s
2026-01-22 23:23:06.833 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1051.29
2026-01-22 23:23:06.834 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:23:06.834 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:23:06.834 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.17.mlp.gate_proj using 3 samples
2026-01-22 23:23:08.616 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.78s
2026-01-22 23:23:08.617 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 41038.88
2026-01-22 23:23:08.618 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:23:08.618 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:23:08.618 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.17.mlp.up_proj using 3 samples
2026-01-22 23:23:10.380 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.76s
2026-01-22 23:23:10.381 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 32226.07
2026-01-22 23:23:10.382 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:23:10.382 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:23:10.382 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.17.mlp.down_proj using 3 samples
2026-01-22 23:23:19.410 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 9.03s
2026-01-22 23:23:19.415 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1830.13
2026-01-22 23:23:19.415 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.68% | total memory: 25 GB
2026-01-22 23:23:19.416 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:23:19.416 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:23:20.016 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:23:20.016 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.18.self_attn.q_proj using 3 samples
2026-01-22 23:23:21.755 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.74s
2026-01-22 23:23:21.757 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 20107.60
2026-01-22 23:23:21.757 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.86% | total memory: 25 GB
2026-01-22 23:23:21.757 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:23:21.757 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.18.self_attn.k_proj using 3 samples
2026-01-22 23:23:23.395 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.64s
2026-01-22 23:23:23.396 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 3334.41
2026-01-22 23:23:23.397 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.86% | total memory: 25 GB
2026-01-22 23:23:23.397 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:23:23.397 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.18.self_attn.v_proj using 3 samples
2026-01-22 23:23:25.037 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.64s
2026-01-22 23:23:25.038 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 897.09
2026-01-22 23:23:25.039 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.86% | total memory: 25 GB
2026-01-22 23:23:25.039 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:23:25.040 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.18.self_attn.o_proj using 3 samples
2026-01-22 23:23:26.622 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.58s
2026-01-22 23:23:26.623 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1311.09
2026-01-22 23:23:26.623 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.88% | total memory: 25 GB
2026-01-22 23:23:26.623 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:23:26.624 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.18.mlp.gate_proj using 3 samples
2026-01-22 23:23:28.074 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.45s
2026-01-22 23:23:28.076 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 45115.95
2026-01-22 23:23:28.076 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 28.05% | total memory: 25 GB
2026-01-22 23:23:28.076 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:23:28.076 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.18.mlp.up_proj using 3 samples
2026-01-22 23:23:29.429 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.35s
2026-01-22 23:23:29.430 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 32768.95
2026-01-22 23:23:29.430 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 28.05% | total memory: 25 GB
2026-01-22 23:23:29.431 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:23:29.431 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.18.mlp.down_proj using 3 samples
2026-01-22 23:23:36.331 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.90s
2026-01-22 23:23:36.335 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1858.33
2026-01-22 23:23:36.336 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 28.06% | total memory: 25 GB
2026-01-22 23:23:36.336 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:23:36.336 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:23:36.876 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:23:36.876 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.19.self_attn.q_proj using 3 samples
2026-01-22 23:23:38.174 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.30s
2026-01-22 23:23:38.175 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 24097.74
2026-01-22 23:23:38.175 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 28.11% | total memory: 25 GB
2026-01-22 23:23:38.176 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:23:38.176 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.19.self_attn.k_proj using 3 samples
2026-01-22 23:23:39.418 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.24s
2026-01-22 23:23:39.419 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 3894.08
2026-01-22 23:23:39.419 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 28.11% | total memory: 25 GB
2026-01-22 23:23:39.420 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:23:39.420 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.19.self_attn.v_proj using 3 samples
2026-01-22 23:23:40.670 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.25s
2026-01-22 23:23:40.672 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1268.01
2026-01-22 23:23:40.672 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 27.97% | total memory: 25 GB
2026-01-22 23:23:40.672 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:23:40.672 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.19.self_attn.o_proj using 3 samples
2026-01-22 23:23:41.928 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.26s
2026-01-22 23:23:41.929 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1432.05
2026-01-22 23:23:41.929 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:23:41.930 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:23:41.930 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.19.mlp.gate_proj using 3 samples
2026-01-22 23:23:43.278 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.35s
2026-01-22 23:23:43.279 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 41910.18
2026-01-22 23:23:43.279 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:23:43.279 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:23:43.280 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.19.mlp.up_proj using 3 samples
2026-01-22 23:23:44.638 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.36s
2026-01-22 23:23:44.639 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 34063.97
2026-01-22 23:23:44.640 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:23:44.640 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:23:44.640 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.19.mlp.down_proj using 3 samples
2026-01-22 23:23:51.587 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.95s
2026-01-22 23:23:51.592 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1951.84
2026-01-22 23:23:51.592 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.25% | total memory: 25 GB
2026-01-22 23:23:51.592 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:23:51.592 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:23:52.131 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:23:52.132 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.20.self_attn.q_proj using 3 samples
2026-01-22 23:23:53.438 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.31s
2026-01-22 23:23:53.439 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 35880.36
2026-01-22 23:23:53.440 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:23:53.440 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:23:53.440 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.20.self_attn.k_proj using 3 samples
2026-01-22 23:23:54.675 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.23s
2026-01-22 23:23:54.676 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 4720.21
2026-01-22 23:23:54.676 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:23:54.677 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:23:54.677 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.20.self_attn.v_proj using 3 samples
2026-01-22 23:23:55.895 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.22s
2026-01-22 23:23:55.897 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2303.33
2026-01-22 23:23:55.897 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:23:55.897 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:23:55.898 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.20.self_attn.o_proj using 3 samples
2026-01-22 23:23:57.133 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.24s
2026-01-22 23:23:57.134 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1496.38
2026-01-22 23:23:57.135 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:23:57.135 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:23:57.135 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.20.mlp.gate_proj using 3 samples
2026-01-22 23:23:58.488 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.35s
2026-01-22 23:23:58.490 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 50708.20
2026-01-22 23:23:58.490 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:23:58.490 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:23:58.491 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.20.mlp.up_proj using 3 samples
2026-01-22 23:23:59.849 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.36s
2026-01-22 23:23:59.850 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 37496.43
2026-01-22 23:23:59.850 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:23:59.850 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:23:59.851 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.20.mlp.down_proj using 3 samples
2026-01-22 23:24:06.764 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.91s
2026-01-22 23:24:06.772 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2556.46
2026-01-22 23:24:06.772 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.25% | total memory: 25 GB
2026-01-22 23:24:06.772 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:24:06.773 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:24:07.302 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:24:07.303 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.21.self_attn.q_proj using 3 samples
2026-01-22 23:24:08.617 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.31s
2026-01-22 23:24:08.619 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 29387.37
2026-01-22 23:24:08.619 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:24:08.619 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:24:08.619 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.21.self_attn.k_proj using 3 samples
2026-01-22 23:24:09.869 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.25s
2026-01-22 23:24:09.870 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 4958.58
2026-01-22 23:24:09.870 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:24:09.870 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:24:09.871 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.21.self_attn.v_proj using 3 samples
2026-01-22 23:24:11.114 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.24s
2026-01-22 23:24:11.115 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1446.05
2026-01-22 23:24:11.115 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:24:11.115 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:24:11.116 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.21.self_attn.o_proj using 3 samples
2026-01-22 23:24:12.362 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.25s
2026-01-22 23:24:12.363 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1522.89
2026-01-22 23:24:12.364 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:24:12.364 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:24:12.364 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.21.mlp.gate_proj using 3 samples
2026-01-22 23:24:13.745 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.38s
2026-01-22 23:24:13.746 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 54111.04
2026-01-22 23:24:13.746 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:24:13.747 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:24:13.747 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.21.mlp.up_proj using 3 samples
2026-01-22 23:24:15.107 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.36s
2026-01-22 23:24:15.108 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 37790.91
2026-01-22 23:24:15.108 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:24:15.108 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:24:15.109 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.21.mlp.down_proj using 3 samples
2026-01-22 23:24:22.056 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.95s
2026-01-22 23:24:22.060 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2239.48
2026-01-22 23:24:22.061 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.25% | total memory: 25 GB
2026-01-22 23:24:22.061 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:24:22.061 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:24:22.592 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:24:22.592 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.22.self_attn.q_proj using 3 samples
2026-01-22 23:24:23.891 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.30s
2026-01-22 23:24:23.892 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 33257.45
2026-01-22 23:24:23.892 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:23.892 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:24:23.893 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.22.self_attn.k_proj using 3 samples
2026-01-22 23:24:25.140 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.25s
2026-01-22 23:24:25.141 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 5761.41
2026-01-22 23:24:25.141 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:25.142 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:24:25.142 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.22.self_attn.v_proj using 3 samples
2026-01-22 23:24:26.370 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.23s
2026-01-22 23:24:26.371 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1941.26
2026-01-22 23:24:26.372 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:26.372 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:24:26.372 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.22.self_attn.o_proj using 3 samples
2026-01-22 23:24:27.625 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.25s
2026-01-22 23:24:27.627 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1373.21
2026-01-22 23:24:27.627 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:27.627 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:24:27.627 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.22.mlp.gate_proj using 3 samples
2026-01-22 23:24:28.981 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.35s
2026-01-22 23:24:28.982 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 54918.15
2026-01-22 23:24:28.983 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:28.983 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:24:28.983 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.22.mlp.up_proj using 3 samples
2026-01-22 23:24:30.322 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.34s
2026-01-22 23:24:30.323 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 40952.77
2026-01-22 23:24:30.323 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:30.323 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:24:30.324 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.22.mlp.down_proj using 3 samples
2026-01-22 23:24:37.243 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.92s
2026-01-22 23:24:37.247 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2433.98
2026-01-22 23:24:37.248 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:37.248 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:24:37.248 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:24:37.783 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:24:37.784 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.23.self_attn.q_proj using 3 samples
2026-01-22 23:24:39.085 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.30s
2026-01-22 23:24:39.086 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 30842.05
2026-01-22 23:24:39.086 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:39.087 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:24:39.087 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.23.self_attn.k_proj using 3 samples
2026-01-22 23:24:40.325 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.24s
2026-01-22 23:24:40.326 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 4325.11
2026-01-22 23:24:40.327 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:40.327 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:24:40.327 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.23.self_attn.v_proj using 3 samples
2026-01-22 23:24:41.568 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.24s
2026-01-22 23:24:41.570 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1470.34
2026-01-22 23:24:41.570 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:41.570 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:24:41.570 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.23.self_attn.o_proj using 3 samples
2026-01-22 23:24:42.834 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.26s
2026-01-22 23:24:42.836 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1375.46
2026-01-22 23:24:42.836 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:42.836 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:24:42.836 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.23.mlp.gate_proj using 3 samples
2026-01-22 23:24:44.199 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.36s
2026-01-22 23:24:44.200 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 55432.46
2026-01-22 23:24:44.200 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:44.201 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:24:44.201 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.23.mlp.up_proj using 3 samples
2026-01-22 23:24:45.552 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.35s
2026-01-22 23:24:45.553 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 44212.45
2026-01-22 23:24:45.553 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:45.553 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:24:45.554 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.23.mlp.down_proj using 3 samples
2026-01-22 23:24:52.509 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.95s
2026-01-22 23:24:52.514 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2900.62
2026-01-22 23:24:52.514 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:52.514 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:24:52.515 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:24:53.044 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:24:53.045 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.24.self_attn.q_proj using 3 samples
2026-01-22 23:24:54.342 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.30s
2026-01-22 23:24:54.343 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 30216.82
2026-01-22 23:24:54.343 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:54.343 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:24:54.344 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.24.self_attn.k_proj using 3 samples
2026-01-22 23:24:55.582 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.24s
2026-01-22 23:24:55.583 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 5034.72
2026-01-22 23:24:55.584 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:55.584 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:24:55.584 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.24.self_attn.v_proj using 3 samples
2026-01-22 23:24:56.817 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.23s
2026-01-22 23:24:56.818 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1269.42
2026-01-22 23:24:56.818 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:56.818 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:24:56.819 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.24.self_attn.o_proj using 3 samples
2026-01-22 23:24:58.061 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.24s
2026-01-22 23:24:58.062 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1445.10
2026-01-22 23:24:58.062 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:58.063 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:24:58.063 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.24.mlp.gate_proj using 3 samples
2026-01-22 23:24:59.426 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.36s
2026-01-22 23:24:59.427 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 56975.14
2026-01-22 23:24:59.427 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:24:59.428 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:24:59.428 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.24.mlp.up_proj using 3 samples
2026-01-22 23:25:00.782 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.35s
2026-01-22 23:25:00.784 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 45432.90
2026-01-22 23:25:00.784 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:25:00.784 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:25:00.784 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.24.mlp.down_proj using 3 samples
2026-01-22 23:25:07.719 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.93s
2026-01-22 23:25:07.724 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2987.22
2026-01-22 23:25:07.724 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:25:07.724 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:25:07.724 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:25:08.255 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:25:08.256 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.25.self_attn.q_proj using 3 samples
2026-01-22 23:25:09.555 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.30s
2026-01-22 23:25:09.556 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 38776.34
2026-01-22 23:25:09.557 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:25:09.557 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:25:09.557 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.25.self_attn.k_proj using 3 samples
2026-01-22 23:25:10.806 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.25s
2026-01-22 23:25:10.807 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 5088.80
2026-01-22 23:25:10.807 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:25:10.808 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:25:10.808 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.25.self_attn.v_proj using 3 samples
2026-01-22 23:25:12.037 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.23s
2026-01-22 23:25:12.038 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2188.49
2026-01-22 23:25:12.038 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:25:12.038 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:25:12.039 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.25.self_attn.o_proj using 3 samples
2026-01-22 23:25:13.268 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.23s
2026-01-22 23:25:13.269 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2069.01
2026-01-22 23:25:13.269 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:25:13.270 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:25:13.270 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.25.mlp.gate_proj using 3 samples
2026-01-22 23:25:14.624 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.35s
2026-01-22 23:25:14.625 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 68494.71
2026-01-22 23:25:14.625 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:25:14.626 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:25:14.626 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.25.mlp.up_proj using 3 samples
2026-01-22 23:25:16.003 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.38s
2026-01-22 23:25:16.004 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 50031.02
2026-01-22 23:25:16.004 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:25:16.004 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:25:16.005 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.25.mlp.down_proj using 3 samples
2026-01-22 23:25:22.934 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.93s
2026-01-22 23:25:22.938 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 4056.32
2026-01-22 23:25:22.939 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:25:22.939 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:25:22.939 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:25:23.475 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:25:23.475 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.26.self_attn.q_proj using 3 samples
2026-01-22 23:25:24.758 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.28s
2026-01-22 23:25:24.759 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 32511.27
2026-01-22 23:25:24.759 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:25:24.759 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:25:24.760 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.26.self_attn.k_proj using 3 samples
2026-01-22 23:25:25.988 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.23s
2026-01-22 23:25:25.989 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 4951.70
2026-01-22 23:25:25.989 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:25:25.990 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:25:25.990 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.26.self_attn.v_proj using 3 samples
2026-01-22 23:25:27.238 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.25s
2026-01-22 23:25:27.239 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2486.30
2026-01-22 23:25:27.240 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:25:27.240 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:25:27.240 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.26.self_attn.o_proj using 3 samples
2026-01-22 23:25:28.488 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.25s
2026-01-22 23:25:28.489 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1660.16
2026-01-22 23:25:28.490 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:25:28.490 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:25:28.490 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.26.mlp.gate_proj using 3 samples
2026-01-22 23:25:29.832 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.34s
2026-01-22 23:25:29.833 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 77328.56
2026-01-22 23:25:29.833 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:25:29.834 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:25:29.834 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.26.mlp.up_proj using 3 samples
2026-01-22 23:25:31.191 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.36s
2026-01-22 23:25:31.192 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 57612.26
2026-01-22 23:25:31.192 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:25:31.192 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:25:31.193 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.26.mlp.down_proj using 3 samples
2026-01-22 23:25:38.127 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.93s
2026-01-22 23:25:38.131 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 5267.02
2026-01-22 23:25:38.131 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:25:38.132 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:25:38.132 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:25:38.662 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:25:38.662 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.27.self_attn.q_proj using 3 samples
2026-01-22 23:25:39.951 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.29s
2026-01-22 23:25:39.952 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 56085.30
2026-01-22 23:25:39.953 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 26.07% | total memory: 25 GB
2026-01-22 23:25:39.953 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:25:39.953 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.27.self_attn.k_proj using 3 samples
2026-01-22 23:25:41.195 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.24s
2026-01-22 23:25:41.197 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 5107.79
2026-01-22 23:25:41.197 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 26.07% | total memory: 25 GB
2026-01-22 23:25:41.197 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:25:41.198 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.27.self_attn.v_proj using 3 samples
2026-01-22 23:25:42.437 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.24s
2026-01-22 23:25:42.438 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 3402.44
2026-01-22 23:25:42.438 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 26.07% | total memory: 25 GB
2026-01-22 23:25:42.438 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:25:42.439 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.27.self_attn.o_proj using 3 samples
2026-01-22 23:25:43.688 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.25s
2026-01-22 23:25:43.689 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 3814.73
2026-01-22 23:25:43.689 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 26.07% | total memory: 25 GB
2026-01-22 23:25:43.689 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:25:43.690 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.27.mlp.gate_proj using 3 samples
2026-01-22 23:25:45.058 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.37s
2026-01-22 23:25:45.060 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 82359.02
2026-01-22 23:25:45.073 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 26.07% | total memory: 25 GB
2026-01-22 23:25:45.073 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:25:45.073 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.27.mlp.up_proj using 3 samples
2026-01-22 23:25:46.427 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.35s
2026-01-22 23:25:46.428 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 60379.89
2026-01-22 23:25:46.428 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 26.07% | total memory: 25 GB
2026-01-22 23:25:46.428 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:25:46.429 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.27.mlp.down_proj using 3 samples
2026-01-22 23:25:53.327 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.90s
2026-01-22 23:25:53.332 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 5811.78
2026-01-22 23:25:53.332 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 26.07% | total memory: 25 GB
2026-01-22 23:25:53.332 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:25:53.332 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:25:53.862 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:25:53.862 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.28.self_attn.q_proj using 3 samples
2026-01-22 23:25:55.179 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.32s
2026-01-22 23:25:55.180 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 36014.34
2026-01-22 23:25:55.181 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:25:55.181 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:25:55.181 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.28.self_attn.k_proj using 3 samples
2026-01-22 23:25:56.419 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.24s
2026-01-22 23:25:56.421 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 5275.32
2026-01-22 23:25:56.421 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:25:56.421 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:25:56.421 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.28.self_attn.v_proj using 3 samples
2026-01-22 23:25:57.664 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.24s
2026-01-22 23:25:57.665 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2886.42
2026-01-22 23:25:57.666 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:25:57.666 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:25:57.666 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.28.self_attn.o_proj using 3 samples
2026-01-22 23:25:58.920 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.25s
2026-01-22 23:25:58.921 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 4309.44
2026-01-22 23:25:58.921 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:25:58.921 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:25:58.922 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.28.mlp.gate_proj using 3 samples
2026-01-22 23:26:00.281 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.36s
2026-01-22 23:26:00.282 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 87072.48
2026-01-22 23:26:00.283 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:26:00.283 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:26:00.283 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.28.mlp.up_proj using 3 samples
2026-01-22 23:26:01.630 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.35s
2026-01-22 23:26:01.632 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 65563.21
2026-01-22 23:26:01.632 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:26:01.632 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:26:01.632 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.28.mlp.down_proj using 3 samples
2026-01-22 23:26:08.621 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.99s
2026-01-22 23:26:08.626 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 6276.17
2026-01-22 23:26:08.626 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.25% | total memory: 25 GB
2026-01-22 23:26:08.626 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:26:08.627 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:26:09.162 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:26:09.162 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.29.self_attn.q_proj using 3 samples
2026-01-22 23:26:10.480 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.32s
2026-01-22 23:26:10.481 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 35790.76
2026-01-22 23:26:10.481 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:26:10.481 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:26:10.482 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.29.self_attn.k_proj using 3 samples
2026-01-22 23:26:11.735 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.25s
2026-01-22 23:26:11.737 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 4829.00
2026-01-22 23:26:11.737 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:26:11.738 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:26:11.738 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.29.self_attn.v_proj using 3 samples
2026-01-22 23:26:13.003 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.26s
2026-01-22 23:26:13.005 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2694.20
2026-01-22 23:26:13.005 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:26:13.005 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:26:13.005 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.29.self_attn.o_proj using 3 samples
2026-01-22 23:26:14.276 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.27s
2026-01-22 23:26:14.277 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1725.67
2026-01-22 23:26:14.277 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:26:14.277 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:26:14.278 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.29.mlp.gate_proj using 3 samples
2026-01-22 23:26:15.653 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.38s
2026-01-22 23:26:15.655 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 103273.83
2026-01-22 23:26:15.655 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:26:15.655 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:26:15.655 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.29.mlp.up_proj using 3 samples
2026-01-22 23:26:17.012 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.36s
2026-01-22 23:26:17.014 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 76694.11
2026-01-22 23:26:17.014 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:26:17.014 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:26:17.014 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.29.mlp.down_proj using 3 samples
2026-01-22 23:26:23.946 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.93s
2026-01-22 23:26:23.951 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 8579.89
2026-01-22 23:26:23.951 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.25% | total memory: 25 GB
2026-01-22 23:26:23.952 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:26:23.952 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:26:24.485 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:26:24.485 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.30.self_attn.q_proj using 3 samples
2026-01-22 23:26:25.791 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.31s
2026-01-22 23:26:25.793 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 38550.55
2026-01-22 23:26:25.793 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:26:25.793 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:26:25.793 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.30.self_attn.k_proj using 3 samples
2026-01-22 23:26:27.030 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.24s
2026-01-22 23:26:27.031 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 5568.43
2026-01-22 23:26:27.031 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:26:27.031 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:26:27.032 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.30.self_attn.v_proj using 3 samples
2026-01-22 23:26:28.273 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.24s
2026-01-22 23:26:28.274 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 9067.57
2026-01-22 23:26:28.275 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:26:28.275 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:26:28.275 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.30.self_attn.o_proj using 3 samples
2026-01-22 23:26:29.538 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.26s
2026-01-22 23:26:29.539 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2361.20
2026-01-22 23:26:29.539 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:26:29.539 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:26:29.540 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.30.mlp.gate_proj using 3 samples
2026-01-22 23:26:30.903 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.36s
2026-01-22 23:26:30.904 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 141040.44
2026-01-22 23:26:30.904 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:26:30.905 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:26:30.905 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.30.mlp.up_proj using 3 samples
2026-01-22 23:26:32.267 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.36s
2026-01-22 23:26:32.269 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 105468.84
2026-01-22 23:26:32.269 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.24% | total memory: 25 GB
2026-01-22 23:26:32.269 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:26:32.270 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.30.mlp.down_proj using 3 samples
2026-01-22 23:26:40.061 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 7.79s
2026-01-22 23:26:40.066 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 38892.09
2026-01-22 23:26:40.080 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.16% | total memory: 25 GB
2026-01-22 23:26:40.081 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:26:40.081 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:26:40.634 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:26:40.634 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.31.self_attn.q_proj using 3 samples
2026-01-22 23:26:42.388 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.75s
2026-01-22 23:26:42.389 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 44195.52
2026-01-22 23:26:42.389 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.15% | total memory: 25 GB
2026-01-22 23:26:42.390 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:26:42.390 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.31.self_attn.k_proj using 3 samples
2026-01-22 23:26:44.086 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.70s
2026-01-22 23:26:44.087 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 6447.58
2026-01-22 23:26:44.087 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.15% | total memory: 25 GB
2026-01-22 23:26:44.087 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:26:44.088 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.31.self_attn.v_proj using 3 samples
2026-01-22 23:26:45.770 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.68s
2026-01-22 23:26:45.771 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 13305.28
2026-01-22 23:26:45.772 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.15% | total memory: 25 GB
2026-01-22 23:26:45.772 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:26:45.772 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.31.self_attn.o_proj using 3 samples
2026-01-22 23:26:47.477 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.70s
2026-01-22 23:26:47.478 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 5184.30
2026-01-22 23:26:47.478 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.15% | total memory: 25 GB
2026-01-22 23:26:47.478 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:26:47.479 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.31.mlp.gate_proj using 3 samples
2026-01-22 23:26:49.272 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.79s
2026-01-22 23:26:49.274 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 152097.50
2026-01-22 23:26:49.274 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.15% | total memory: 25 GB
2026-01-22 23:26:49.274 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:26:49.274 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.31.mlp.up_proj using 3 samples
2026-01-22 23:26:51.077 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.80s
2026-01-22 23:26:51.079 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 130170.89
2026-01-22 23:26:51.079 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.15% | total memory: 25 GB
2026-01-22 23:26:51.079 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:26:51.079 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.31.mlp.down_proj using 3 samples
2026-01-22 23:27:00.455 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 9.38s
2026-01-22 23:27:00.460 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 21461.12
2026-01-22 23:27:00.460 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.16% | total memory: 25 GB
2026-01-22 23:27:00.460 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:27:00.461 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:27:01.019 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:27:01.019 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.32.self_attn.q_proj using 3 samples
2026-01-22 23:27:02.780 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.76s
2026-01-22 23:27:02.781 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 47609.41
2026-01-22 23:27:02.782 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.15% | total memory: 25 GB
2026-01-22 23:27:02.782 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:27:02.782 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.32.self_attn.k_proj using 3 samples
2026-01-22 23:27:04.469 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.69s
2026-01-22 23:27:04.471 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 6178.14
2026-01-22 23:27:04.471 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.15% | total memory: 25 GB
2026-01-22 23:27:04.471 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:27:04.471 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.32.self_attn.v_proj using 3 samples
2026-01-22 23:27:06.174 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.70s
2026-01-22 23:27:06.176 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 41950.00
2026-01-22 23:27:06.176 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.15% | total memory: 25 GB
2026-01-22 23:27:06.176 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:27:06.177 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.32.self_attn.o_proj using 3 samples
2026-01-22 23:27:07.885 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.71s
2026-01-22 23:27:07.886 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 5589.12
2026-01-22 23:27:07.886 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.15% | total memory: 25 GB
2026-01-22 23:27:07.886 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:27:07.887 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.32.mlp.gate_proj using 3 samples
2026-01-22 23:27:09.692 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.81s
2026-01-22 23:27:09.694 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 145678.95
2026-01-22 23:27:09.694 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.15% | total memory: 25 GB
2026-01-22 23:27:09.694 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:27:09.694 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.32.mlp.up_proj using 3 samples
2026-01-22 23:27:11.514 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.82s
2026-01-22 23:27:11.515 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 127900.45
2026-01-22 23:27:11.516 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.15% | total memory: 25 GB
2026-01-22 23:27:11.516 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:27:11.516 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.32.mlp.down_proj using 3 samples
2026-01-22 23:27:20.863 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 9.35s
2026-01-22 23:27:20.867 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 22836.04
2026-01-22 23:27:20.868 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.16% | total memory: 25 GB
2026-01-22 23:27:20.868 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:27:20.868 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:27:21.422 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:27:21.422 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.33.self_attn.q_proj using 3 samples
2026-01-22 23:27:23.196 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.77s
2026-01-22 23:27:23.198 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 58618.07
2026-01-22 23:27:23.198 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.09% | total memory: 25 GB
2026-01-22 23:27:23.198 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:27:23.198 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.33.self_attn.k_proj using 3 samples
2026-01-22 23:27:24.896 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.70s
2026-01-22 23:27:24.897 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 5560.95
2026-01-22 23:27:24.898 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.09% | total memory: 25 GB
2026-01-22 23:27:24.898 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:27:24.898 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.33.self_attn.v_proj using 3 samples
2026-01-22 23:27:26.586 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.69s
2026-01-22 23:27:26.588 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 69089.96
2026-01-22 23:27:26.588 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.09% | total memory: 25 GB
2026-01-22 23:27:26.588 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:27:26.588 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.33.self_attn.o_proj using 3 samples
2026-01-22 23:27:28.291 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.70s
2026-01-22 23:27:28.292 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 3913.39
2026-01-22 23:27:28.293 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.09% | total memory: 25 GB
2026-01-22 23:27:28.293 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:27:28.293 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.33.mlp.gate_proj using 3 samples
2026-01-22 23:27:29.645 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.35s
2026-01-22 23:27:29.647 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 136874.78
2026-01-22 23:27:29.647 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.09% | total memory: 25 GB
2026-01-22 23:27:29.647 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:27:29.647 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.33.mlp.up_proj using 3 samples
2026-01-22 23:27:31.001 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.35s
2026-01-22 23:27:31.002 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 132508.91
2026-01-22 23:27:31.003 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.09% | total memory: 25 GB
2026-01-22 23:27:31.003 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:27:31.003 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.33.mlp.down_proj using 3 samples
2026-01-22 23:27:37.905 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.90s
2026-01-22 23:27:37.912 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 45053.80
2026-01-22 23:27:37.913 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:27:37.913 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:27:37.913 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:27:38.444 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:27:38.445 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.34.self_attn.q_proj using 3 samples
2026-01-22 23:27:39.745 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.30s
2026-01-22 23:27:39.747 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 34718.26
2026-01-22 23:27:39.747 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:27:39.747 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:27:39.748 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.34.self_attn.k_proj using 3 samples
2026-01-22 23:27:40.981 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.23s
2026-01-22 23:27:40.982 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 3960.39
2026-01-22 23:27:40.982 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:27:40.983 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:27:40.983 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.34.self_attn.v_proj using 3 samples
2026-01-22 23:27:42.207 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.22s
2026-01-22 23:27:42.209 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 5691.74
2026-01-22 23:27:42.209 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:27:42.209 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:27:42.209 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.34.self_attn.o_proj using 3 samples
2026-01-22 23:27:43.450 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.24s
2026-01-22 23:27:43.451 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 6349.64
2026-01-22 23:27:43.452 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:27:43.452 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:27:43.452 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.34.mlp.gate_proj using 3 samples
2026-01-22 23:27:44.799 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.35s
2026-01-22 23:27:44.800 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 149382.94
2026-01-22 23:27:44.801 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:27:44.801 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:27:44.801 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.34.mlp.up_proj using 3 samples
2026-01-22 23:27:46.170 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.37s
2026-01-22 23:27:46.171 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 147726.72
2026-01-22 23:27:46.172 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:27:46.172 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:27:46.172 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.34.mlp.down_proj using 3 samples
2026-01-22 23:27:53.066 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.89s
2026-01-22 23:27:53.074 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 35348.78
2026-01-22 23:27:53.074 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:27:53.074 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:27:53.075 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:27:53.569 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:27:53.570 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.35.self_attn.q_proj using 3 samples
2026-01-22 23:27:54.866 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.30s
2026-01-22 23:27:54.867 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 29650.00
2026-01-22 23:27:54.867 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:27:54.868 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.949248 MB
2026-01-22 23:27:54.868 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.35.self_attn.k_proj using 3 samples
2026-01-22 23:27:56.118 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.25s
2026-01-22 23:27:56.119 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 3708.69
2026-01-22 23:27:56.119 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:27:56.119 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:27:56.119 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.35.self_attn.v_proj using 3 samples
2026-01-22 23:27:57.381 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.26s
2026-01-22 23:27:57.382 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 6710.51
2026-01-22 23:27:57.382 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.18% | total memory: 25 GB
2026-01-22 23:27:57.382 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 2.118656 MB
2026-01-22 23:27:57.383 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.35.self_attn.o_proj using 3 samples
2026-01-22 23:27:58.632 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.25s
2026-01-22 23:27:58.633 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 4623.92
2026-01-22 23:27:58.634 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.22% | total memory: 25 GB
2026-01-22 23:27:58.634 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 16.941056 MB
2026-01-22 23:27:58.634 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.35.mlp.gate_proj using 3 samples
2026-01-22 23:28:00.007 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.37s
2026-01-22 23:28:00.009 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 227924.50
2026-01-22 23:28:00.009 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 34.82% | total memory: 25 GB
2026-01-22 23:28:00.009 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:28:00.009 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.35.mlp.up_proj using 3 samples
2026-01-22 23:28:01.373 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.36s
2026-01-22 23:28:01.374 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 211811.22
2026-01-22 23:28:01.375 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 34.97% | total memory: 25 GB
2026-01-22 23:28:01.375 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:28:01.375 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.language_model.layers.35.mlp.down_proj using 3 samples
2026-01-22 23:28:10.582 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 9.21s
2026-01-22 23:28:10.587 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 160853.59
2026-01-22 23:28:10.588 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 43.10% | total memory: 25 GB
2026-01-22 23:28:10.588 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 91.058176 MB
2026-01-22 23:28:10.588 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:28:11.363 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:28:11.363 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:28:12.556 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.CALIBRATION_EPOCH_END
2026-01-22 23:28:12.571 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=True sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:28:12.581 | DEBUG    | llmcompressor.core.lifecycle:finalize:134 - Finalizing compression lifecycle
2026-01-22 23:28:12.582 | DEBUG    | llmcompressor.core.lifecycle:finalize:138 - Finalized modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=True started_=True ended_=True smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:28:12.582 | DEBUG    | llmcompressor.core.lifecycle:finalize:138 - Finalized modifier: config_groups=None targets=['Linear'] ignore=['lm_head', 're:visual.*', 're:model.visual.*'] scheme='W4A16' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=True started_=True ended_=True sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:28:12.582 | INFO     | llmcompressor.core.lifecycle:finalize:144 - Compression lifecycle finalized for 2 modifiers
2026-01-22 23:28:12.714 | INFO     | llmcompressor.transformers.sparsification.compressed_tensors_utils:get_model_compressor:195 - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.
2026-01-22 23:28:38.434 | DEBUG    | llmcompressor.transformers.utils.helpers:recipe_from_huggingface_model_id:146 - Unable to find recipe recipe.yaml for model ID: Qwen/Qwen2.5-VL-3B-Instruct: 404 Client Error. (Request ID: Root=1-69724226-7c0dbaaf2f12405b4d0c3863;25080c0d-ca75-4877-8f38-1430a209f6db)

Entry Not Found for url: https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/recipe.yaml..Skipping recipe resolution.
2026-01-22 23:28:38.436 | DEBUG    | llmcompressor.transformers.utils.helpers:infer_recipe_from_model_path:112 - Failed to infer the recipe from the model_path
2026-01-22 23:28:41.586 | INFO     | llmcompressor.transformers.sparsification.compressed_tensors_utils:get_model_compressor:195 - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.
2026-01-22 23:28:51.327 | DEBUG    | llmcompressor.transformers.utils.helpers:recipe_from_huggingface_model_id:146 - Unable to find recipe recipe.yaml for model ID: Qwen/Qwen2.5-VL-3B-Instruct: 404 Client Error. (Request ID: Root=1-69724233-01ea8eef49cccaec69916c0b;0fa39ed9-9f6c-493b-b35c-c39cb2f25025)

Entry Not Found for url: https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct/resolve/main/recipe.yaml..Skipping recipe resolution.
2026-01-22 23:28:51.330 | DEBUG    | llmcompressor.transformers.utils.helpers:infer_recipe_from_model_path:112 - Failed to infer the recipe from the model_path
