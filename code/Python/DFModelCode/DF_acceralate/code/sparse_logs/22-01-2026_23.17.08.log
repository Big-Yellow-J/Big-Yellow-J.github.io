2026-01-22 23:17:08.839 | INFO     | llmcompressor.metrics.logger:_create_default_logger:356 - Logging all LLM Compressor modifier-level logs to sparse_logs/22-01-2026_23.17.08.log
2026-01-22 23:17:08.840 | DEBUG    | llmcompressor.core.lifecycle:initialize:92 - Initializing compression lifecycle
2026-01-22 23:17:08.840 | INFO     | llmcompressor.recipe.recipe:from_modifiers:68 - Creating recipe from modifiers
2026-01-22 23:17:08.840 | INFO     | llmcompressor.modifiers.smoothquant.base:_infer_mappings_from_model:188 - No SmoothQuantModifier.mappings provided, inferring from model...
2026-01-22 23:17:10.111 | DEBUG    | llmcompressor.core.lifecycle:initialize:105 - Initialized modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=False ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:10.159 | DEBUG    | llmcompressor.core.lifecycle:initialize:105 - Initialized modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=False ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:17:10.159 | INFO     | llmcompressor.core.lifecycle:initialize:110 - Compression lifecycle initialized for 2 modifiers
2026-01-22 23:17:10.159 | INFO     | llmcompressor.pipelines.independent.pipeline:IndependentPipeline:43 - Inferred `SequentialPipeline` for `SmoothQuantModifier`
2026-01-22 23:17:10.548 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:17:10.548 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(input_ids, inputs_embeds):
    if (input_ids is None) ^ (inputs_embeds is not None):
        raise ValueError('You must specify exactly one of input_ids or inputs_embeds')
    return ()
2026-01-22 23:17:10.549 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:17:10.549 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:17:10.549 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_1(input_ids, inputs_embeds):
    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)
    return (inputs_embeds,)
2026-01-22 23:17:10.549 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:17:10.550 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:17:10.550 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_2(past_key_values, use_cache):
    if use_cache and past_key_values is None:
        past_key_values = DynamicCache()
    return (past_key_values,)
2026-01-22 23:17:10.550 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:17:10.550 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:17:10.551 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_3(cache_position, inputs_embeds, past_key_values, *, past_seen_tokens=None):
    if cache_position is None:
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        cache_position = torch.arange(past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device)
    return (cache_position, past_seen_tokens)
2026-01-22 23:17:10.551 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:17:10.551 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:17:10.551 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_4(cache_position, position_ids):
    if position_ids is None:
        position_ids = cache_position.unsqueeze(0)
    return (position_ids,)
2026-01-22 23:17:10.551 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:17:10.552 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:17:10.552 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_5(attention_mask, cache_position, inputs_embeds, past_key_values, position_ids, *, causal_mask_mapping=None, mask_kwargs=None):
    if not isinstance((causal_mask_mapping := attention_mask), dict):
        mask_kwargs = {'config': self.config, 'input_embeds': inputs_embeds, 'attention_mask': attention_mask, 'cache_position': cache_position, 'past_key_values': past_key_values, 'position_ids': position_ids}
        causal_mask_mapping = {'full_attention': create_causal_mask(**mask_kwargs)}
        if self.has_sliding_layers:
            causal_mask_mapping['sliding_attention'] = create_sliding_window_causal_mask(**mask_kwargs)
    return (causal_mask_mapping, mask_kwargs)
2026-01-22 23:17:10.553 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:17:10.558 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:17:10.558 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(kwargs, labels, logits, loss):
    if labels is not None:
        loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
    return (loss,)
2026-01-22 23:17:10.559 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:17:10.739 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.CALIBRATION_EPOCH_START
2026-01-22 23:17:10.740 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8785730>
2026-01-22 23:17:10.740 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8485fd0>
2026-01-22 23:17:10.740 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486060>
2026-01-22 23:17:10.740 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486120>
2026-01-22 23:17:10.740 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e84861e0>
2026-01-22 23:17:10.740 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e84862a0>
2026-01-22 23:17:10.740 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486300>
2026-01-22 23:17:10.741 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486390>
2026-01-22 23:17:10.741 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486450>
2026-01-22 23:17:10.741 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486510>
2026-01-22 23:17:10.741 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e84865d0>
2026-01-22 23:17:10.741 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486690>
2026-01-22 23:17:10.741 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e84865a0>
2026-01-22 23:17:10.741 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486780>
2026-01-22 23:17:10.741 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486750>
2026-01-22 23:17:10.742 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486870>
2026-01-22 23:17:10.742 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486930>
2026-01-22 23:17:10.742 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e84869f0>
2026-01-22 23:17:10.742 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486a80>
2026-01-22 23:17:10.742 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486b40>
2026-01-22 23:17:10.742 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486bd0>
2026-01-22 23:17:10.742 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486c90>
2026-01-22 23:17:10.742 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486d50>
2026-01-22 23:17:10.742 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486e10>
2026-01-22 23:17:10.743 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486ed0>
2026-01-22 23:17:10.743 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8486fc0>
2026-01-22 23:17:10.743 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e84870b0>
2026-01-22 23:17:10.743 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8487170>
2026-01-22 23:17:10.743 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8487260>
2026-01-22 23:17:10.743 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8487350>
2026-01-22 23:17:10.743 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8487440>
2026-01-22 23:17:10.743 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8487530>
2026-01-22 23:17:10.744 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8487620>
2026-01-22 23:17:10.744 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8487710>
2026-01-22 23:17:10.744 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e84877d0>
2026-01-22 23:17:10.744 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e84878c0>
2026-01-22 23:17:10.744 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e84879b0>
2026-01-22 23:17:10.744 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8487aa0>
2026-01-22 23:17:10.744 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8487b90>
2026-01-22 23:17:10.744 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8487c80>
2026-01-22 23:17:10.744 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8487d70>
2026-01-22 23:17:10.745 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8487e30>
2026-01-22 23:17:10.745 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8487f20>
2026-01-22 23:17:10.745 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e8487f80>
2026-01-22 23:17:10.745 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e849c140>
2026-01-22 23:17:10.745 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e849c1d0>
2026-01-22 23:17:10.745 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e849c290>
2026-01-22 23:17:10.745 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e849c380>
2026-01-22 23:17:10.745 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e849c440>
2026-01-22 23:17:10.746 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e849c530>
2026-01-22 23:17:10.746 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e849c620>
2026-01-22 23:17:10.746 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e849c6e0>
2026-01-22 23:17:10.746 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e849c7a0>
2026-01-22 23:17:10.746 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e849c890>
2026-01-22 23:17:10.746 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e849c950>
2026-01-22 23:17:10.746 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None added <torch.utils.hooks.RemovableHandle object at 0x7f56e849ca40>
2026-01-22 23:17:10.746 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:11.397 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:11.398 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.0.input_layernorm
2026-01-22 23:17:11.437 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.0.post_attention_layernorm
2026-01-22 23:17:11.439 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:11.923 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:11.924 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.1.input_layernorm
2026-01-22 23:17:11.925 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.1.post_attention_layernorm
2026-01-22 23:17:11.926 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:12.050 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:12.050 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.2.input_layernorm
2026-01-22 23:17:12.051 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.2.post_attention_layernorm
2026-01-22 23:17:12.052 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:12.173 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:12.173 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.3.input_layernorm
2026-01-22 23:17:12.174 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.3.post_attention_layernorm
2026-01-22 23:17:12.175 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:12.299 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:12.299 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.4.input_layernorm
2026-01-22 23:17:12.300 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.4.post_attention_layernorm
2026-01-22 23:17:12.302 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:12.424 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:12.425 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.5.input_layernorm
2026-01-22 23:17:12.426 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.5.post_attention_layernorm
2026-01-22 23:17:12.427 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:12.545 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:12.546 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.6.input_layernorm
2026-01-22 23:17:12.547 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.6.post_attention_layernorm
2026-01-22 23:17:12.548 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:12.671 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:12.671 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.7.input_layernorm
2026-01-22 23:17:12.672 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.7.post_attention_layernorm
2026-01-22 23:17:12.673 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:12.795 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:12.795 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.8.input_layernorm
2026-01-22 23:17:12.796 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.8.post_attention_layernorm
2026-01-22 23:17:12.797 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:12.913 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:12.913 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.9.input_layernorm
2026-01-22 23:17:12.914 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.9.post_attention_layernorm
2026-01-22 23:17:12.916 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:13.037 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:13.037 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.10.input_layernorm
2026-01-22 23:17:13.038 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.10.post_attention_layernorm
2026-01-22 23:17:13.040 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:13.159 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:13.159 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.11.input_layernorm
2026-01-22 23:17:13.160 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.11.post_attention_layernorm
2026-01-22 23:17:13.161 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:13.284 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:13.284 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.12.input_layernorm
2026-01-22 23:17:13.285 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.12.post_attention_layernorm
2026-01-22 23:17:13.286 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:13.405 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:13.406 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.13.input_layernorm
2026-01-22 23:17:13.407 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.13.post_attention_layernorm
2026-01-22 23:17:13.408 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:13.530 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:13.531 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.14.input_layernorm
2026-01-22 23:17:13.532 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.14.post_attention_layernorm
2026-01-22 23:17:13.533 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:13.651 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:13.651 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.15.input_layernorm
2026-01-22 23:17:13.652 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.15.post_attention_layernorm
2026-01-22 23:17:13.653 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:13.772 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:13.772 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.16.input_layernorm
2026-01-22 23:17:13.773 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.16.post_attention_layernorm
2026-01-22 23:17:13.775 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:13.898 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:13.898 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.17.input_layernorm
2026-01-22 23:17:13.899 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.17.post_attention_layernorm
2026-01-22 23:17:13.900 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:14.024 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:14.025 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.18.input_layernorm
2026-01-22 23:17:14.026 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.18.post_attention_layernorm
2026-01-22 23:17:14.027 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:14.151 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:14.152 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.19.input_layernorm
2026-01-22 23:17:14.153 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.19.post_attention_layernorm
2026-01-22 23:17:14.154 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:14.278 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:14.278 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.20.input_layernorm
2026-01-22 23:17:14.279 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.20.post_attention_layernorm
2026-01-22 23:17:14.280 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:14.400 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:14.401 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.21.input_layernorm
2026-01-22 23:17:14.402 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.21.post_attention_layernorm
2026-01-22 23:17:14.403 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:14.526 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:14.527 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.22.input_layernorm
2026-01-22 23:17:14.528 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.22.post_attention_layernorm
2026-01-22 23:17:14.529 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:14.649 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:14.649 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.23.input_layernorm
2026-01-22 23:17:14.650 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.23.post_attention_layernorm
2026-01-22 23:17:14.651 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:14.774 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:14.774 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.24.input_layernorm
2026-01-22 23:17:14.775 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.24.post_attention_layernorm
2026-01-22 23:17:14.776 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:14.900 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:14.900 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.25.input_layernorm
2026-01-22 23:17:14.901 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.25.post_attention_layernorm
2026-01-22 23:17:14.902 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:15.024 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:15.024 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.26.input_layernorm
2026-01-22 23:17:15.025 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.26.post_attention_layernorm
2026-01-22 23:17:15.032 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:15.190 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:15.190 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.27.input_layernorm
2026-01-22 23:17:15.191 | INFO     | llmcompressor.modifiers.smoothquant.base:_apply_smoothing:276 - Smoothing with model.layers.27.post_attention_layernorm
2026-01-22 23:17:15.192 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:15.387 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:15.387 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:15.919 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.CALIBRATION_EPOCH_END
2026-01-22 23:17:15.919 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=True smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:17:15.923 | INFO     | llmcompressor.pipelines.independent.pipeline:IndependentPipeline:43 - Inferred `SequentialPipeline` for `GPTQModifier`
2026-01-22 23:17:16.105 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:17:16.106 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(input_ids, inputs_embeds):
    if (input_ids is None) ^ (inputs_embeds is not None):
        raise ValueError('You must specify exactly one of input_ids or inputs_embeds')
    return ()
2026-01-22 23:17:16.106 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:17:16.106 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:17:16.106 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_1(input_ids, inputs_embeds):
    if inputs_embeds is None:
        inputs_embeds = self.embed_tokens(input_ids)
    return (inputs_embeds,)
2026-01-22 23:17:16.106 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:17:16.107 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:17:16.107 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_2(past_key_values, use_cache):
    if use_cache and past_key_values is None:
        past_key_values = DynamicCache()
    return (past_key_values,)
2026-01-22 23:17:16.107 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:17:16.107 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:17:16.108 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_3(cache_position, inputs_embeds, past_key_values, *, past_seen_tokens=None):
    if cache_position is None:
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        cache_position = torch.arange(past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device)
    return (cache_position, past_seen_tokens)
2026-01-22 23:17:16.108 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:17:16.108 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:17:16.108 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_4(cache_position, position_ids):
    if position_ids is None:
        position_ids = cache_position.unsqueeze(0)
    return (position_ids,)
2026-01-22 23:17:16.108 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:17:16.109 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:17:16.109 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_5(attention_mask, cache_position, inputs_embeds, past_key_values, position_ids, *, causal_mask_mapping=None, mask_kwargs=None):
    if not isinstance((causal_mask_mapping := attention_mask), dict):
        mask_kwargs = {'config': self.config, 'input_embeds': inputs_embeds, 'attention_mask': attention_mask, 'cache_position': cache_position, 'past_key_values': past_key_values, 'position_ids': position_ids}
        causal_mask_mapping = {'full_attention': create_causal_mask(**mask_kwargs)}
        if self.has_sliding_layers:
            causal_mask_mapping['sliding_attention'] = create_sliding_window_causal_mask(**mask_kwargs)
    return (causal_mask_mapping, mask_kwargs)
2026-01-22 23:17:16.110 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:17:16.115 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:267 - ---- Autowrapper ----
2026-01-22 23:17:16.115 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:268 - @torch.fx.wrap
def wrapped_0(kwargs, labels, logits, loss):
    if labels is not None:
        loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)
    return (loss,)
2026-01-22 23:17:16.115 | DEBUG    | llmcompressor.pipelines.sequential.ast_utils.auto_wrapper:_wrap_stmt:269 - ---------------------
2026-01-22 23:17:16.279 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.CALIBRATION_EPOCH_START
2026-01-22 23:17:16.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56ea828b30>
2026-01-22 23:17:16.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8785fa0>
2026-01-22 23:17:16.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8785400>
2026-01-22 23:17:16.287 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e82bb5c0>
2026-01-22 23:17:16.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e82bb6e0>
2026-01-22 23:17:16.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e82bb5f0>
2026-01-22 23:17:16.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e82bb800>
2026-01-22 23:17:16.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8784140>
2026-01-22 23:17:16.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83a5160>
2026-01-22 23:17:16.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e82bb9b0>
2026-01-22 23:17:16.288 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e82bbad0>
2026-01-22 23:17:16.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e82bbb30>
2026-01-22 23:17:16.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e82bbc20>
2026-01-22 23:17:16.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e82bba10>
2026-01-22 23:17:16.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e82bbd70>
2026-01-22 23:17:16.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83a7da0>
2026-01-22 23:17:16.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e82bbef0>
2026-01-22 23:17:16.289 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e82bbfb0>
2026-01-22 23:17:16.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8300050>
2026-01-22 23:17:16.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83001a0>
2026-01-22 23:17:16.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e849ccb0>
2026-01-22 23:17:16.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8300320>
2026-01-22 23:17:16.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8300380>
2026-01-22 23:17:16.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83004a0>
2026-01-22 23:17:16.290 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8300500>
2026-01-22 23:17:16.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83005f0>
2026-01-22 23:17:16.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8300680>
2026-01-22 23:17:16.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8300740>
2026-01-22 23:17:16.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8300860>
2026-01-22 23:17:16.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83008c0>
2026-01-22 23:17:16.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83009b0>
2026-01-22 23:17:16.291 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83009e0>
2026-01-22 23:17:16.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8300b00>
2026-01-22 23:17:16.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8300bc0>
2026-01-22 23:17:16.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8300cb0>
2026-01-22 23:17:16.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8300b30>
2026-01-22 23:17:16.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8785610>
2026-01-22 23:17:16.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8300e30>
2026-01-22 23:17:16.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8300ec0>
2026-01-22 23:17:16.292 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8300f80>
2026-01-22 23:17:16.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301040>
2026-01-22 23:17:16.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301100>
2026-01-22 23:17:16.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301160>
2026-01-22 23:17:16.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301280>
2026-01-22 23:17:16.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301370>
2026-01-22 23:17:16.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301430>
2026-01-22 23:17:16.293 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83012e0>
2026-01-22 23:17:16.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301550>
2026-01-22 23:17:16.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301640>
2026-01-22 23:17:16.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301760>
2026-01-22 23:17:16.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83017f0>
2026-01-22 23:17:16.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83018e0>
2026-01-22 23:17:16.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301970>
2026-01-22 23:17:16.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301a00>
2026-01-22 23:17:16.294 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301af0>
2026-01-22 23:17:16.295 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301be0>
2026-01-22 23:17:16.295 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301b80>
2026-01-22 23:17:16.295 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301d30>
2026-01-22 23:17:16.295 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301dc0>
2026-01-22 23:17:16.295 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301e80>
2026-01-22 23:17:16.295 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301f40>
2026-01-22 23:17:16.295 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302030>
2026-01-22 23:17:16.295 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83020f0>
2026-01-22 23:17:16.296 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8301190>
2026-01-22 23:17:16.296 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83021b0>
2026-01-22 23:17:16.296 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302270>
2026-01-22 23:17:16.296 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302360>
2026-01-22 23:17:16.296 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302450>
2026-01-22 23:17:16.296 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302540>
2026-01-22 23:17:16.296 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302600>
2026-01-22 23:17:16.297 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302720>
2026-01-22 23:17:16.297 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83027e0>
2026-01-22 23:17:16.297 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302870>
2026-01-22 23:17:16.297 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302960>
2026-01-22 23:17:16.297 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302a50>
2026-01-22 23:17:16.297 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302b40>
2026-01-22 23:17:16.297 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302c30>
2026-01-22 23:17:16.297 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302cf0>
2026-01-22 23:17:16.298 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302d80>
2026-01-22 23:17:16.298 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302e40>
2026-01-22 23:17:16.298 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302f00>
2026-01-22 23:17:16.298 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8302ff0>
2026-01-22 23:17:16.298 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83030b0>
2026-01-22 23:17:16.298 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8303170>
2026-01-22 23:17:16.298 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8303260>
2026-01-22 23:17:16.298 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83032f0>
2026-01-22 23:17:16.299 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83031a0>
2026-01-22 23:17:16.299 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8303410>
2026-01-22 23:17:16.299 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8741100>
2026-01-22 23:17:16.299 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8303560>
2026-01-22 23:17:16.299 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83034a0>
2026-01-22 23:17:16.299 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8303680>
2026-01-22 23:17:16.299 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8303770>
2026-01-22 23:17:16.300 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83037a0>
2026-01-22 23:17:16.300 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83035c0>
2026-01-22 23:17:16.300 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83038f0>
2026-01-22 23:17:16.300 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83039b0>
2026-01-22 23:17:16.300 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8303a70>
2026-01-22 23:17:16.300 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8303b30>
2026-01-22 23:17:16.300 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8303bf0>
2026-01-22 23:17:16.301 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8303ce0>
2026-01-22 23:17:16.301 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8303d70>
2026-01-22 23:17:16.301 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8303e00>
2026-01-22 23:17:16.301 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8303ef0>
2026-01-22 23:17:16.301 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8303fb0>
2026-01-22 23:17:16.301 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e87859d0>
2026-01-22 23:17:16.301 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83241a0>
2026-01-22 23:17:16.302 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8324230>
2026-01-22 23:17:16.302 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8324290>
2026-01-22 23:17:16.302 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83243b0>
2026-01-22 23:17:16.302 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8324470>
2026-01-22 23:17:16.302 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8324530>
2026-01-22 23:17:16.302 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83245f0>
2026-01-22 23:17:16.302 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83246e0>
2026-01-22 23:17:16.302 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83247a0>
2026-01-22 23:17:16.303 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8324860>
2026-01-22 23:17:16.303 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8324920>
2026-01-22 23:17:16.303 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8324a10>
2026-01-22 23:17:16.303 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83249b0>
2026-01-22 23:17:16.303 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8324b60>
2026-01-22 23:17:16.303 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8324bf0>
2026-01-22 23:17:16.303 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8324d10>
2026-01-22 23:17:16.304 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8324ad0>
2026-01-22 23:17:16.304 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8324e30>
2026-01-22 23:17:16.304 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8324f20>
2026-01-22 23:17:16.304 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8324dd0>
2026-01-22 23:17:16.304 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325070>
2026-01-22 23:17:16.304 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8324e60>
2026-01-22 23:17:16.304 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325160>
2026-01-22 23:17:16.304 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325250>
2026-01-22 23:17:16.305 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325310>
2026-01-22 23:17:16.305 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325400>
2026-01-22 23:17:16.305 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83254f0>
2026-01-22 23:17:16.305 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83255b0>
2026-01-22 23:17:16.305 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325670>
2026-01-22 23:17:16.305 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325730>
2026-01-22 23:17:16.305 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325790>
2026-01-22 23:17:16.306 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83258b0>
2026-01-22 23:17:16.306 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325970>
2026-01-22 23:17:16.306 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325a30>
2026-01-22 23:17:16.306 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83259d0>
2026-01-22 23:17:16.306 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83250d0>
2026-01-22 23:17:16.306 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325be0>
2026-01-22 23:17:16.306 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325ca0>
2026-01-22 23:17:16.307 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325d60>
2026-01-22 23:17:16.307 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325e50>
2026-01-22 23:17:16.307 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325df0>
2026-01-22 23:17:16.307 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325f40>
2026-01-22 23:17:16.307 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8325f70>
2026-01-22 23:17:16.307 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326090>
2026-01-22 23:17:16.307 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326180>
2026-01-22 23:17:16.307 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326240>
2026-01-22 23:17:16.308 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83262d0>
2026-01-22 23:17:16.308 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326300>
2026-01-22 23:17:16.308 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326420>
2026-01-22 23:17:16.308 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326510>
2026-01-22 23:17:16.308 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326600>
2026-01-22 23:17:16.308 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83266c0>
2026-01-22 23:17:16.308 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8740440>
2026-01-22 23:17:16.309 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83267b0>
2026-01-22 23:17:16.309 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326390>
2026-01-22 23:17:16.309 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326720>
2026-01-22 23:17:16.309 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326930>
2026-01-22 23:17:16.309 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326a20>
2026-01-22 23:17:16.309 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326ae0>
2026-01-22 23:17:16.309 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326ba0>
2026-01-22 23:17:16.310 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326c90>
2026-01-22 23:17:16.310 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326d80>
2026-01-22 23:17:16.310 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326e70>
2026-01-22 23:17:16.310 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326f00>
2026-01-22 23:17:16.310 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8326fc0>
2026-01-22 23:17:16.310 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8327050>
2026-01-22 23:17:16.310 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8327080>
2026-01-22 23:17:16.311 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83270b0>
2026-01-22 23:17:16.311 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83271a0>
2026-01-22 23:17:16.311 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8327260>
2026-01-22 23:17:16.311 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8327290>
2026-01-22 23:17:16.311 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83273b0>
2026-01-22 23:17:16.311 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83274a0>
2026-01-22 23:17:16.311 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8327500>
2026-01-22 23:17:16.311 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8327620>
2026-01-22 23:17:16.312 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83276e0>
2026-01-22 23:17:16.312 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e83277a0>
2026-01-22 23:17:16.312 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8327860>
2026-01-22 23:17:16.312 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8327950>
2026-01-22 23:17:16.312 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8327a10>
2026-01-22 23:17:16.312 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8327b00>
2026-01-22 23:17:16.312 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8327bf0>
2026-01-22 23:17:16.313 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8327ce0>
2026-01-22 23:17:16.313 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8327da0>
2026-01-22 23:17:16.313 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8327e90>
2026-01-22 23:17:16.313 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8327f50>
2026-01-22 23:17:16.313 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e8327fb0>
2026-01-22 23:17:16.313 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e833c140>
2026-01-22 23:17:16.313 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e833c200>
2026-01-22 23:17:16.313 | DEBUG    | llmcompressor.modifiers.utils.hooks:register_hook:98 - config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False added <torch.utils.hooks.RemovableHandle object at 0x7f56e833c2f0>
2026-01-22 23:17:16.314 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:17:16.501 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:16.501 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.0.self_attn.q_proj using 3 samples
2026-01-22 23:17:17.497 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.00s
2026-01-22 23:17:17.498 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2.54
2026-01-22 23:17:17.498 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 23.25% | total memory: 25 GB
2026-01-22 23:17:17.498 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:17:17.499 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.0.self_attn.k_proj using 3 samples
2026-01-22 23:17:18.481 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 0.98s
2026-01-22 23:17:18.482 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.03
2026-01-22 23:17:18.482 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 24.49% | total memory: 25 GB
2026-01-22 23:17:18.482 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:17:18.483 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.0.self_attn.v_proj using 3 samples
2026-01-22 23:17:19.510 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.03s
2026-01-22 23:17:19.510 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.00
2026-01-22 23:17:19.511 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 32.58% | total memory: 25 GB
2026-01-22 23:17:19.511 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:17:19.511 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.0.self_attn.o_proj using 3 samples
2026-01-22 23:17:21.215 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.70s
2026-01-22 23:17:21.216 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:17:21.216 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 46.69% | total memory: 25 GB
2026-01-22 23:17:21.216 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:17:21.217 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.0.mlp.gate_proj using 3 samples
2026-01-22 23:17:22.367 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.15s
2026-01-22 23:17:22.368 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.16
2026-01-22 23:17:22.368 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 46.69% | total memory: 25 GB
2026-01-22 23:17:22.368 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:17:22.369 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.0.mlp.up_proj using 3 samples
2026-01-22 23:17:23.507 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.14s
2026-01-22 23:17:23.508 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.08
2026-01-22 23:17:23.509 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 46.69% | total memory: 25 GB
2026-01-22 23:17:23.509 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:17:23.509 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.0.mlp.down_proj using 3 samples
2026-01-22 23:17:29.892 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.38s
2026-01-22 23:17:29.896 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:17:29.897 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 47.96% | total memory: 25 GB
2026-01-22 23:17:29.897 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:17:29.897 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:17:30.382 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:30.382 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.1.self_attn.q_proj using 3 samples
2026-01-22 23:17:31.562 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.18s
2026-01-22 23:17:31.563 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.08
2026-01-22 23:17:31.563 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 44.89% | total memory: 25 GB
2026-01-22 23:17:31.563 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:17:31.563 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.1.self_attn.k_proj using 3 samples
2026-01-22 23:17:32.643 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:17:32.644 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.02
2026-01-22 23:17:32.644 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 44.89% | total memory: 25 GB
2026-01-22 23:17:32.644 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:17:32.645 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.1.self_attn.v_proj using 3 samples
2026-01-22 23:17:33.729 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:17:33.730 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.00
2026-01-22 23:17:33.730 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 44.89% | total memory: 25 GB
2026-01-22 23:17:33.730 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:17:33.731 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.1.self_attn.o_proj using 3 samples
2026-01-22 23:17:34.819 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:17:34.820 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.00
2026-01-22 23:17:34.820 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 44.89% | total memory: 25 GB
2026-01-22 23:17:34.820 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:17:34.821 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.1.mlp.gate_proj using 3 samples
2026-01-22 23:17:35.963 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.14s
2026-01-22 23:17:35.964 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 3.01
2026-01-22 23:17:35.965 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 44.89% | total memory: 25 GB
2026-01-22 23:17:35.965 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:17:35.965 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.1.mlp.up_proj using 3 samples
2026-01-22 23:17:37.102 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.14s
2026-01-22 23:17:37.103 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1.10
2026-01-22 23:17:37.103 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 44.89% | total memory: 25 GB
2026-01-22 23:17:37.103 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:17:37.103 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.1.mlp.down_proj using 3 samples
2026-01-22 23:17:37.263 | WARNING  | llmcompressor.modifiers.quantization.gptq.gptq_quantize:quantize_weight:165 - Failed to invert hessian due to numerical instability. Consider increasing GPTQModifier.dampening_frac, increasing the number of calibration samples, or shuffling the calibration dataset. Falling back to round-to-nearest for this module.
2026-01-22 23:17:43.583 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.48s
2026-01-22 23:17:43.583 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.72
2026-01-22 23:17:43.583 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 22.94% | total memory: 25 GB
2026-01-22 23:17:43.584 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:17:43.584 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:17:43.704 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:43.704 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.2.self_attn.q_proj using 3 samples
2026-01-22 23:17:44.760 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.06s
2026-01-22 23:17:44.761 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.15
2026-01-22 23:17:44.761 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.54% | total memory: 25 GB
2026-01-22 23:17:44.761 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:17:44.762 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.2.self_attn.k_proj using 3 samples
2026-01-22 23:17:45.837 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:17:45.838 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.03
2026-01-22 23:17:45.839 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.54% | total memory: 25 GB
2026-01-22 23:17:45.839 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:17:45.839 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.2.self_attn.v_proj using 3 samples
2026-01-22 23:17:46.911 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:17:46.912 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.00
2026-01-22 23:17:46.912 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.54% | total memory: 25 GB
2026-01-22 23:17:46.913 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:17:46.913 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.2.self_attn.o_proj using 3 samples
2026-01-22 23:17:48.002 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:17:48.003 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.00
2026-01-22 23:17:48.004 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.55% | total memory: 25 GB
2026-01-22 23:17:48.004 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:17:48.004 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.2.mlp.gate_proj using 3 samples
2026-01-22 23:17:49.141 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.14s
2026-01-22 23:17:49.142 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 13.96
2026-01-22 23:17:49.142 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.55% | total memory: 25 GB
2026-01-22 23:17:49.143 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:17:49.143 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.2.mlp.up_proj using 3 samples
2026-01-22 23:17:50.284 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.14s
2026-01-22 23:17:50.285 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1.27
2026-01-22 23:17:50.285 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.55% | total memory: 25 GB
2026-01-22 23:17:50.286 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:17:50.286 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.2.mlp.down_proj using 3 samples
2026-01-22 23:17:56.706 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.42s
2026-01-22 23:17:56.710 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 173.34
2026-01-22 23:17:56.710 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.80% | total memory: 25 GB
2026-01-22 23:17:56.711 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:17:56.711 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:17:56.839 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:17:56.840 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.3.self_attn.q_proj using 3 samples
2026-01-22 23:17:57.919 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:17:57.920 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.15
2026-01-22 23:17:57.920 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.54% | total memory: 25 GB
2026-01-22 23:17:57.920 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:17:57.921 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.3.self_attn.k_proj using 3 samples
2026-01-22 23:17:58.996 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:17:58.997 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.04
2026-01-22 23:17:58.997 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.54% | total memory: 25 GB
2026-01-22 23:17:58.998 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:17:58.998 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.3.self_attn.v_proj using 3 samples
2026-01-22 23:18:00.073 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:18:00.074 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:18:00.075 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.54% | total memory: 25 GB
2026-01-22 23:18:00.076 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:18:00.076 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.3.self_attn.o_proj using 3 samples
2026-01-22 23:18:01.158 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:18:01.159 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.00
2026-01-22 23:18:01.159 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.55% | total memory: 25 GB
2026-01-22 23:18:01.159 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:18:01.160 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.3.mlp.gate_proj using 3 samples
2026-01-22 23:18:02.293 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.13s
2026-01-22 23:18:02.294 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 13.04
2026-01-22 23:18:02.294 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.55% | total memory: 25 GB
2026-01-22 23:18:02.294 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:18:02.294 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.3.mlp.up_proj using 3 samples
2026-01-22 23:18:03.445 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.15s
2026-01-22 23:18:03.446 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.57
2026-01-22 23:18:03.446 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.56% | total memory: 25 GB
2026-01-22 23:18:03.446 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:18:03.446 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.3.mlp.down_proj using 3 samples
2026-01-22 23:18:09.960 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.51s
2026-01-22 23:18:09.964 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.15
2026-01-22 23:18:09.964 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.90% | total memory: 25 GB
2026-01-22 23:18:09.964 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:18:09.964 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:18:10.095 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:18:10.095 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.4.self_attn.q_proj using 3 samples
2026-01-22 23:18:11.162 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:18:11.163 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.19
2026-01-22 23:18:11.163 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:11.163 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:18:11.163 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.4.self_attn.k_proj using 3 samples
2026-01-22 23:18:12.238 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:18:12.239 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.04
2026-01-22 23:18:12.239 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:12.240 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:18:12.240 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.4.self_attn.v_proj using 3 samples
2026-01-22 23:18:13.291 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.05s
2026-01-22 23:18:13.292 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:18:13.293 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:13.293 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:18:13.293 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.4.self_attn.o_proj using 3 samples
2026-01-22 23:18:14.473 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.18s
2026-01-22 23:18:14.474 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.00
2026-01-22 23:18:14.474 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:14.474 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:18:14.475 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.4.mlp.gate_proj using 3 samples
2026-01-22 23:18:15.629 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.15s
2026-01-22 23:18:15.630 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 7.94
2026-01-22 23:18:15.630 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:15.631 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:18:15.631 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.4.mlp.up_proj using 3 samples
2026-01-22 23:18:16.784 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.15s
2026-01-22 23:18:16.785 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.32
2026-01-22 23:18:16.785 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:16.785 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:18:16.786 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.4.mlp.down_proj using 3 samples
2026-01-22 23:18:23.330 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.54s
2026-01-22 23:18:23.334 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.06
2026-01-22 23:18:23.334 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 20.65% | total memory: 25 GB
2026-01-22 23:18:23.334 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:18:23.334 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:18:23.461 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:18:23.461 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.5.self_attn.q_proj using 3 samples
2026-01-22 23:18:24.626 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.16s
2026-01-22 23:18:24.627 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.20
2026-01-22 23:18:24.627 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.63% | total memory: 25 GB
2026-01-22 23:18:24.627 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:18:24.628 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.5.self_attn.k_proj using 3 samples
2026-01-22 23:18:25.713 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:18:25.713 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.04
2026-01-22 23:18:25.714 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:25.714 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:18:25.714 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.5.self_attn.v_proj using 3 samples
2026-01-22 23:18:26.801 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:18:26.802 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:18:26.802 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:26.803 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:18:26.803 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.5.self_attn.o_proj using 3 samples
2026-01-22 23:18:27.894 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:18:27.895 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:18:27.895 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:27.895 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:18:27.896 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.5.mlp.gate_proj using 3 samples
2026-01-22 23:18:29.068 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.17s
2026-01-22 23:18:29.069 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 14.10
2026-01-22 23:18:29.069 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:29.070 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:18:29.070 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.5.mlp.up_proj using 3 samples
2026-01-22 23:18:30.213 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.14s
2026-01-22 23:18:30.214 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.69
2026-01-22 23:18:30.214 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:30.214 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:18:30.215 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.5.mlp.down_proj using 3 samples
2026-01-22 23:18:36.681 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.47s
2026-01-22 23:18:36.685 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.23
2026-01-22 23:18:36.685 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.89% | total memory: 25 GB
2026-01-22 23:18:36.686 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:18:36.686 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:18:36.813 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:18:36.813 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.6.self_attn.q_proj using 3 samples
2026-01-22 23:18:37.889 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:18:37.890 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.25
2026-01-22 23:18:37.890 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:37.890 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:18:37.891 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.6.self_attn.k_proj using 3 samples
2026-01-22 23:18:38.980 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:18:38.981 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.05
2026-01-22 23:18:38.982 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:38.982 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:18:38.982 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.6.self_attn.v_proj using 3 samples
2026-01-22 23:18:40.068 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:18:40.069 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:18:40.070 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:40.070 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:18:40.071 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.6.self_attn.o_proj using 3 samples
2026-01-22 23:18:41.159 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:18:41.160 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.00
2026-01-22 23:18:41.160 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:41.160 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:18:41.160 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.6.mlp.gate_proj using 3 samples
2026-01-22 23:18:42.315 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.15s
2026-01-22 23:18:42.316 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1.82
2026-01-22 23:18:42.316 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.64% | total memory: 25 GB
2026-01-22 23:18:42.316 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:18:42.317 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.6.mlp.up_proj using 3 samples
2026-01-22 23:18:43.520 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.20s
2026-01-22 23:18:43.521 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.24
2026-01-22 23:18:43.521 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.24% | total memory: 25 GB
2026-01-22 23:18:43.522 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:18:43.522 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.6.mlp.down_proj using 3 samples
2026-01-22 23:18:49.977 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.45s
2026-01-22 23:18:49.980 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.02
2026-01-22 23:18:49.981 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.50% | total memory: 25 GB
2026-01-22 23:18:49.981 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:18:49.981 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:18:50.122 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:18:50.122 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.7.self_attn.q_proj using 3 samples
2026-01-22 23:18:51.199 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:18:51.200 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.14
2026-01-22 23:18:51.200 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.25% | total memory: 25 GB
2026-01-22 23:18:51.200 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:18:51.201 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.7.self_attn.k_proj using 3 samples
2026-01-22 23:18:52.235 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.03s
2026-01-22 23:18:52.236 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.03
2026-01-22 23:18:52.236 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.25% | total memory: 25 GB
2026-01-22 23:18:52.237 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:18:52.237 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.7.self_attn.v_proj using 3 samples
2026-01-22 23:18:53.310 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:18:53.310 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:18:53.311 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.25% | total memory: 25 GB
2026-01-22 23:18:53.311 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:18:53.311 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.7.self_attn.o_proj using 3 samples
2026-01-22 23:18:54.498 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.19s
2026-01-22 23:18:54.499 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:18:54.500 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.25% | total memory: 25 GB
2026-01-22 23:18:54.500 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:18:54.500 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.7.mlp.gate_proj using 3 samples
2026-01-22 23:18:55.659 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.16s
2026-01-22 23:18:55.660 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1.17
2026-01-22 23:18:55.660 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.25% | total memory: 25 GB
2026-01-22 23:18:55.660 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:18:55.661 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.7.mlp.up_proj using 3 samples
2026-01-22 23:18:56.817 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.16s
2026-01-22 23:18:56.818 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.25
2026-01-22 23:18:56.818 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 29.25% | total memory: 25 GB
2026-01-22 23:18:56.818 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:18:56.819 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.7.mlp.down_proj using 3 samples
2026-01-22 23:19:03.347 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.53s
2026-01-22 23:19:03.351 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.02
2026-01-22 23:19:03.351 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.91% | total memory: 25 GB
2026-01-22 23:19:03.351 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:19:03.351 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:19:03.476 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:19:03.476 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.8.self_attn.q_proj using 3 samples
2026-01-22 23:19:04.552 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:19:04.553 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.22
2026-01-22 23:19:04.553 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.65% | total memory: 25 GB
2026-01-22 23:19:04.554 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:19:04.554 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.8.self_attn.k_proj using 3 samples
2026-01-22 23:19:05.625 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:19:05.626 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.06
2026-01-22 23:19:05.626 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:05.626 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:19:05.626 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.8.self_attn.v_proj using 3 samples
2026-01-22 23:19:06.714 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:19:06.715 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:19:06.715 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:06.715 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:19:06.715 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.8.self_attn.o_proj using 3 samples
2026-01-22 23:19:07.792 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:19:07.793 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.00
2026-01-22 23:19:07.793 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:07.793 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:19:07.794 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.8.mlp.gate_proj using 3 samples
2026-01-22 23:19:08.936 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.14s
2026-01-22 23:19:08.937 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1.35
2026-01-22 23:19:08.938 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:08.938 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:19:08.938 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.8.mlp.up_proj using 3 samples
2026-01-22 23:19:10.046 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.11s
2026-01-22 23:19:10.047 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.25
2026-01-22 23:19:10.064 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:10.064 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:19:10.064 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.8.mlp.down_proj using 3 samples
2026-01-22 23:19:16.592 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.53s
2026-01-22 23:19:16.596 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.02
2026-01-22 23:19:16.596 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:19:16.597 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:19:16.597 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:19:16.721 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:19:16.721 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.9.self_attn.q_proj using 3 samples
2026-01-22 23:19:17.810 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:19:17.811 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.15
2026-01-22 23:19:17.811 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:17.811 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:19:17.811 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.9.self_attn.k_proj using 3 samples
2026-01-22 23:19:18.891 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:19:18.892 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.04
2026-01-22 23:19:18.892 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:18.892 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:19:18.893 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.9.self_attn.v_proj using 3 samples
2026-01-22 23:19:19.971 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:19:19.972 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:19:19.973 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:19.973 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:19:19.973 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.9.self_attn.o_proj using 3 samples
2026-01-22 23:19:21.048 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:19:21.049 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:19:21.049 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:21.049 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:19:21.050 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.9.mlp.gate_proj using 3 samples
2026-01-22 23:19:22.226 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.18s
2026-01-22 23:19:22.227 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.97
2026-01-22 23:19:22.228 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.67% | total memory: 25 GB
2026-01-22 23:19:22.228 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:19:22.228 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.9.mlp.up_proj using 3 samples
2026-01-22 23:19:23.387 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.16s
2026-01-22 23:19:23.388 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.22
2026-01-22 23:19:23.389 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.65% | total memory: 25 GB
2026-01-22 23:19:23.389 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:19:23.389 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.9.mlp.down_proj using 3 samples
2026-01-22 23:19:29.762 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.37s
2026-01-22 23:19:29.766 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.02
2026-01-22 23:19:29.766 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:19:29.766 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:19:29.767 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:19:29.895 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:19:29.896 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.10.self_attn.q_proj using 3 samples
2026-01-22 23:19:30.961 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.06s
2026-01-22 23:19:30.962 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.16
2026-01-22 23:19:30.962 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:30.962 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:19:30.963 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.10.self_attn.k_proj using 3 samples
2026-01-22 23:19:32.008 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.05s
2026-01-22 23:19:32.009 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.04
2026-01-22 23:19:32.010 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:32.010 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:19:32.010 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.10.self_attn.v_proj using 3 samples
2026-01-22 23:19:33.087 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:19:33.088 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:19:33.089 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:33.089 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:19:33.089 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.10.self_attn.o_proj using 3 samples
2026-01-22 23:19:34.239 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.15s
2026-01-22 23:19:34.240 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:19:34.240 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:34.241 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:19:34.241 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.10.mlp.gate_proj using 3 samples
2026-01-22 23:19:35.387 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.15s
2026-01-22 23:19:35.388 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1.07
2026-01-22 23:19:35.388 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:35.389 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:19:35.389 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.10.mlp.up_proj using 3 samples
2026-01-22 23:19:36.524 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.13s
2026-01-22 23:19:36.525 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.24
2026-01-22 23:19:36.525 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:36.525 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:19:36.526 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.10.mlp.down_proj using 3 samples
2026-01-22 23:19:42.880 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.35s
2026-01-22 23:19:42.884 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:19:42.884 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.91% | total memory: 25 GB
2026-01-22 23:19:42.884 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:19:42.885 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:19:43.004 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:19:43.004 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.11.self_attn.q_proj using 3 samples
2026-01-22 23:19:44.097 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:19:44.097 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.15
2026-01-22 23:19:44.098 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:44.098 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:19:44.098 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.11.self_attn.k_proj using 3 samples
2026-01-22 23:19:45.177 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:19:45.178 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.04
2026-01-22 23:19:45.179 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:45.179 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:19:45.179 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.11.self_attn.v_proj using 3 samples
2026-01-22 23:19:46.244 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:19:46.245 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:19:46.246 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:46.246 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:19:46.246 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.11.self_attn.o_proj using 3 samples
2026-01-22 23:19:47.308 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.06s
2026-01-22 23:19:47.309 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:19:47.309 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:47.310 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:19:47.310 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.11.mlp.gate_proj using 3 samples
2026-01-22 23:19:48.444 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.13s
2026-01-22 23:19:48.445 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1.40
2026-01-22 23:19:48.445 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:48.446 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:19:48.446 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.11.mlp.up_proj using 3 samples
2026-01-22 23:19:49.550 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.10s
2026-01-22 23:19:49.551 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.28
2026-01-22 23:19:49.551 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:49.551 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:19:49.551 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.11.mlp.down_proj using 3 samples
2026-01-22 23:19:56.004 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.45s
2026-01-22 23:19:56.008 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:19:56.008 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:19:56.008 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:19:56.009 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:19:56.136 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:19:56.136 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.12.self_attn.q_proj using 3 samples
2026-01-22 23:19:57.216 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:19:57.217 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.18
2026-01-22 23:19:57.217 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:57.218 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:19:57.218 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.12.self_attn.k_proj using 3 samples
2026-01-22 23:19:58.280 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.06s
2026-01-22 23:19:58.281 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.04
2026-01-22 23:19:58.281 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:58.281 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:19:58.281 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.12.self_attn.v_proj using 3 samples
2026-01-22 23:19:59.358 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:19:59.359 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:19:59.359 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:19:59.359 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:19:59.359 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.12.self_attn.o_proj using 3 samples
2026-01-22 23:20:00.422 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.06s
2026-01-22 23:20:00.423 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:20:00.424 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.67% | total memory: 25 GB
2026-01-22 23:20:00.424 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:20:00.424 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.12.mlp.gate_proj using 3 samples
2026-01-22 23:20:01.565 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.14s
2026-01-22 23:20:01.566 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.92
2026-01-22 23:20:01.566 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.68% | total memory: 25 GB
2026-01-22 23:20:01.566 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:20:01.567 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.12.mlp.up_proj using 3 samples
2026-01-22 23:20:02.766 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.20s
2026-01-22 23:20:02.767 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.26
2026-01-22 23:20:02.768 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:02.768 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:20:02.768 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.12.mlp.down_proj using 3 samples
2026-01-22 23:20:09.156 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.39s
2026-01-22 23:20:09.160 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:20:09.160 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.86% | total memory: 25 GB
2026-01-22 23:20:09.161 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:20:09.161 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:20:09.252 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:20:09.252 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.13.self_attn.q_proj using 3 samples
2026-01-22 23:20:10.324 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:20:10.325 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.14
2026-01-22 23:20:10.325 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:10.326 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:20:10.326 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.13.self_attn.k_proj using 3 samples
2026-01-22 23:20:11.362 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.04s
2026-01-22 23:20:11.363 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.04
2026-01-22 23:20:11.363 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:11.363 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:20:11.364 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.13.self_attn.v_proj using 3 samples
2026-01-22 23:20:12.423 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.06s
2026-01-22 23:20:12.424 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:20:12.424 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:12.425 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:20:12.425 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.13.self_attn.o_proj using 3 samples
2026-01-22 23:20:13.610 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.18s
2026-01-22 23:20:13.611 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:20:13.611 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.61% | total memory: 25 GB
2026-01-22 23:20:13.611 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:20:13.612 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.13.mlp.gate_proj using 3 samples
2026-01-22 23:20:14.744 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.13s
2026-01-22 23:20:14.745 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.73
2026-01-22 23:20:14.746 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.61% | total memory: 25 GB
2026-01-22 23:20:14.746 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:20:14.746 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.13.mlp.up_proj using 3 samples
2026-01-22 23:20:15.880 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.13s
2026-01-22 23:20:15.881 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.25
2026-01-22 23:20:15.881 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.61% | total memory: 25 GB
2026-01-22 23:20:15.881 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:20:15.881 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.13.mlp.down_proj using 3 samples
2026-01-22 23:20:22.302 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.42s
2026-01-22 23:20:22.305 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:20:22.306 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:20:22.306 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:20:22.306 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:20:22.410 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:20:22.410 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.14.self_attn.q_proj using 3 samples
2026-01-22 23:20:23.488 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:20:23.489 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.25
2026-01-22 23:20:23.489 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.59% | total memory: 25 GB
2026-01-22 23:20:23.489 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:20:23.490 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.14.self_attn.k_proj using 3 samples
2026-01-22 23:20:24.566 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:20:24.567 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.04
2026-01-22 23:20:24.567 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:24.568 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:20:24.568 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.14.self_attn.v_proj using 3 samples
2026-01-22 23:20:25.625 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.06s
2026-01-22 23:20:25.626 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.02
2026-01-22 23:20:25.626 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:25.627 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:20:25.627 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.14.self_attn.o_proj using 3 samples
2026-01-22 23:20:26.713 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:20:26.714 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.02
2026-01-22 23:20:26.714 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:26.714 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:20:26.714 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.14.mlp.gate_proj using 3 samples
2026-01-22 23:20:27.860 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.15s
2026-01-22 23:20:27.861 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.82
2026-01-22 23:20:27.861 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:27.861 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:20:27.861 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.14.mlp.up_proj using 3 samples
2026-01-22 23:20:28.984 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.12s
2026-01-22 23:20:28.985 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.25
2026-01-22 23:20:28.986 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:28.986 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:20:28.986 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.14.mlp.down_proj using 3 samples
2026-01-22 23:20:35.422 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.44s
2026-01-22 23:20:35.425 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.02
2026-01-22 23:20:35.425 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:20:35.426 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:20:35.426 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:20:35.514 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:20:35.514 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.15.self_attn.q_proj using 3 samples
2026-01-22 23:20:36.571 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.06s
2026-01-22 23:20:36.572 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.64
2026-01-22 23:20:36.573 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:36.573 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:20:36.573 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.15.self_attn.k_proj using 3 samples
2026-01-22 23:20:37.634 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.06s
2026-01-22 23:20:37.635 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.03
2026-01-22 23:20:37.635 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:37.635 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:20:37.636 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.15.self_attn.v_proj using 3 samples
2026-01-22 23:20:38.699 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.06s
2026-01-22 23:20:38.700 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:20:38.701 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:38.701 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:20:38.701 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.15.self_attn.o_proj using 3 samples
2026-01-22 23:20:39.766 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.06s
2026-01-22 23:20:39.767 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:20:39.768 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:39.768 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:20:39.768 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.15.mlp.gate_proj using 3 samples
2026-01-22 23:20:40.905 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.14s
2026-01-22 23:20:40.906 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.83
2026-01-22 23:20:40.906 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:40.907 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:20:40.907 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.15.mlp.up_proj using 3 samples
2026-01-22 23:20:42.111 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.20s
2026-01-22 23:20:42.111 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.27
2026-01-22 23:20:42.112 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.59% | total memory: 25 GB
2026-01-22 23:20:42.112 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:20:42.112 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.15.mlp.down_proj using 3 samples
2026-01-22 23:20:48.474 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.36s
2026-01-22 23:20:48.478 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:20:48.478 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:20:48.478 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:20:48.478 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:20:48.567 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:20:48.568 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.16.self_attn.q_proj using 3 samples
2026-01-22 23:20:49.639 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:20:49.640 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.13
2026-01-22 23:20:49.640 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:49.640 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:20:49.641 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.16.self_attn.k_proj using 3 samples
2026-01-22 23:20:50.684 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.04s
2026-01-22 23:20:50.685 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.05
2026-01-22 23:20:50.685 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:50.686 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:20:50.686 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.16.self_attn.v_proj using 3 samples
2026-01-22 23:20:51.765 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:20:51.766 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:20:51.766 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:51.767 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:20:51.767 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.16.self_attn.o_proj using 3 samples
2026-01-22 23:20:52.930 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.16s
2026-01-22 23:20:52.931 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.02
2026-01-22 23:20:52.931 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:52.931 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:20:52.931 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.16.mlp.gate_proj using 3 samples
2026-01-22 23:20:54.047 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.12s
2026-01-22 23:20:54.048 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.78
2026-01-22 23:20:54.048 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:54.049 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:20:54.049 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.16.mlp.up_proj using 3 samples
2026-01-22 23:20:55.164 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.11s
2026-01-22 23:20:55.165 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.32
2026-01-22 23:20:55.165 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:20:55.165 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:20:55.166 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.16.mlp.down_proj using 3 samples
2026-01-22 23:21:01.640 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.47s
2026-01-22 23:21:01.644 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.02
2026-01-22 23:21:01.644 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.84% | total memory: 25 GB
2026-01-22 23:21:01.644 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:21:01.644 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:21:01.734 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:21:01.734 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.17.self_attn.q_proj using 3 samples
2026-01-22 23:21:02.804 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:21:02.804 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.20
2026-01-22 23:21:02.805 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.59% | total memory: 25 GB
2026-01-22 23:21:02.805 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:21:02.805 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.17.self_attn.k_proj using 3 samples
2026-01-22 23:21:03.886 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:21:03.887 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.03
2026-01-22 23:21:03.887 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:03.887 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:21:03.887 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.17.self_attn.v_proj using 3 samples
2026-01-22 23:21:04.961 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:21:04.962 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:21:04.962 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:04.962 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:21:04.963 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.17.self_attn.o_proj using 3 samples
2026-01-22 23:21:06.036 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:21:06.037 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:21:06.038 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:06.038 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:21:06.038 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.17.mlp.gate_proj using 3 samples
2026-01-22 23:21:07.154 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.12s
2026-01-22 23:21:07.155 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.92
2026-01-22 23:21:07.155 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:07.156 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:21:07.156 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.17.mlp.up_proj using 3 samples
2026-01-22 23:21:08.248 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:21:08.249 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.39
2026-01-22 23:21:08.249 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:08.249 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:21:08.250 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.17.mlp.down_proj using 3 samples
2026-01-22 23:21:14.687 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.44s
2026-01-22 23:21:14.691 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.02
2026-01-22 23:21:14.691 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:21:14.691 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:21:14.692 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:21:14.782 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:21:14.782 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.18.self_attn.q_proj using 3 samples
2026-01-22 23:21:15.859 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:21:15.860 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.18
2026-01-22 23:21:15.861 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:15.861 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:21:15.861 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.18.self_attn.k_proj using 3 samples
2026-01-22 23:21:16.935 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:21:16.936 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.04
2026-01-22 23:21:16.936 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:16.937 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:21:16.937 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.18.self_attn.v_proj using 3 samples
2026-01-22 23:21:18.023 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:21:18.024 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.02
2026-01-22 23:21:18.024 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:18.024 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:21:18.025 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.18.self_attn.o_proj using 3 samples
2026-01-22 23:21:19.106 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:21:19.107 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:21:19.107 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:19.107 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:21:19.108 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.18.mlp.gate_proj using 3 samples
2026-01-22 23:21:20.215 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.11s
2026-01-22 23:21:20.216 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.89
2026-01-22 23:21:20.216 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:20.217 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:21:20.217 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.18.mlp.up_proj using 3 samples
2026-01-22 23:21:21.379 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.16s
2026-01-22 23:21:21.380 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.44
2026-01-22 23:21:21.380 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.59% | total memory: 25 GB
2026-01-22 23:21:21.381 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:21:21.381 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.18.mlp.down_proj using 3 samples
2026-01-22 23:21:27.770 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.39s
2026-01-22 23:21:27.774 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.03
2026-01-22 23:21:27.774 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:21:27.774 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:21:27.774 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:21:27.863 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:21:27.863 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.19.self_attn.q_proj using 3 samples
2026-01-22 23:21:28.912 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.05s
2026-01-22 23:21:28.913 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.24
2026-01-22 23:21:28.914 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:28.914 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:21:28.914 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.19.self_attn.k_proj using 3 samples
2026-01-22 23:21:29.943 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.03s
2026-01-22 23:21:29.944 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.05
2026-01-22 23:21:29.945 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:29.945 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:21:29.945 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.19.self_attn.v_proj using 3 samples
2026-01-22 23:21:31.024 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:21:31.025 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.03
2026-01-22 23:21:31.026 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:31.026 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:21:31.026 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.19.self_attn.o_proj using 3 samples
2026-01-22 23:21:32.196 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.17s
2026-01-22 23:21:32.197 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.02
2026-01-22 23:21:32.197 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:32.197 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:21:32.198 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.19.mlp.gate_proj using 3 samples
2026-01-22 23:21:33.322 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.12s
2026-01-22 23:21:33.323 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1.18
2026-01-22 23:21:33.324 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:33.324 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:21:33.324 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.19.mlp.up_proj using 3 samples
2026-01-22 23:21:34.444 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.12s
2026-01-22 23:21:34.445 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.59
2026-01-22 23:21:34.445 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:34.445 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:21:34.446 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.19.mlp.down_proj using 3 samples
2026-01-22 23:21:40.845 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.40s
2026-01-22 23:21:40.849 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.06
2026-01-22 23:21:40.850 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 22.06% | total memory: 25 GB
2026-01-22 23:21:40.850 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:21:40.850 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:21:40.951 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:21:40.951 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.20.self_attn.q_proj using 3 samples
2026-01-22 23:21:42.119 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.17s
2026-01-22 23:21:42.120 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.22
2026-01-22 23:21:42.120 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.59% | total memory: 25 GB
2026-01-22 23:21:42.120 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:21:42.120 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.20.self_attn.k_proj using 3 samples
2026-01-22 23:21:43.209 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:21:43.210 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.06
2026-01-22 23:21:43.211 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:43.211 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:21:43.211 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.20.self_attn.v_proj using 3 samples
2026-01-22 23:21:44.306 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:21:44.307 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.03
2026-01-22 23:21:44.307 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:44.308 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:21:44.308 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.20.self_attn.o_proj using 3 samples
2026-01-22 23:21:45.390 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:21:45.391 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.02
2026-01-22 23:21:45.391 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:45.391 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:21:45.391 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.20.mlp.gate_proj using 3 samples
2026-01-22 23:21:46.514 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.12s
2026-01-22 23:21:46.515 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1.41
2026-01-22 23:21:46.515 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:46.516 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:21:46.516 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.20.mlp.up_proj using 3 samples
2026-01-22 23:21:47.632 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.12s
2026-01-22 23:21:47.633 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.64
2026-01-22 23:21:47.633 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:47.633 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:21:47.634 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.20.mlp.down_proj using 3 samples
2026-01-22 23:21:54.093 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.46s
2026-01-22 23:21:54.096 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.05
2026-01-22 23:21:54.097 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:21:54.097 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:21:54.097 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:21:54.187 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:21:54.187 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.21.self_attn.q_proj using 3 samples
2026-01-22 23:21:55.261 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:21:55.262 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.23
2026-01-22 23:21:55.262 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:55.262 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:21:55.263 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.21.self_attn.k_proj using 3 samples
2026-01-22 23:21:56.337 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:21:56.339 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.05
2026-01-22 23:21:56.339 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:56.339 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:21:56.340 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.21.self_attn.v_proj using 3 samples
2026-01-22 23:21:57.414 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:21:57.415 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.03
2026-01-22 23:21:57.415 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:57.415 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:21:57.416 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.21.self_attn.o_proj using 3 samples
2026-01-22 23:21:58.490 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:21:58.491 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.02
2026-01-22 23:21:58.492 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:58.492 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:21:58.492 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.21.mlp.gate_proj using 3 samples
2026-01-22 23:21:59.601 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.11s
2026-01-22 23:21:59.602 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1.60
2026-01-22 23:21:59.602 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:21:59.603 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:21:59.603 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.21.mlp.up_proj using 3 samples
2026-01-22 23:22:00.768 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.16s
2026-01-22 23:22:00.769 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.74
2026-01-22 23:22:00.769 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.57% | total memory: 25 GB
2026-01-22 23:22:00.770 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:22:00.770 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.21.mlp.down_proj using 3 samples
2026-01-22 23:22:07.210 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.44s
2026-01-22 23:22:07.214 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.09
2026-01-22 23:22:07.214 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.85% | total memory: 25 GB
2026-01-22 23:22:07.215 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:22:07.215 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:22:07.304 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:22:07.304 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.22.self_attn.q_proj using 3 samples
2026-01-22 23:22:08.370 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:22:08.371 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.22
2026-01-22 23:22:08.371 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:22:08.372 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:22:08.372 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.22.self_attn.k_proj using 3 samples
2026-01-22 23:22:09.436 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.06s
2026-01-22 23:22:09.437 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.06
2026-01-22 23:22:09.437 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:22:09.437 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:22:09.438 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.22.self_attn.v_proj using 3 samples
2026-01-22 23:22:10.525 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:22:10.526 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.06
2026-01-22 23:22:10.526 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.60% | total memory: 25 GB
2026-01-22 23:22:10.526 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:22:10.527 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.22.self_attn.o_proj using 3 samples
2026-01-22 23:22:11.713 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.19s
2026-01-22 23:22:11.714 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.01
2026-01-22 23:22:11.714 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.61% | total memory: 25 GB
2026-01-22 23:22:11.714 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:22:11.714 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.22.mlp.gate_proj using 3 samples
2026-01-22 23:22:12.846 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.13s
2026-01-22 23:22:12.847 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2.08
2026-01-22 23:22:12.847 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.61% | total memory: 25 GB
2026-01-22 23:22:12.847 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:22:12.848 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.22.mlp.up_proj using 3 samples
2026-01-22 23:22:13.973 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.13s
2026-01-22 23:22:13.974 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.94
2026-01-22 23:22:13.974 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.61% | total memory: 25 GB
2026-01-22 23:22:13.975 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:22:13.975 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.22.mlp.down_proj using 3 samples
2026-01-22 23:22:20.386 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.41s
2026-01-22 23:22:20.392 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.10
2026-01-22 23:22:20.392 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 22.07% | total memory: 25 GB
2026-01-22 23:22:20.392 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:22:20.393 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:22:20.491 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:22:20.491 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.23.self_attn.q_proj using 3 samples
2026-01-22 23:22:21.668 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.18s
2026-01-22 23:22:21.669 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.30
2026-01-22 23:22:21.670 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.65% | total memory: 25 GB
2026-01-22 23:22:21.670 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:22:21.670 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.23.self_attn.k_proj using 3 samples
2026-01-22 23:22:22.748 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:22:22.749 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.08
2026-01-22 23:22:22.749 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:22.749 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:22:22.750 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.23.self_attn.v_proj using 3 samples
2026-01-22 23:22:23.844 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:22:23.846 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.09
2026-01-22 23:22:23.846 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:23.846 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:22:23.846 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.23.self_attn.o_proj using 3 samples
2026-01-22 23:22:24.929 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:22:24.930 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.03
2026-01-22 23:22:24.931 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.67% | total memory: 25 GB
2026-01-22 23:22:24.931 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:22:24.931 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.23.mlp.gate_proj using 3 samples
2026-01-22 23:22:26.053 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.12s
2026-01-22 23:22:26.054 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2.36
2026-01-22 23:22:26.055 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.67% | total memory: 25 GB
2026-01-22 23:22:26.055 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:22:26.055 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.23.mlp.up_proj using 3 samples
2026-01-22 23:22:27.178 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.12s
2026-01-22 23:22:27.179 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1.06
2026-01-22 23:22:27.179 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.67% | total memory: 25 GB
2026-01-22 23:22:27.179 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:22:27.180 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.23.mlp.down_proj using 3 samples
2026-01-22 23:22:33.706 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.53s
2026-01-22 23:22:33.709 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.12
2026-01-22 23:22:33.710 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:22:33.710 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:22:33.710 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:22:33.802 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:22:33.802 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.24.self_attn.q_proj using 3 samples
2026-01-22 23:22:34.897 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:22:34.898 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.30
2026-01-22 23:22:34.899 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:34.899 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:22:34.899 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.24.self_attn.k_proj using 3 samples
2026-01-22 23:22:35.991 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:22:35.992 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.07
2026-01-22 23:22:35.992 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:35.992 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:22:35.993 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.24.self_attn.v_proj using 3 samples
2026-01-22 23:22:37.080 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:22:37.081 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.18
2026-01-22 23:22:37.082 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:37.082 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:22:37.082 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.24.self_attn.o_proj using 3 samples
2026-01-22 23:22:38.170 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:22:38.171 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.02
2026-01-22 23:22:38.171 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:38.171 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:22:38.172 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.24.mlp.gate_proj using 3 samples
2026-01-22 23:22:39.298 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.13s
2026-01-22 23:22:39.298 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2.59
2026-01-22 23:22:39.299 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:39.299 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:22:39.299 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.24.mlp.up_proj using 3 samples
2026-01-22 23:22:40.490 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.19s
2026-01-22 23:22:40.491 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1.11
2026-01-22 23:22:40.491 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.65% | total memory: 25 GB
2026-01-22 23:22:40.491 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:22:40.492 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.24.mlp.down_proj using 3 samples
2026-01-22 23:22:46.913 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.42s
2026-01-22 23:22:46.916 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.14
2026-01-22 23:22:46.917 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:22:46.917 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:22:46.917 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:22:47.008 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:22:47.008 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.25.self_attn.q_proj using 3 samples
2026-01-22 23:22:48.063 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.05s
2026-01-22 23:22:48.064 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.24
2026-01-22 23:22:48.064 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:48.064 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:22:48.065 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.25.self_attn.k_proj using 3 samples
2026-01-22 23:22:49.130 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:22:49.131 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.06
2026-01-22 23:22:49.131 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:49.131 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:22:49.131 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.25.self_attn.v_proj using 3 samples
2026-01-22 23:22:50.208 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:22:50.209 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.29
2026-01-22 23:22:50.209 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:50.209 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:22:50.210 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.25.self_attn.o_proj using 3 samples
2026-01-22 23:22:51.387 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.18s
2026-01-22 23:22:51.388 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.09
2026-01-22 23:22:51.388 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:51.388 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:22:51.389 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.25.mlp.gate_proj using 3 samples
2026-01-22 23:22:52.516 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.13s
2026-01-22 23:22:52.517 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 3.00
2026-01-22 23:22:52.517 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:52.517 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:22:52.517 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.25.mlp.up_proj using 3 samples
2026-01-22 23:22:53.635 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.12s
2026-01-22 23:22:53.636 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1.44
2026-01-22 23:22:53.636 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:22:53.637 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:22:53.637 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.25.mlp.down_proj using 3 samples
2026-01-22 23:23:00.045 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.41s
2026-01-22 23:23:00.049 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.17
2026-01-22 23:23:00.080 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 20.24% | total memory: 25 GB
2026-01-22 23:23:00.080 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:23:00.080 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:23:00.231 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:23:00.232 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.26.self_attn.q_proj using 3 samples
2026-01-22 23:23:01.342 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.11s
2026-01-22 23:23:01.343 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.37
2026-01-22 23:23:01.344 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.65% | total memory: 25 GB
2026-01-22 23:23:01.344 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:23:01.344 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.26.self_attn.k_proj using 3 samples
2026-01-22 23:23:02.426 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:23:02.427 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.07
2026-01-22 23:23:02.427 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:23:02.427 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:23:02.428 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.26.self_attn.v_proj using 3 samples
2026-01-22 23:23:03.505 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:23:03.506 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.35
2026-01-22 23:23:03.506 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:23:03.506 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:23:03.506 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.26.self_attn.o_proj using 3 samples
2026-01-22 23:23:04.602 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:23:04.603 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.04
2026-01-22 23:23:04.603 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:23:04.603 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:23:04.603 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.26.mlp.gate_proj using 3 samples
2026-01-22 23:23:05.717 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.11s
2026-01-22 23:23:05.718 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 3.77
2026-01-22 23:23:05.718 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:23:05.718 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:23:05.719 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.26.mlp.up_proj using 3 samples
2026-01-22 23:23:06.850 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.13s
2026-01-22 23:23:06.851 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2.11
2026-01-22 23:23:06.851 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:23:06.851 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:23:06.852 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.26.mlp.down_proj using 3 samples
2026-01-22 23:23:06.978 | WARNING  | llmcompressor.modifiers.quantization.gptq.gptq_quantize:quantize_weight:165 - Failed to invert hessian due to numerical instability. Consider increasing GPTQModifier.dampening_frac, increasing the number of calibration samples, or shuffling the calibration dataset. Falling back to round-to-nearest for this module.
2026-01-22 23:23:13.207 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.36s
2026-01-22 23:23:13.211 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.73
2026-01-22 23:23:13.211 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.92% | total memory: 25 GB
2026-01-22 23:23:13.211 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:23:13.212 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:23:13.299 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:23:13.299 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.27.self_attn.q_proj using 3 samples
2026-01-22 23:23:14.391 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:23:14.392 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.41
2026-01-22 23:23:14.392 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:23:14.393 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.726272 MB
2026-01-22 23:23:14.393 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.27.self_attn.k_proj using 3 samples
2026-01-22 23:23:15.482 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.09s
2026-01-22 23:23:15.482 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.06
2026-01-22 23:23:15.483 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:23:15.483 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:23:15.483 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.27.self_attn.v_proj using 3 samples
2026-01-22 23:23:16.549 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.07s
2026-01-22 23:23:16.550 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.32
2026-01-22 23:23:16.550 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.66% | total memory: 25 GB
2026-01-22 23:23:16.551 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 0.787712 MB
2026-01-22 23:23:16.551 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.27.self_attn.o_proj using 3 samples
2026-01-22 23:23:17.629 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.08s
2026-01-22 23:23:17.630 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 0.24
2026-01-22 23:23:17.630 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.67% | total memory: 25 GB
2026-01-22 23:23:17.630 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 4.7232 MB
2026-01-22 23:23:17.630 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.27.mlp.gate_proj using 3 samples
2026-01-22 23:23:18.761 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.13s
2026-01-22 23:23:18.762 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2.37
2026-01-22 23:23:18.762 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 30.67% | total memory: 25 GB
2026-01-22 23:23:18.762 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:23:18.762 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.27.mlp.up_proj using 3 samples
2026-01-22 23:23:19.871 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 1.11s
2026-01-22 23:23:19.872 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 2.32
2026-01-22 23:23:19.872 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 20.82% | total memory: 25 GB
2026-01-22 23:23:19.872 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.552 MB
2026-01-22 23:23:19.872 | INFO     | llmcompressor.modifiers.quantization.gptq.base:compress_modules:253 - Quantizing model.layers.27.mlp.down_proj using 3 samples
2026-01-22 23:23:26.407 | METRIC   | llmcompressor.utils.metric_logging:compress:127 - time 6.53s
2026-01-22 23:23:26.410 | METRIC   | llmcompressor.utils.metric_logging:compress:129 - error 1.78
2026-01-22 23:23:26.411 | METRIC   | llmcompressor.utils.metric_logging:compress:136 - GPU 0 | usage: 31.86% | total memory: 25 GB
2026-01-22 23:23:26.411 | METRIC   | llmcompressor.utils.metric_logging:compress:145 - Compressed module size: 27.529728 MB
2026-01-22 23:23:26.411 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:23:26.572 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.SEQUENTIAL_EPOCH_END
2026-01-22 23:23:26.573 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=False sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:23:26.945 | DEBUG    | llmcompressor.core.lifecycle:event:195 - Handling event: EventType.CALIBRATION_EPOCH_END
2026-01-22 23:23:26.953 | DEBUG    | llmcompressor.core.lifecycle:event:205 - Updated event with modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=False started_=True ended_=True sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:23:26.958 | DEBUG    | llmcompressor.core.lifecycle:finalize:134 - Finalizing compression lifecycle
2026-01-22 23:23:26.958 | DEBUG    | llmcompressor.core.lifecycle:finalize:138 - Finalized modifier: index=None group=None start=None end=None update=None initialized_=True finalized_=True started_=True ended_=True smoothing_strength=0.8 mappings=[LayerMap(balance_layers=['re:.*q_proj', 're:.*k_proj', 're:.*v_proj'], smooth_layers='re:.*input_layernorm'), LayerMap(balance_layers=['re:.*gate_proj', 're:.*up_proj'], smooth_layers='re:.*post_attention_layernorm')] ignore=[] num_calibration_steps=None calibration_function=None
2026-01-22 23:23:26.958 | DEBUG    | llmcompressor.core.lifecycle:finalize:138 - Finalized modifier: config_groups=None targets=['Linear'] ignore=['lm_head'] scheme='W8A8' kv_cache_scheme=None index=None group=None start=None end=None update=None initialized_=True finalized_=True started_=True ended_=True sequential_update=True sequential_targets=None block_size=128 dampening_frac=0.01 actorder=None offload_hessians=False
2026-01-22 23:23:26.959 | INFO     | llmcompressor.core.lifecycle:finalize:144 - Compression lifecycle finalized for 2 modifiers
2026-01-22 23:23:27.030 | INFO     | llmcompressor.transformers.sparsification.compressed_tensors_utils:get_model_compressor:195 - skip_sparsity_compression_stats set to True. Skipping sparsity compression statistic calculations. No sparsity compressor will be applied.
2026-01-22 23:23:35.334 | DEBUG    | llmcompressor.transformers.utils.helpers:recipe_from_huggingface_model_id:146 - Unable to find recipe recipe.yaml for model ID: Qwen/Qwen2.5-1.5B-Instruct: 404 Client Error. (Request ID: Root=1-697240f6-2d5fff2e6c20552e70f5b197;a581c68f-53cb-4f68-b61a-cf750acff1f3)

Entry Not Found for url: https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct/resolve/main/recipe.yaml..Skipping recipe resolution.
2026-01-22 23:23:35.336 | DEBUG    | llmcompressor.transformers.utils.helpers:infer_recipe_from_model_path:112 - Failed to infer the recipe from the model_path
