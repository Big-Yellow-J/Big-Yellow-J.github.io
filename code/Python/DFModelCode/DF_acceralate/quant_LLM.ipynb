{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca4d5c76",
   "metadata": {},
   "source": [
    "# 模型量化处理\n",
    "* 数据选择\n",
    "\n",
    "![](https://s2.loli.net/2026/01/21/aXEqLHontgUS1Pd.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa24808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea3f3b",
   "metadata": {},
   "source": [
    "## 使用llama.cpp量化\n",
    "### 基本原理解释\n",
    "#### 1、量化精度\n",
    "\n",
    "#### 2、量化范式\n",
    "### 1、量化大语言模型\n",
    "> https://qwen.readthedocs.io/zh-cn/latest/quantization/llama.cpp.html\n",
    "\n",
    "为了实现GGUF格式转换，按照下面步骤进行\n",
    "#### 1、无校准量化GGUF\n",
    "```python\n",
    "# 首先创建GGUF文件  转换文件\n",
    "python convert-hf-to-gguf.py 模型名称/地址 --outtype bf16 --outfile Qwen3-1.5B-BF16.gguf \n",
    "# 其中的 outtype后面的数据类型可以是：bf16 f32等\n",
    "```\n",
    "在执行上面代码之后直接将模型量化到`8 bit`：\n",
    "```python\n",
    "./build/bin/llama-quantize --quantize --model Qwen3-1.5B-F16.gguf --output Qwen3-1.5B-Q8_0.gguf --type Q8_0\n",
    "```\n",
    "`Q8_0`是一个量化预设的代号\n",
    "#### 2、使用重要性矩阵量化GGUF\n",
    "```python\n",
    "# 首先\n",
    "./llama-imatrix -m Qwen3-1.5B-F16.gguf -f calibration-text.txt --chunk 512 -o Qwen3-1.5B-imatrix.dat -ngl 80\n",
    "# \n",
    "./llama-quantize --imatrix Qwen3-1.5B-imatrix.dat Qwen3-1.5B-F16.gguf Qwen3-1.5B-Q4_K_M.gguf Q4_K_M\n",
    "```\n",
    "#### 两种量化方式代码解释\n",
    "在此之前可以使用`python convert_hf_to_gguf.py --help`了解一些这个脚本都有哪些参数，`python convert_hf_to_gguf.py --print-supported-models`直接看一下支持哪些模型进行量化，一般而言模型名称就和`config.json`中是一致的比如`Qwen2ForCausalLM`。量化开始：**首先**，在下载得到模型权重之后，在`cache_dir`里面就会有模型权重，比如说上面过程中模型权重就会保存为：`/Model/models--Qwen--Qwen2.5-1.5B-Instruct`，那么在创建以及转换文件过程中就需要将其中的 **模型名称/地址**进行替换，那么**完整带脚本**就是：`python convert_hf_to_gguf.py /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/Model/models--Qwen--Qwen2.5-1.5B-Instruct --outfile /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-BF16.gguf --outtype bf16 --verbose`\n",
    "**不过**，上面代码还是会出错，因为实际模型文件在`code/Python/DFModelCode/DF_acceralate/Model/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306`因此需要将上面的文件路径改为：`/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/Model/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306`最后输出：\n",
    "```bash\n",
    "...\n",
    "INFO:hf-to-gguf:Model successfully exported to /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-BF16.gguf\n",
    "```\n",
    "就表示成功了，而后可以通过`./build/bin/llama-quantize --help`去检查支持哪些参数，**并且进行量化**：`./build/bin/llama-quantize /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-BF16.gguf /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-Q8_0.gguf q8_0`。不过还有一点有意思的，比如说我的模型一般都会用lora进行微调，我需要对lora微调模型进行处理，类似的：\n",
    "```bash\n",
    "python convert_lora_to_gguf.py /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/test-lora/ --outfile /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Lora-Qwen3-1.5B-BF16.gguf --outtype bf16 --verbose\n",
    "\n",
    "./build/bin/llama-quantize /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Lora-Qwen3-1.5B-BF16.gguf /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Lora-Qwen3-1.5B-Q8_0.gguf q8_0\n",
    "```\n",
    "> **总结上面过程代码**：\n",
    "> `python convert_hf_to_gguf.py /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/Model/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306 --outfile /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-BF16.gguf --outtype bf16 --verbose`\n",
    "> `./build/bin/llama-quantize /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-BF16.gguf /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-Q8_0.gguf q8_0`\n",
    "\n",
    "### 使用GGUF量化模型\n",
    "```bash\n",
    "pip install llama-cpp-python\n",
    "# 直接去 https://github.com/abetlen/llama-cpp-python/releases 下载对应的版本 和flash-attn一样\n",
    "pip install llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl\n",
    "```\n",
    "这样一来就安装好了就可以直接加载GGUF模型权重了（和非量化模型推理10.8s，**量化后推理：3.5s**）：\n",
    "```python\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-Q8_0.gguf\",\n",
    "    # lora_path= \n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=-1,\n",
    "    chat_format=\"chatml\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个有帮助的助手，用简洁的中文回答。\"},\n",
    "    {\"role\": \"user\",   \"content\": \"晚上睡不着怎么办\"}\n",
    "]\n",
    "\n",
    "# 生成\n",
    "response = llm.create_chat_completion(\n",
    "    messages,\n",
    "    max_tokens=300,\n",
    "    temperature=0.75,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.1,\n",
    "    stream=False  # 改成 True 即可流式输出\n",
    ")\n",
    "\n",
    "print(\"AI 回答：\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "```\n",
    "\n",
    "#### 2、量化范式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b5a8878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "\n",
    "def load_model():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "                                                cache_dir= '/root/autodl-tmp/Model/', \n",
    "                                                mirror='https://hf-mirror.com')\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "                                                cache_dir= '/root/autodl-tmp/Model/', \n",
    "                                                mirror='https://hf-mirror.com')\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_GGUF(lora_path=None):\n",
    "    from llama_cpp import Llama\n",
    "\n",
    "    llm = Llama(\n",
    "        model_path=\"/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-Q8_0.gguf\",\n",
    "        lora_path= lora_path,\n",
    "        n_ctx=8192,\n",
    "        n_gpu_layers=-1,\n",
    "        chat_format=\"chatml\",\n",
    "        verbose=False\n",
    "    )\n",
    "    return llm\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个有帮助的助手，用简洁的中文回答。\"},\n",
    "    {\"role\": \"user\",   \"content\": \"晚上睡不着怎么办?\"}\n",
    "]\n",
    "model, tokenizer = load_model()\n",
    "model_gguf = load_GGUF()\n",
    "model_gguf_lora = load_GGUF(lora_path= '/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Lora-Qwen3-1.5B-BF16.gguf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82746fc",
   "metadata": {},
   "source": [
    "## 使用llmcompressor进行模型量化\n",
    "> `llmcompressor==0.7.1、transformers==4.55.2、torch==2.8.0+cu126、trl==0.22.2、vllm==0.11.0` \n",
    "### 原理及官方文档总结\n",
    "\n",
    "| 方法 | 描述 | 准确性恢复 vs. 时间 |\n",
    "|---|---|---|\n",
    "| GPTQ | 利用二阶逐层优化来优先处理重要的权重/激活，并允许更新剩余权重 | 准确性恢复高，但运行成本更高/更慢 |\n",
    "| AWQ | 使用通道级缩放更好地保留权重和激活中的重要异常值 | 比GPTQ具有更好的准确性恢复和更快的运行时间 |\n",
    "| SmoothQuant | 通过将异常值折叠到权重中来平滑激活中的异常值，确保权重和激活量化模型具有更好的准确性 | 准确性恢复良好，校准时间最短；可与其他方法组合使用 |\n",
    "| 就近取整(RTN) | 简单的量化技术，将每个值四舍五入到目标精度中最接近的可表示级别 | 在大多数情况下提供中等准确性恢复。计算成本低且实现速度快，适用于实时或资源受限的环境 |\n",
    "| AutoRound | 通过符号梯度下降优化舍入和裁剪范围 | 与GPTQ/AWQ相比，提供领先的4位和优异的低于4位精度，运行时间比GPTQ快，与AWQ相当 |\n",
    "\n",
    "直接对[官方](https://docs.vllm.ai/projects/llm-compressor/en/0.7.1/getting-started/compress/#apply-the-recipe)中描述进行总结，收其提供的压缩方式进行操作，直接使用llmcompressor中的压缩方式使用起来很简单，可以直接使用`oneshot`进行量化，比如说：\n",
    "```python\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor import oneshot\n",
    "\n",
    "recipe = [\n",
    "    SmoothQuantModifier(smoothing_strength=0.8),\n",
    "    GPTQModifier(scheme=\"W8A8\", targets=\"Linear\", ignore=[\"lm_head\"]),\n",
    "]\n",
    "oneshot(\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    dataset=\"open_platypus\",\n",
    "    recipe=recipe,\n",
    "    output_dir=\"TinyLlama-1.1B-Chat-v1.0-INT8\",\n",
    "    max_seq_length=2048,\n",
    "    num_calibration_samples=512,\n",
    ")\n",
    "```\n",
    "**值得注意的是**，在使用其进行模型压缩过程中对于显存的估计大概是：`mem(1B parameters) ~= (1B parameters) * (2 bytes / parameter) = 2B bytes ~= 2Gb`，除此之外压缩模型过程中对于文本解码层（LLM部分）以及视觉编码层（Vision Layer部分）两部分压缩过程中处理存在差异，**前者**会动态地将模型逐层加载到GPU进行计算，其余部分仍保留在CPU内存中。**后者**会一次性全部加载到GPU。与文本模型不同，视觉编码层在加载到GPU前不会被拆分为独立层。对于视觉塔层规模大于文本层的模型，这种机制可能导致GPU内存瓶颈。目前（**测试的是0.7.1版本实际对于最新的也是如此**） LLM Compressor 暂不对视觉编码器进行量化处理，因量化技术在延迟/吞吐量与精度损失间的权衡通常得不偿失。比如说：\n",
    "**1、四舍五入量化（Round To Nearest RTN）**  \n",
    "![](https://s2.loli.net/2026/01/20/JiXhKPoFfpNdv7e.png)  \n",
    "**2、GPTQ**  \n",
    "![](https://s2.loli.net/2026/01/20/m2IqEYWsFKUPbkT.png)  \n",
    "在llm compressor中支持量化操作  \n",
    "![](https://s2.loli.net/2026/01/20/br2FjTWAOXp3nU5.png)    \n",
    "对于[上述内容解释](https://github.com/vllm-project/llm-compressor/blob/main/docs/guides/compression_schemes.md?plain=1)：  \n",
    "\n",
    "| Scheme | Description | Data required? | vLLM Hardware Compatibility |\n",
    "|--------|-------------|----------------|-----------------------------|\n",
    "| **[W8A8-FP8](../../examples/quantization_w8a8_fp8/README.md)** | 8位浮点（FP8）量化用于权重和激活值，通过8位算术运算实现约2倍的权重压缩。采用逐通道（channel-wise）量化将权重压缩至8位，并采用动态逐token或静态逐张量（per-tensor）量化将激活值压缩至8位。权重的缩放因子既可以按通道生成，也可以按张量生成。其中，逐通道权重量化配合动态逐token激活量化是性能最优的选项。W8A8-FP8无需校准数据集，激活值的量化在vLLM推理过程中实时进行。该方案在通用性能和压缩率方面表现良好，尤其适用于服务器端和批处理推理场景。 | 无需校准数据集，除非你采用静态逐张量激活量化。 | 支持最新的 NVIDIA GPU（Hopper 架构及之后）和最新的 AMD GPU。推荐用于计算能力 >=8.9 的 NVIDIA GPU（包括 Hopper、Ada Lovelace 和 Blackwell 架构）。 |\n",
    "| **[W8A8-INT8](../../examples/quantization_w8a8_int8/README.md)** | 8位整数（INT8）量化用于权重和激活值，通过8位算术运算实现约2倍的权重压缩。使用GPTQ进行逐通道（channel-wise）量化将权重压缩至8位，并采用动态逐token量化将激活值压缩至8位。INT8权重量化既支持逐张量（per-tensor）也支持逐通道（per-channel）方式。W8A8-INT8在通用性能和压缩率方面表现良好，尤其适用于服务器端和批处理推理。激活值的量化在vLLM推理过程中执行，可选择静态或动态方式；此外，INT8激活值还支持非对称量化。W8A8-INT8有助于在高QPS场景或使用vLLM进行离线服务时提升推理速度。 | 权重量化以及静态逐张量激活量化需要校准数据集 | 支持所有NVIDIA GPU、AMD GPU、TPU、CPU及其他加速器。推荐用于计算能力 <8.9 的NVIDIA GPU（如Ampere、Turing、Volta、Pascal或更早架构）。 |\n",
    "| **[W4A16](../../examples/quantization_w4a16/README.md)** | 仅将权重量化为4位整数（INT4）精度，激活值保持16位浮点（FP16）精度。W4A16可实现约3.7倍的权重压缩，但仍需使用16位算术运算。W4A16还支持非对称权重量化。该方案在内存受限、对延迟敏感的应用中提供最大压缩率，并在低QPS场景下通过更高的权重压缩带来显著加速。所附示例采用GPTQ算法以降低量化损失，但也可使用其他算法，如[AWQ](../../examples/awq/llama_example.py) 进行| 需要数据集 | 所有设备 |\n",
    "| **W8A16** | 将模型权重编码为8位整数，激活值编码为16位整数。W8A16压缩相比FP32可生成更小的模型输出体积，并在具备原生8位整数计算单元的硬件上实现更快的推理速度。相比浮点运算，其功耗和内存带宽需求更低| 需要数据集 | 所有设备 |\n",
    "| **NVFP4** | NVFP4 是随 NVIDIA Blackwell GPU 架构推出的4位浮点编码格式。它通过高精度缩放编码和两级微块（micro-block）缩放策略，在张量值的宽动态范围内保持数值精度。NVFP4 压缩为每个张量生成一个全局缩放因子，并为每16个元素组成的组生成局部量化缩放因子。权重和激活值均使用全局缩放因子和局部量化缩放因子进行量化。组大小固定，不可更改。 | 需要数据集 | 仅支持 NVIDIA Blackwell 架构及后续 GPU |\n",
    "| [**W8A8-FP8_BLOCK**](../../examples/quantization_w8a8_fp8/fp8_block_example.py)| 采用分块量化（通常为 128×128 的块）将权重压缩至 FP8，并对激活值使用动态逐 token 组（每组 128 个 token）量化。无需校准数据集，激活量化在 vLLM 推理过程中实时执行 | 不需要数据集 | 支持最新的 NVIDIA GPU（Hopper 及之后）和最新的 AMD GPU。推荐用于计算能力 ≥8.9 的 NVIDIA GPU（包括 Hopper、Ada Lovelace 和 Blackwell 架构） |\n",
    "| **[2:4 Semi-structured Sparsity](../../examples/sparse_2of4_quantization_fp8/README.md)** | U采用半结构化稀疏（如 SparseGPT）：在张量中每连续 4 个权重中，将其中 2 个置零。使用权重逐通道量化压缩至 8 位，并对激活值采用动态逐 token 量化压缩至 8 位。相比标准 W8A8-FP8，可实现更优推理性能，且评估分数几乎无下降，参考[evaluation score](https://neuralmagic.com/blog/24-sparse-llama-fp8-sota-performance-for-nvidia-hopper-gpus/). 但小型模型可能因剩余非零权重不足以还原原始分布而出现精度下降。| 需要数据集 |  支持所有 NVIDIA GPU、AMD GPU、TPU、CPU 及其他加速器。推荐用于计算能力 ≥9.0 的 GPU（Hopper 和 Blackwell 架构） |\n",
    "| **Unstructured Sparsity** | 非结构化稀疏量化以无固定模式的方式将模型中的个别权重置零。与分块或通道剪枝不同，它在模型中任意位置移除贡献最小的权重，从而生成细粒度的稀疏矩阵。 |不需要数据集 | 支持所有 NVIDIA GPU、AMD GPU、TPU、CPU 及其他加速器 |\n",
    "\n",
    "### 关键参数解释\n",
    "介绍其支持量化范式之后直接看代码操作\n",
    "```python\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor import oneshot\n",
    "\n",
    "recipe = [\n",
    "    SmoothQuantModifier(smoothing_strength=0.8),\n",
    "    GPTQModifier(scheme=\"W8A8\", targets=\"Linear\", ignore=[\"lm_head\"]),\n",
    "]\n",
    "oneshot(\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    dataset=\"open_platypus\",\n",
    "    recipe=recipe,\n",
    "    output_dir=\"TinyLlama-1.1B-Chat-v1.0-INT8\",\n",
    "    max_seq_length=2048,\n",
    "    num_calibration_samples=512,\n",
    ")\n",
    "```\n",
    "#### 1、oneshot参数解释\n",
    "**首先**看`oneshot`中参数操作，在代码`llmcompressor/entrypoints/oneshot.py`以及[文档](https://docs.vllm.ai/projects/llm-compressor/en/0.7.1/reference/llmcompressor/entrypoints/oneshot/?h=oneshot)中参数，着重了解：  \n",
    "* `model`：直接使用hf上模型名称/本地名称，对于本地需要具体到config.json位置比如说：`models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa30/`  \n",
    "* `cache_dir`：缓存路径：str\n",
    "* `output_dir`：保存路径：str\n",
    "* `log_dir`: 日志路径：str\n",
    "* `text_column`：指定数据的columns（也就是用数据集中那一个标签的数据量化处理）\n",
    "* `dataset`：使用的数据集，str, Dataset, DatasetDict，如果在直接使用hf上数据集那么直接用名称就行，如open_platypus。（具体代码见下）对于后面两项可以直接用 `datasets`中load方式加载数据即可\n",
    "\n",
    "此处简单介绍其中**数据处理逻辑**，在`llmcompressor`中数据的处理逻辑和`datasets`库（该库一般就是两个核心操作load、process，前者加载数据后者直接编码数据）处理逻辑一致，datasets中**加载本地数据**[具体逻辑](https://huggingface.co/docs/datasets/loading)如下：**1、如果就只有一个文件**（比如说我的所有数据都存储在一个jsonl中），那么可以直接使用`load_dataset('json', data_files= './test_datasets.jsonl',streaming=True)`;**2、如果有多个文件**，比较简单方式直接将`data_files`替换为`tmp/TestDatasets-Json/*.jsonl`其中用 * 通配符去匹配所有的jsonl文件，除此之外还可以**用一个字典去表示**，比如说：`{'train': [xxx, ..., xxx]}`\n",
    "> 上面代码中用 steaming=True 流式处理去处理数据较大情况，访问数据可以直接`next(iter(json_one_dataset[\"train\"]))`\n",
    "\n",
    "> **处理数据**，将数据丢给llm/vllm进行处理之前，需要将数据进行变换，比如说转化为对应的数据格式/对数据进行tokenizer进行处理，这里处理方式在`datasets`中可以直接使用map处理即可（具体处理看下面例子）。回到`oneshot`中的数据处理过程中，其内部逻辑和`datasets`中处理逻辑一致，处理本地文件逻辑如下：`get_processed_dataset`(llmcompressor/datasets/utils.py)判断数据是不是提前被tokenized处理，**如果是**那么直接返回即可，**如果不是**直接使用`TextGenerationDataset.load_from_registry`(llmcompressor/transformers/finetune/data/base.py)处理数据  \n",
    "==>`TextGenerationDataset`处理数据逻辑，加载数据-->process数据（两部分数据和hf库datasets的处理逻辑是一致的），对于第一部分**加载数据处理如下**：\n",
    "1、对于hf上数据直接加载==>返回  \n",
    "2、对于自定义的数据（也支持使用下载好的hf本地数据），使用`get_custom_datasets_from_path`(llmcompressor/transformers/finetune/data/data_helpers.py)，在这个函数中默认两个参数：path、ext（文件类型）。对于path格式要求  \n",
    "![](https://s2.loli.net/2026/01/20/my5v1aBonNuh9IZ.png)  \n",
    "最后都统一返回。**process数据处理过程**和`datasets`中处理是一致的，再代码中`TextGenerationDataset`(llmcompressor/transformers/finetune/data/base.py)中直接使用`self.map()`处理数据而后再去通过`format_calibration_data`（llmcompressor/datasets/utils.py）使用DataLoader进行处理\n",
    "#### 2、量化方式参数解释\n",
    "\n",
    "### AWQ量化\n",
    "[https://www.big-yellow-j.top/posts/2025/10/11/Quantized.html](https://www.big-yellow-j.top/posts/2025/10/11/Quantized.html)\n",
    "### GPTQ量化\n",
    "[https://www.big-yellow-j.top/posts/2025/10/11/Quantized.html](https://www.big-yellow-j.top/posts/2025/10/11/Quantized.html)\n",
    "### 1、量化大语言模型\n",
    "\n",
    "### 2、量化多模态模型\n",
    "### 量化模型使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff86ccc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': '../data/OmniDoc/images/eastmoney_1885ca41425d245551f3482457304f78b48186bff625fa91e675eaf6bba5229f.pdf_0.jpg', 'height': 2338, 'width': 1654, 'prefix': 'QwenVL HTML', 'suffix': '<body><h2 data-bbox=\"67 227 1138 289\">索通发展 Sunstone Development（603612 CH）</h2><h2 data-bbox=\"65 297 1566 355\">首次覆盖：扩张阳极产能稳固龙头地位，布局负极培育第二成长曲线</h2><h2 data-bbox=\"64 364 1597 449\">Expand Anode Production Capacity to Stabilize the Leading Position, and Lay out the Second Growth Curve of Anode Cultivation: Initiation</h2><p data-bbox=\"197 484 517 516\">观点聚焦 Investment Focus</p><h2 data-bbox=\"90 536 691 570\">首次覆盖优于大市 Initiate with OUTPERFORM</h2><div class=\"table\" data-bbox=\"69 571 703 885\"><table><tr><td>评级</td><td>优于大市OUTPERFORM</td></tr><tr><td>现价</td><td>Rmb18.38</td></tr><tr><td>目标价</td><td>Rmb24.50</td></tr><tr><td>市值</td><td>Rmb9.94bn/US$1.36bn</td></tr><tr><td>日交易额（3个月均值）</td><td>US$15.46mn</td></tr><tr><td>发行股票数目</td><td>540.85mn</td></tr><tr><td>自由流通股（%）</td><td>50%</td></tr><tr><td>1年股价最高最低值</td><td>Rmb33.62-Rmb15.99</td></tr></table></div><p data-bbox=\"75 895 484 918\">注：现价Rmb18.38为2023年9月21日收盘价</p><div class=\"image\" data-bbox=\"108 950 676 1352\"></div><p data-bbox=\"78 1356 216 1376\">资料来源：Factset</p><div class=\"table\" data-bbox=\"74 1414 701 1532\"><table><tr><td></td><td>1mth</td><td>3mth</td><td>12mth</td></tr><tr><td>绝对值</td><td>5.8%</td><td>9.7%</td><td>-40.8%</td></tr><tr><td>绝对值(美元)</td><td>5.7%</td><td>7.9%</td><td>-43.1%</td></tr><tr><td>相对 MSCI China</td><td>5.8%</td><td>9.7%</td><td>-40.8%</td></tr></table></div><div class=\"table\" data-bbox=\"75 1581 701 1878\"><table><tr><td>(Rmb mn)</td><td>Dec-22A</td><td>Dec-23E</td><td>Dec-24E</td><td>Dec-25E</td></tr><tr><td>营业收入</td><td>19,401</td><td>17,097</td><td>22,311</td><td>28,205</td></tr><tr><td>(+/-)</td><td>105%</td><td>-12%</td><td>31%</td><td>26%</td></tr><tr><td>净利润</td><td>905</td><td>189</td><td>948</td><td>1,391</td></tr><tr><td>(+/-)</td><td>46%</td><td>-79%</td><td>401%</td><td>47%</td></tr><tr><td>全面摊薄EPS (Rmb)</td><td>1.67</td><td>0.35</td><td>1.75</td><td>2.57</td></tr><tr><td>毛利率</td><td>13.2%</td><td>10.7%</td><td>14.4%</td><td>14.9%</td></tr><tr><td>净资产收益率</td><td>16.6%</td><td>2.7%</td><td>11.8%</td><td>14.7%</td></tr><tr><td>市盈率</td><td>10.75</td><td>51.38</td><td>10.27</td><td>7.00</td></tr></table></div><p data-bbox=\"76 1880 263 1907\">资料来源：公司信息，HTI</p><p data-bbox=\"737 531 1252 562\">(Please see APPENDIX 1 for English summary)</p><p data-bbox=\"734 597 1599 919\">- 预焙阳极行业龙头，盈利能力逐年提升。索通发展拥有国内最大的预焙阳极产能，占据着预焙阳极行业龙头地位，目前在产产能282万吨。公司的主营业务为预焙阳极的研发、生产及销售业务。公司营业收入增长迅猛，2022年实现营业收入 194.01 亿元，同比增长 105.12% ，三年 CAGR 达 64% ；归母净利持续提升，2022年实现归母净利9.05亿元，同比增长 45.99% ，三年 CAGR 达 119% 。2023年上半年实现营业收入81.30亿元，同比增长 2.09% ；实现归母净利 - 4.09亿元，同比减少 171.97% ；实现扣非归母净利 - 4.12亿元，同比减少 172.43%。</p><p data-bbox=\"732 953 1599 1205\">- 预焙阳极：下游需求端稳步回升，公司市占率不断提高。电解铝增产带动预焙阳极需求快速提升，而区域间供需错配仍然严重。在这种情况下，公司积极抓住北铝南移机遇，持续扩张西南地区预焙阳极产能，公司预计2025年阳极总产能将达到约500万吨，公司市占率有望快速提升。通过优化上游原材料布局，公司综合成本优势日益显著，预焙阳极单吨毛利从2019年的 407.6元／吨，提高至2022年的866.6元/吨。 866.6元／吨。</p><p data-bbox=\"732 1239 1599 1562\">- 负极：进军锂电负极行业，加速布局第二成长曲线。锂电池行业发展空间广阔，对负极材料需求较大。公司通过收购欣源股份快速切入锂电负极行业，进而构建以风光储氢一体化的绿色能源供应为基础的“预焙阳极+锂电负极”碳材料产业，打造公司的第二增长曲线。负极产能正处于加速扩建阶段。内蒙欣源新建4万吨石墨化项目与盛源负极项目首期一步2.5万吨石墨化产能投产， 770MW光伏发电项目已主体完工，公司预计2023年下半年通电。可以预见，未来公司负极材料业务的活力将得到大幅度增强，第二增长曲线将为公司的成长和发展提供源源不竭的能量。</p><p data-bbox=\"736 1598 1598 1811\">- 盈利预测与估值：我们预计2023-2025年公司收入分别为170.97、 223.11、282.05亿元，同比增长- 11.9%、30.5%、26.42%我们预计公司2023-2025年EPS分别为0.35、1.75、2.57元，根据可比公司估值，考虑到公司预焙阳极产能密集投产，切入锂电负极带来协同效应，我们给予公司2024年14倍PE，对应合理价值24.50元，首次覆盖给予“优于大市”评级。</p><p data-bbox=\"736 1848 1598 1915\">- 风险提示：原材料价格波动、下游需求不及预期、产能建设及释放不及预期。</p><div class=\"header\" data-bbox=\"1336 109 1538 203\">海通國际 HAITONG</div><div class=\"header\" data-bbox=\"63 134 311 204\">研究报告 Research Report 24 Sep 2023</div><div class=\"footer\" data-bbox=\"64 2066 266 2125\">吴旖婕 Yijie Wu lisa.yj.wu@htisec.com</div><div class=\"footer\" data-bbox=\"21 2156 1632 2306\">本研究报告由海通国际分销，海通国际是由海通国际研究有限公司，海通证券印度私人有限公司，海通国际株式会社和海通国际证券集团其他各成员单位的证券研究团队所组成的全球品牌，海通国际证券集团各成员分别在其许可的司法管辖区内从事证券活动。关于海通国际的分析师证明，重要披露声明和免责声明，请参阅附录。(Please see appendix for English translation of the disclaimer)</div></body>'} \n",
      " {'image': '../data/OmniDoc/images/eastmoney_d2bb183f5254ede5760932ac3118524aa7eddad551935713b435541f1525c1ee.pdf_3.jpg', 'height': 2339, 'width': 1654, 'prefix': 'QwenVL HTML', 'suffix': '<body><p data-bbox=\"125 239 612 272\">图表2：全国R&D经费投入强度（%）</p><div class=\"image\" data-bbox=\"115 286 1286 689\"></div><p data-bbox=\"119 704 461 737\">来源：国家统计局、农银国际证券</p><h2 data-bbox=\"192 845 1273 890\">四、专利密集型产业步入快速增长轨道，成为拉动经济增长的重要因素</h2><p data-bbox=\"122 938 1539 1374\">要将研发成果商业化，推动经济增长，关键因素是专利保护。专利是企业业务的一条护城河，专利也是研发活动的成果。有了专利保护，商业化价值就会提升。据国家统计局的数据，2022年专利密集型产业增加值占GDP的比重为 12.71% ，而2018年为 11.6% 。2018年至2022年，专利密集型产业增加值年复合增长率为 9.4% ，同期名义GDP值年复合增长率为 7.1% 。专利密集型产业经济的增长速度快于整体经济的增长速度。专利密集型产业虽然取得了较快的经济增长速度，但其在整体经济中的比重仍然较小，约为 13% 。由此看来，持续新质生产力发展的工作对于推动研发、实现创新、获得专利保护、提升商业价值、拉动经济增长具有重要意义。</p><p data-bbox=\"124 1414 559 1451\">图表3：专利密集型产业增加值</p><p data-bbox=\"122 1458 863 1495\">新装备制造业、新材料制造业和医药医疗产业增速较慢</p><div class=\"table\" data-bbox=\"121 1493 1299 1992\"><table><tr><td>(亿元)</td><td>2018</td><td>2020</td><td>2022</td><td>2018-2022 复合年增长率</td></tr><tr><td>专利密集型产业增加值</td><td>107,090</td><td>121,289</td><td>153,176</td><td>9.4%</td></tr><tr><td>新装备制造业</td><td>32,833</td><td>34,194</td><td>41,643</td><td>6.1%</td></tr><tr><td>信息通信技术服务业</td><td>19,472</td><td>26,415</td><td>33,888</td><td>14.9%</td></tr><tr><td>信息通信技术制造业</td><td>21,551</td><td>24,177</td><td>31,818</td><td>10.2%</td></tr><tr><td>新材料制造业</td><td>14,130</td><td>14,064</td><td>18,575</td><td>7.1%</td></tr><tr><td>医药医疗产业</td><td>9,465</td><td>10,984</td><td>12,880</td><td>8.0%</td></tr><tr><td>研发、设计和技术服务业</td><td>7,215</td><td>8,708</td><td>10,953</td><td>11.0%</td></tr><tr><td>环保产业</td><td>2,424</td><td>2,748</td><td>3,419</td><td>9.0%</td></tr></table></div><p data-bbox=\"123 1998 459 2029\">来源：国家统计局、农银国际证券</p><div class=\"header\" data-bbox=\"1123 54 1449 165\">農銀國際 ABC INTERNATIONAL ABCI SECURITIES COMPANY LIMITED</div><div class=\"footer\" data-bbox=\"816 2123 836 2149\">4</div></body>'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# hf_dataset = load_dataset(\"garage-bAInd/Open-Platypus\", \n",
    "#                        split=\"train\",\n",
    "#                        cache_dir= '/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/')\n",
    "# print(f\"{hf_dataset[0]}\")\n",
    "\n",
    "path_dir = './tmp/TestDatasets-Json/'\n",
    "json_dir_dataset = load_dataset('json', \n",
    "                                 data_files= {'train': [os.path.join(path_dir, f) for f in os.listdir(path_dir)]},)\n",
    "\n",
    "json_one_dataset = load_dataset('json', \n",
    "                                 data_files= './test_datasets.jsonl',\n",
    "                                 streaming=True)\n",
    "json_dict_dataset = load_dataset('json', \n",
    "                                 data_files= {'train': [os.path.join(path_dir, f) for f in os.listdir(path_dir)],\n",
    "                                              'test': [os.path.join(path_dir, f) for f in os.listdir(path_dir)]})\n",
    "\n",
    "# print(json_one_dataset, '\\n', json_dir_dataset, '\\n', json_dict_dataset)\n",
    "print(next(iter(json_one_dataset[\"train\"])), '\\n', json_dict_dataset['train'][0])\n",
    "\n",
    "# 处理数据\n",
    "def mulitmodel_data_process(data):\n",
    "    '''处理多模态数据'''\n",
    "    messages = [\n",
    "        {\"role\": \"system\",\"content\": [{\"type\": \"text\", \"text\": \"You are a helpful multimodal assistant.\"}]},\n",
    "        {\"role\": \"user\",\"content\": [\n",
    "            {\"type\": \"image\", \"image\": data['image']},\n",
    "            {\"type\": \"text\", \"text\": data['prefix']},],},\n",
    "        {\"role\": \"assistant\",\"content\": [\n",
    "            {\"type\": \"text\", \"text\": data['suffix']}],},]\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "json_dict_dataset_map = json_dict_dataset.map(mulitmodel_data_process)\n",
    "json_dict_dataset_map['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f0cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 452.88 examples/s]\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 498.81 examples/s]\n",
      "Tokenizing: 100%|██████████| 3/3 [00:00<00:00, 126.29 examples/s]\n",
      "Tokenizing: 100%|██████████| 3/3 [00:00<00:00, 123.23 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-21T23:29:40.431532+0800 | format_calibration_data | WARNING - Requested 512 calibration samples but the provided dataset only has 3. \n",
      "2026-01-21T23:29:40.438831+0800 | reset | INFO - Compression lifecycle reset\n",
      "2026-01-21T23:29:40.443245+0800 | from_modifiers | INFO - Creating recipe from modifiers\n",
      "2026-01-21T23:29:40.444274+0800 | _infer_mappings_from_model | INFO - No SmoothQuantModifier.mappings provided, inferring from model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-21T23:29:41.771372+0800 | initialize | INFO - Compression lifecycle initialized for 2 modifiers\n",
      "2026-01-21T23:29:41.773017+0800 | IndependentPipeline | INFO - Inferred `SequentialPipeline` for `SmoothQuantModifier`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing cache: 100%|██████████| 3/3 [00:00<00:00, 237.19it/s]\n",
      "(1/29): Calibrating: 100%|██████████| 3/3 [00:00<00:00,  3.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-21T23:29:43.289540+0800 | _apply_smoothing | INFO - Smoothing with model.layers.0.input_layernorm\n",
      "2026-01-21T23:29:43.304885+0800 | _apply_smoothing | INFO - Smoothing with model.layers.0.post_attention_layernorm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(1/29): Propagating: 100%|██████████| 3/3 [00:00<00:00, 16.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[\u001b[33m'\u001b[39m\u001b[33mprefix\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[\u001b[33m'\u001b[39m\u001b[33msuffix\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m}\n\u001b[32m     34\u001b[39m json_dict_dataset = json_dict_dataset.map(to_prompt)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43moneshot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/root/autodl-tmp/Model/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#\"Qwen/Qwen2.5-1.5B-Instruct\",\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m/root/autodl-tmp/Model\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson_dict_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_column\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrecipe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./tmp/LLMCompressor-GPTQ-W8A8-0.8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./tmp/LLMCompressor-GPTQ-W8A8-0.8/log\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_calibration_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/miniconda3/envs/docparse/lib/python3.12/site-packages/llmcompressor/entrypoints/oneshot.py:319\u001b[39m, in \u001b[36moneshot\u001b[39m\u001b[34m(model, distill_teacher, config_name, tokenizer, processor, cache_dir, use_auth_token, precision, tie_word_embeddings, trust_remote_code_model, save_compressed, model_revision, recipe, recipe_args, clear_sparse_session, stage, dataset, dataset_config_name, dataset_path, num_calibration_samples, shuffle_calibration_samples, max_seq_length, pad_to_max_length, text_column, concatenate_data, streaming, overwrite_cache, preprocessing_num_workers, min_tokens_per_module, calibrate_moe_context, quantization_aware_calibration, output_dir, log_dir, **kwargs)\u001b[39m\n\u001b[32m    315\u001b[39m local_args = {\n\u001b[32m    316\u001b[39m     k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m().items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mlocal_args\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mkwargs\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    317\u001b[39m }\n\u001b[32m    318\u001b[39m one_shot = Oneshot(**local_args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m \u001b[43mone_shot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    321\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m one_shot.model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/miniconda3/envs/docparse/lib/python3.12/site-packages/llmcompressor/entrypoints/oneshot.py:149\u001b[39m, in \u001b[36mOneshot.__call__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[33;03mPerforms one-shot calibration.\u001b[39;00m\n\u001b[32m    138\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    146\u001b[39m calibration_dataloader = get_calibration_dataloader(\n\u001b[32m    147\u001b[39m     \u001b[38;5;28mself\u001b[39m.dataset_args, \u001b[38;5;28mself\u001b[39m.processor\n\u001b[32m    148\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_recipe_modifiers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcalibration_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcalibration_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrecipe_stage\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecipe_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m post_process(\n\u001b[32m    154\u001b[39m     model_args=\u001b[38;5;28mself\u001b[39m.model_args,\n\u001b[32m    155\u001b[39m     recipe_args=\u001b[38;5;28mself\u001b[39m.recipe_args,\n\u001b[32m    156\u001b[39m     output_dir=\u001b[38;5;28mself\u001b[39m.output_dir,\n\u001b[32m    157\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/miniconda3/envs/docparse/lib/python3.12/site-packages/llmcompressor/entrypoints/oneshot.py:192\u001b[39m, in \u001b[36mOneshot.apply_recipe_modifiers\u001b[39m\u001b[34m(self, calibration_dataloader, recipe_stage)\u001b[39m\n\u001b[32m    190\u001b[39m modifiers = session.lifecycle.recipe.modifiers\n\u001b[32m    191\u001b[39m pipeline = CalibrationPipeline.from_modifiers(modifiers, user=user_pipeline)\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcalibration_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m session.finalize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/miniconda3/envs/docparse/lib/python3.12/site-packages/llmcompressor/pipelines/independent/pipeline.py:45\u001b[39m, in \u001b[36mIndependentPipeline.__call__\u001b[39m\u001b[34m(model, dataloader, dataset_args)\u001b[39m\n\u001b[32m     42\u001b[39m pipeline_name = pipeline.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\n\u001b[32m     43\u001b[39m _logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInferred `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmod_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/miniconda3/envs/docparse/lib/python3.12/site-packages/llmcompressor/pipelines/sequential/pipeline.py:100\u001b[39m, in \u001b[36mSequentialPipeline.__call__\u001b[39m\u001b[34m(model, dataloader, dataset_args)\u001b[39m\n\u001b[32m     97\u001b[39m prop_desc = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubgraph_index\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_subgraphs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): Propagating\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# reduce memory movement by keeping modules onloaded\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_offloading\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# do a preliminary pass to trigger modifier hooks\u001b[39;49;00m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcalib_desc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/miniconda3/envs/docparse/lib/python3.12/contextlib.py:144\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/miniconda3/envs/docparse/lib/python3.12/site-packages/compressed_tensors/utils/offload.py:613\u001b[39m, in \u001b[36mdisable_offloading\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    611\u001b[39m hook.offload = offload\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m module.named_parameters(recurse=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m     \u001b[43mupdate_offload_parameter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    614\u001b[39m hook.post_forward(module, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/miniconda3/envs/docparse/lib/python3.12/site-packages/compressed_tensors/utils/offload.py:276\u001b[39m, in \u001b[36mupdate_offload_parameter\u001b[39m\u001b[34m(module, name, data, offload_device)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_offloaded_params(module):\n\u001b[32m    275\u001b[39m     weights_map = module._hf_hook.weights_map\n\u001b[32m--> \u001b[39m\u001b[32m276\u001b[39m     \u001b[43moffload_to_weights_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffload_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/miniconda3/envs/docparse/lib/python3.12/site-packages/compressed_tensors/utils/offload.py:335\u001b[39m, in \u001b[36moffload_to_weights_map\u001b[39m\u001b[34m(weights_map, key, value, offload_device)\u001b[39m\n\u001b[32m    333\u001b[39m     dataset = weights_map.dataset\n\u001b[32m    334\u001b[39m     key = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweights_map.prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m335\u001b[39m     \u001b[43moffload_to_weights_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffload_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weights_map, OffloadedWeightsLoader):\n\u001b[32m    338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m weights_map.all_keys:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/miniconda3/envs/docparse/lib/python3.12/site-packages/compressed_tensors/utils/offload.py:342\u001b[39m, in \u001b[36moffload_to_weights_map\u001b[39m\u001b[34m(weights_map, key, value, offload_device)\u001b[39m\n\u001b[32m    339\u001b[39m     weights_map.all_keys.append(key)\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(weights_map.index) <= \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m offload_device != \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     \u001b[43moffload_to_weights_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_map\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffload_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    345\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    346\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUpdating weights_map with disk offloading is not implemented yet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    347\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/autodl-tmp/miniconda3/envs/docparse/lib/python3.12/site-packages/compressed_tensors/utils/offload.py:365\u001b[39m, in \u001b[36moffload_to_weights_map\u001b[39m\u001b[34m(weights_map, key, value, offload_device)\u001b[39m\n\u001b[32m    360\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    361\u001b[39m                     \u001b[33m\"\u001b[39m\u001b[33mCannot infer offload device from empty weights_map\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m                 )\n\u001b[32m    363\u001b[39m             offload_device = tens.device\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     weights_map[key] = \u001b[43mvalue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_device\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    368\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[32m    369\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUpdating offload data not implemented for weights_map of type \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    370\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(weights_map)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    371\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "from llmcompressor.modifiers.smoothquant import SmoothQuantModifier\n",
    "from llmcompressor.modifiers.quantization import GPTQModifier\n",
    "from llmcompressor import oneshot\n",
    "\n",
    "recipe = [\n",
    "    SmoothQuantModifier(smoothing_strength=0.8),\n",
    "    GPTQModifier(scheme=\"W8A8\", targets=\"Linear\", ignore=[\"lm_head\"]),\n",
    "]\n",
    "# 直接使用hf上数据\n",
    "# oneshot(\n",
    "#     model='/root/autodl-tmp/Model/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306',#\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "#     cache_dir= '/root/autodl-tmp/Model',\n",
    "#     dataset=\"open_platypus\",\n",
    "#     recipe=recipe,\n",
    "#     output_dir=\"./tmp/LLMCompressor-GPTQ-W8A8-0.8\",\n",
    "#     log_dir= './tmp/LLMCompressor-GPTQ-W8A8-0.8/log',\n",
    "    \n",
    "#     max_seq_length=2048,\n",
    "#     num_calibration_samples=512,\n",
    "# )\n",
    "# 直接使用自定义数据集 其支持参数类型为 Optional[Union[str, \"Dataset\", \"DatasetDict\"]] 那么就不能使用 流处理加载数据\n",
    "\n",
    "def to_prompt(example):\n",
    "    return {\"prompt\": f\"{example['prefix']} {example['suffix']}\"}\n",
    "json_dict_dataset = json_dict_dataset.map(to_prompt)\n",
    "oneshot(\n",
    "    model='/root/autodl-tmp/Model/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306',#\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    cache_dir= '/root/autodl-tmp/Model',\n",
    "    dataset= json_dict_dataset,\n",
    "    text_column=\"prompt\",\n",
    "\n",
    "    recipe=recipe,\n",
    "    output_dir=\"./tmp/LLMCompressor-GPTQ-W8A8-0.8\",\n",
    "    log_dir= './tmp/LLMCompressor-GPTQ-W8A8-0.8/log',\n",
    "    \n",
    "    max_seq_length=2048,\n",
    "    num_calibration_samples=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f842d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/miniconda3/envs/docparse/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-22 22:35:44 [__init__.py:216] Automatically detected platform cuda.\n",
      "INFO 01-22 22:35:48 [utils.py:233] non-default args: {'disable_log_stats': True, 'model': '/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/Qwen2.5-1.5B-GPTQ-W8A8'}\n",
      "INFO 01-22 22:35:48 [model.py:547] Resolved architecture: Qwen2ForCausalLM\n",
      "INFO 01-22 22:35:48 [model.py:1510] Using max model len 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-22 22:35:54,970\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 01-22 22:35:54 [scheduler.py:205] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:35:56 [core.py:644] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:35:56 [core.py:77] Initializing a V1 LLM engine (v0.11.0) with config: model='/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/Qwen2.5-1.5B-GPTQ-W8A8', speculative_config=None, tokenizer='/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/Qwen2.5-1.5B-GPTQ-W8A8', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=compressed-tensors, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/Qwen2.5-1.5B-GPTQ-W8A8, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\",\"vllm.sparse_attn_indexer\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":[2,1],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"use_inductor_graph_partition\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:35:57 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m WARNING 01-22 22:35:58 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:35:58 [gpu_model_runner.py:2602] Starting to load model /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/Qwen2.5-1.5B-GPTQ-W8A8...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:35:58 [gpu_model_runner.py:2634] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:35:58 [compressed_tensors_w8a8_int8.py:52] Using CutlassScaledMMLinearKernel for CompressedTensorsW8A8Int8\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:35:59 [cuda.py:366] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  1.20it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:36:00 [default_loader.py:267] Loading weights took 0.97 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:36:01 [gpu_model_runner.py:2653] Model loading took 2.1034 GiB and 1.283012 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:36:12 [backends.py:548] Using cache directory: /root/.cache/vllm/torch_compile_cache/c3c5955daf/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:36:12 [backends.py:559] Dynamo bytecode transform time: 9.80 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:36:17 [backends.py:197] Cache the graph for dynamic shape for later use\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:36:53 [backends.py:218] Compiling a graph for dynamic shape takes 40.38 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:37:01 [monitor.py:34] torch.compile takes 50.18 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:37:03 [gpu_worker.py:280] The init_snapshot total_memory: 23.55 GiB The non_kv_cache_memory: 3.78 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:37:03 [gpu_worker.py:304] Available KV cache memory: 17.41 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:37:04 [kv_cache_utils.py:1089] GPU KV cache size: 651,984 tokens（40749// 1// 16）\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:37:04 [kv_cache_utils.py:1094] Maximum concurrency for 32,768 tokens per request: 19.90x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:37:04 [gpu_model_runner.py:3771] KVCacheConfig(num_blocks=40749, kv_cache_tensors=[KVCacheTensor(size=667631616, shared_by=['model.layers.0.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.1.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.2.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.3.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.4.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.5.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.6.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.7.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.8.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.9.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.10.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.11.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.12.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.13.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.14.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.15.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.16.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.17.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.18.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.19.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.20.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.21.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.22.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.23.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.24.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.25.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.26.self_attn.attn']), KVCacheTensor(size=667631616, shared_by=['model.layers.27.self_attn.attn'])], kv_cache_groups=[KVCacheGroupSpec(layer_names=['model.layers.0.self_attn.attn', 'model.layers.1.self_attn.attn', 'model.layers.2.self_attn.attn', 'model.layers.3.self_attn.attn', 'model.layers.4.self_attn.attn', 'model.layers.5.self_attn.attn', 'model.layers.6.self_attn.attn', 'model.layers.7.self_attn.attn', 'model.layers.8.self_attn.attn', 'model.layers.9.self_attn.attn', 'model.layers.10.self_attn.attn', 'model.layers.11.self_attn.attn', 'model.layers.12.self_attn.attn', 'model.layers.13.self_attn.attn', 'model.layers.14.self_attn.attn', 'model.layers.15.self_attn.attn', 'model.layers.16.self_attn.attn', 'model.layers.17.self_attn.attn', 'model.layers.18.self_attn.attn', 'model.layers.19.self_attn.attn', 'model.layers.20.self_attn.attn', 'model.layers.21.self_attn.attn', 'model.layers.22.self_attn.attn', 'model.layers.23.self_attn.attn', 'model.layers.24.self_attn.attn', 'model.layers.25.self_attn.attn', 'model.layers.26.self_attn.attn', 'model.layers.27.self_attn.attn'], kv_cache_spec=FullAttentionSpec(block_size=16, num_kv_heads=2, head_size=128, dtype=torch.bfloat16, sliding_window=None, attention_chunk_size=None))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:03<00:00, 21.53it/s]\n",
      "Capturing CUDA graphs (decode, FULL): 100%|██████████| 35/35 [00:01<00:00, 17.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:37:11 [gpu_model_runner.py:3480] Graph capturing finished in 7 secs, took 0.65 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=677787)\u001b[0;0m INFO 01-22 22:37:11 [core.py:210] init engine (profile, create kv cache, warmup model) took 70.23 seconds\n",
      "INFO 01-22 22:37:14 [llm.py:306] Supported_tasks: ['generate']\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "multiprocessing.set_start_method('spawn', force=True)\n",
    "model = LLM(\"/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/Qwen2.5-1.5B-GPTQ-W8A8\")\n",
    "\n",
    "# output = model.generate(\"晚上睡不着怎么办?\")\n",
    "# output[0].outputs[0].text\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,           # 温度：值越高输出越随机\n",
    "    top_p=0.9,                # 核采样：控制词汇多样性\n",
    "    top_k=50,                 # Top-k采样\n",
    "    max_tokens=1024,          # 最大生成长度\n",
    "    frequency_penalty=0.1,    # 频率惩罚：减少重复内容\n",
    "    presence_penalty=0.1,     # 存在惩罚：鼓励新话题\n",
    "    repetition_penalty=1.1,   # 重复惩罚（某些模型支持）\n",
    ")\n",
    "\n",
    "output = model.generate(\"晚上睡不着怎么办?\", sampling_params)\n",
    "print(output[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea2517d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/autodl-tmp/miniconda3/envs/docparse/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`generation_config` default values have been modified to match model-specific defaults: {'top_k': 20, 'pad_token_id': 151643, 'bos_token_id': 151643, 'eos_token_id': [151645, 151643]}. If this is not desired, please set these values explicitly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: 晚上睡不着怎么办？\n",
      "回复(45.25086069107056): 这里有一些建议，可能对你有帮助：\n",
      "\n",
      "1. **尝试放松技巧**：深呼吸、渐进性肌肉放松或冥想等方法可以帮助你减轻焦虑和压力。\n",
      "\n",
      "2. **限制咖啡因和酒精的摄入**：特别是在睡前，避免饮用含咖啡因的饮料（如咖啡、茶）和酒精，因为它们可能会干扰你的睡眠。\n",
      "\n",
      "3. **保持良好的睡眠卫生**：确保你的睡眠环境舒适、安静且黑暗。使用舒适的床上用品，并尽量在相同的时间上床睡觉和起床。\n",
      "\n",
      "4. **定期锻炼**：尽管睡前锻炼可能会使你更加清醒，但定期的轻度至中度运动可以帮助你改善睡眠质量。\n",
      "\n",
      "5. **晚餐选择**：避免晚餐过晚或过重，特别是避免摄入辛辣食物，因为它们可能会引起胃部不适，从而影响睡眠。\n",
      "\n",
      "6. **记录日志**：有时写下你的想法和担忧，可以帮助你更好地理解自己的思维模式，从而更容易\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt: 写一段关于秋天的诗\n",
      "回复(33.00411057472229): 秋风起，落叶舞，\n",
      "稻田金黄，丰收喜。\n",
      "山河换新装，色彩斑斓，\n",
      "秋天来了，带来希望。\n",
      "\n",
      "果实累累挂枝头，\n",
      "菊花绽放，香气扑鼻。\n",
      "寒露滴滴，滴落心间，\n",
      "秋天的美，让人陶醉。\n",
      "\n",
      "天空高远，白云悠闲，\n",
      "秋水共长天一色。\n",
      "人们穿着轻盈，踏着秋韵，\n",
      "享受着这个季节的静谧与温柔。\n",
      "\n",
      "夜晚降临，月色皎洁，\n",
      "星星闪烁，宛如繁星点点。\n",
      "秋夜宁静，万物沉睡，\n",
      "让心灵得以净化，回归自然。\n",
      "\n",
      "秋天，你是大自然的杰作，\n",
      "你是生命的赞歌，是希望的使者。\n",
      "在这个收获的季节里，\n",
      "让我们一同品味，这份美好的馈赠。\n",
      "------------------------------------------------------------\n",
      "\n",
      "Prompt: 解释一下量子纠缠是什么\n",
      "回复(16.59041476249695): 量子纠缠是一种量子力学现象，指两个或多个粒子之间形成一种特殊的关系，使得它们的某些性质在空间上是相关联的。当其中一个粒子的状态改变时，另一个粒子的状态也会立即发生变化，即使它们相隔很远的距离。\n",
      "\n",
      "量子纠缠是一个非常有趣和神秘的现象，因为它超越了经典物理学的理解，并且在量子计算、量子通信等领域有广泛的应用。\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "llm_model_path = \"/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/Qwen2.5-1.5B-GPTQ-W8A8\"\n",
    "model = AutoModelForCausalLM.from_pretrained(llm_model_path,device_map=\"auto\",)\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_path)\n",
    "\n",
    "gen_config = GenerationConfig(\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.05,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "SAMPLE_PROMPTS = [\n",
    "    \"晚上睡不着怎么办？\",\n",
    "    \"写一段关于秋天的诗\",\n",
    "    \"解释一下量子纠缠是什么\",\n",
    "]\n",
    "\n",
    "for prompt in SAMPLE_PROMPTS:\n",
    "    s_time = time.time()\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\",   \"content\": prompt},\n",
    "    ]\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    output_ids = model.generate(**inputs, generation_config=gen_config)\n",
    "    \n",
    "    response = tokenizer.decode(output_ids[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    print(f\"回复({time.time()- s_time}): {response.strip()}\\n{'-'*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270244b6",
   "metadata": {},
   "source": [
    "## 模型微调\n",
    "> https://gist.github.com/willccbb/4676755236bb08cab5f4e54a0475d6fb\n",
    "\n",
    "https://github.com/shangxiaaabb/ProjectCode/blob/main/code/Python/DFModelCode/DF_acceralate/GRPO_training.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0282f618",
   "metadata": {},
   "source": [
    "## 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccd767a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "\n",
    "def load_model():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "                                                cache_dir= '/root/autodl-tmp/Model/', \n",
    "                                                mirror='https://hf-mirror.com')\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "                                                cache_dir= '/root/autodl-tmp/Model/', \n",
    "                                                mirror='https://hf-mirror.com')\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_GGUF(lora_path=None):\n",
    "    from llama_cpp import Llama\n",
    "\n",
    "    llm = Llama(\n",
    "        model_path=\"/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-Q8_0.gguf\",\n",
    "        lora_path= lora_path,\n",
    "        n_ctx=8192,\n",
    "        n_gpu_layers=-1,\n",
    "        chat_format=\"chatml\",\n",
    "        verbose=False\n",
    "    )\n",
    "    return llm\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个有帮助的助手，用简洁的中文回答。\"},\n",
    "    {\"role\": \"user\",   \"content\": \"晚上睡不着怎么办?\"}\n",
    "]\n",
    "model, tokenizer = load_model()\n",
    "model_gguf = load_GGUF()\n",
    "model_gguf_lora = load_GGUF(lora_path= '/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Lora-Qwen3-1.5B-BF16.gguf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb75de67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可以试试深呼吸、听轻音乐、喝杯热牛奶或泡个热水澡等方法。如果经常失眠，建议咨询医生。<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adea9d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI 回答：\n",
      "可尝试用热水泡脚，听听轻音乐，或者看看电视。\n"
     ]
    }
   ],
   "source": [
    "response = model_gguf.create_chat_completion(\n",
    "    messages,\n",
    "    max_tokens=300,\n",
    "    temperature=0.75,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.1,\n",
    "    stream=False  # 改成 True 即可流式输出\n",
    ")\n",
    "\n",
    "print(\"AI 回答：\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docparse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
