{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca4d5c76",
   "metadata": {},
   "source": [
    "# 大语言模型量化处理\n",
    "测试模型直接为：Qwen2-0.5B-Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cea3f3b",
   "metadata": {},
   "source": [
    "## 使用llama.cpp量化  \n",
    "> https://qwen.readthedocs.io/zh-cn/latest/quantization/llama.cpp.html\n",
    "\n",
    "比如说对于`Qwen2.5-1.5B-Instruct`模型，一般使用是：\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))\n",
    "```\n",
    "为了实现GGUF格式转换，按照下面步骤进行（为了步骤简洁，内容全部放到终结里面）\n",
    "### 1、无校准量化GGUF\n",
    "```python\n",
    "# 首先创建GGUF文件  转换文件\n",
    "python convert-hf-to-gguf.py 模型名称/地址 --outtype bf16 --outfile Qwen3-1.5B-BF16.gguf \n",
    "# 其中的 outtype后面的数据类型可以是：bf16 f32等\n",
    "```\n",
    "在执行上面代码之后直接将模型量化到`8 bit`：\n",
    "```python\n",
    "./build/bin/llama-quantize --quantize --model Qwen3-1.5B-F16.gguf --output Qwen3-1.5B-Q8_0.gguf --type Q8_0\n",
    "```\n",
    "`Q8_0`是一个量化预设的代号\n",
    "### 2、使用重要性矩阵量化GGUF\n",
    "```python\n",
    "# 首先\n",
    "./llama-imatrix -m Qwen3-1.5B-F16.gguf -f calibration-text.txt --chunk 512 -o Qwen3-1.5B-imatrix.dat -ngl 80\n",
    "# \n",
    "./llama-quantize --imatrix Qwen3-1.5B-imatrix.dat Qwen3-1.5B-F16.gguf Qwen3-1.5B-Q4_K_M.gguf Q4_K_M\n",
    "```\n",
    "### 两种量化方式总结\n",
    "* **首先解释代码使用**：\n",
    "\n",
    "在此之前可以使用`python convert_hf_to_gguf.py --help`了解一些这个脚本都有哪些参数，`python convert_hf_to_gguf.py --print-supported-models`直接看一下支持哪些模型进行量化，一般而言模型名称就和`config.json`中是一致的比如`Qwen2ForCausalLM`。量化开始：**首先**，在下载得到模型权重之后，在`cache_dir`里面就会有模型权重，比如说上面过程中模型权重就会保存为：`/Model/models--Qwen--Qwen2.5-1.5B-Instruct`，那么在创建以及转换文件过程中就需要将其中的 **模型名称/地址**进行替换，那么**完整带脚本**就是：`python convert_hf_to_gguf.py /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/Model/models--Qwen--Qwen2.5-1.5B-Instruct --outfile /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-BF16.gguf --outtype bf16 --verbose`\n",
    "**不过**，上面代码还是会出错，因为实际模型文件在`code/Python/DFModelCode/DF_acceralate/Model/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306`因此需要将上面的文件路径改为：`/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/Model/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306`最后输出：\n",
    "```bash\n",
    "...\n",
    "INFO:hf-to-gguf:Model successfully exported to /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-BF16.gguf\n",
    "```\n",
    "就表示成功了，而后可以通过`./build/bin/llama-quantize --help`去检查支持哪些参数，**并且进行量化**：`./build/bin/llama-quantize /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-BF16.gguf /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-Q8_0.gguf q8_0`。不过还有一点有意思的，比如说我的模型一般都会用lora进行微调，我需要对lora微调模型进行处理，类似的：\n",
    "```bash\n",
    "python convert_lora_to_gguf.py /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/test-lora/ --outfile /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Lora-Qwen3-1.5B-BF16.gguf --outtype bf16 --verbose\n",
    "\n",
    "./build/bin/llama-quantize /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Lora-Qwen3-1.5B-BF16.gguf /root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Lora-Qwen3-1.5B-Q8_0.gguf q8_0\n",
    "```\n",
    "### 使用GGUF量化模型\n",
    "```bash\n",
    "pip install llama-cpp-python\n",
    "# 直接去 https://github.com/abetlen/llama-cpp-python/releases 下载对应的版本 和flash-attn一样\n",
    "pip install llama_cpp_python-0.3.16-cp312-cp312-linux_x86_64.whl\n",
    "```\n",
    "这样一来就安装好了就可以直接加载GGUF模型权重了（和非量化模型推理10.8s，**量化后推理：3.5s**）：\n",
    "```python\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama(\n",
    "    model_path=\"/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-Q8_0.gguf\",\n",
    "    n_ctx=8192,\n",
    "    n_gpu_layers=-1,\n",
    "    chat_format=\"chatml\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个有帮助的助手，用简洁的中文回答。\"},\n",
    "    {\"role\": \"user\",   \"content\": \"晚上睡不着怎么办\"}\n",
    "]\n",
    "\n",
    "# 生成\n",
    "response = llm.create_chat_completion(\n",
    "    messages,\n",
    "    max_tokens=300,\n",
    "    temperature=0.75,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.1,\n",
    "    stream=False  # 改成 True 即可流式输出\n",
    ")\n",
    "\n",
    "print(\"AI 回答：\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b5a8878",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_context: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "\n",
    "def load_model():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "                                                cache_dir= '/root/autodl-tmp/Model/', \n",
    "                                                mirror='https://hf-mirror.com')\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "                                                cache_dir= '/root/autodl-tmp/Model/', \n",
    "                                                mirror='https://hf-mirror.com')\n",
    "    return model, tokenizer\n",
    "\n",
    "def load_GGUF(lora_path=None):\n",
    "    from llama_cpp import Llama\n",
    "\n",
    "    llm = Llama(\n",
    "        model_path=\"/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Qwen3-1.5B-Q8_0.gguf\",\n",
    "        lora_path= lora_path,\n",
    "        n_ctx=8192,\n",
    "        n_gpu_layers=-1,\n",
    "        chat_format=\"chatml\",\n",
    "        verbose=False\n",
    "    )\n",
    "    return llm\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"你是一个有帮助的助手，用简洁的中文回答。\"},\n",
    "    {\"role\": \"user\",   \"content\": \"晚上睡不着怎么办?\"}\n",
    "]\n",
    "model, tokenizer = load_model()\n",
    "model_gguf = load_GGUF()\n",
    "model_gguf_lora = load_GGUF(lora_path= '/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/GGUF1-Lora-Qwen3-1.5B-BF16.gguf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb0723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "全零 LoRA adapter 已生成到：/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/test-lora/\n",
      "文件列表：\n",
      " - README.md\n",
      " - adapter_model.safetensors\n",
      " - adapter_config.json\n",
      "\n",
      "所有 LoRA 权重均为 0，加载后效果等同于基模型（可用于测试 GGUF 转换/量化/加载链路）\n",
      "警告：base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight 不为零！\n",
      "警告：base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight 不为零！\n"
     ]
    }
   ],
   "source": [
    "# 生成测试lora\n",
    "import os\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "rank = 8  \n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"] \n",
    "output_dir = \"/root/autodl-tmp/Code/Big-Yellow-J.github.io/code/Python/DFModelCode/DF_acceralate/tmp/test-lora/\"  \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"cpu\",\n",
    "    cache_dir= '/root/autodl-tmp/Model/', \n",
    "    low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=rank,\n",
    "    lora_alpha=16,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.0,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    init_lora_weights=False\n",
    ")\n",
    "\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "peft_model.save_pretrained(output_dir, save_embedding_layers=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb75de67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可以试试深呼吸、听轻音乐、喝杯热牛奶或泡个热水澡等方法。如果经常失眠，建议咨询医生。<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=40)\n",
    "print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "adea9d6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI 回答：\n",
      "可尝试用热水泡脚，听听轻音乐，或者看看电视。\n"
     ]
    }
   ],
   "source": [
    "response = model_gguf.create_chat_completion(\n",
    "    messages,\n",
    "    max_tokens=300,\n",
    "    temperature=0.75,\n",
    "    top_p=0.95,\n",
    "    repeat_penalty=1.1,\n",
    "    stream=False  # 改成 True 即可流式输出\n",
    ")\n",
    "\n",
    "print(\"AI 回答：\")\n",
    "print(response[\"choices\"][0][\"message\"][\"content\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "z-image",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
