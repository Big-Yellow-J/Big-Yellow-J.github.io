---
layout: mypost
title: 模型推理框架——SGLang（1）
categories: 量化部署
address: 武汉🏯
extMath: true
show_footer_image: true
tags:
- 模型推理
- 模型部署
---
SGLang[^2] 是一款面向大语言模型（LLM）和视觉语言模型（VLM）的高性能推理框架，通过精心设计的后端运行时与前端语言协同工作，使模型交互更加高效且可控。按照论文里面的描述对于SGLang中分为前后端，大致结构如下：
![](https://s2.loli.net/2025/10/28/UjuBCk9FoTdJXml.png)
> 和vllm一样SGLang都是一个加速推理框架，两者对比效果[^1][^3]

## SGLang核心技术分析
### RadixAttention
为了节约KV-cache显存占用以及加速推理过程在SGLang中引入了RadixAttention，其主要思路如下：
**智能缓存保留**：每次请求完成后，当前请求的 KV Cache 不会立即释放，而是通过 RadixAttention 算法精心保留在 GPU 显存中。同时，RadixAttention 将序列 token 与 KV Cache 之间的映射关系存储在专用的 radix tree 数据结构中。
**高效缓存命中**：新请求到达时，SGLang 调度器利用 radix tree 执行前缀匹配算法，快速判断新请求是否能够复用已有缓存。一旦命中，调度器立即复用现有 KV Cache 处理该请求，显著减少计算开销。
**智能缓存管理**：考虑到 GPU 显存资源有限，缓存中的 KV Cache 需要合理管理。RadixAttention 采用经典的 LRU（Least Recently Used，最近最少使用）策略进行缓存淘汰，这一机制与操作系统内存管理中的页面置换算法相似，确保最有价值的缓存得到保留。
![](https://s2.loli.net/2025/10/28/Ebq2GNgiJY3XDWx.png)
上图中：绿色代表新添加的节点、蓝色代表当前时间点访问的缓存节点、红色代表淘汰的节点。其实就是根据用户提出问题去到radix tree中找到对应的缓存然后加入到计算中。
### Compressed FSMs
对于Compressed FSMs过程主要是根据：一般而言用户在输出内容过程一般会有一个“前缀词”但是根据LLM生成过程这个“前缀词”对于模型的输出影响不大，因此可以将“前缀词”进行压缩，从而提高模型推理速度。
![](https://s2.loli.net/2025/10/28/wu46HSdKUil3sVI.png)
> 在LLM生成过程中[^5]往往是得到 $i$ 之后继续使用A然后去生成后续的 $i+1$，这就是所谓的FSM（Finite State Machine，有限状态机），争对这种问题提出改进 Interleaved-Based方法[^6]具体的原理为：把整个结构化输出（比如一个 JSON 对象）分割成若干段（chunks）— 这些段有的“固定片段”（prefill／填充片段）、有的“生成片段”（需要模型生成／受控生成）。然后“交错（interleave）”执行：借助 “prefill 多 token一次走”＋“受控生成少量 tokens” 的混合模式，从而加快整体生成
> ![](https://s2.loli.net/2025/10/29/SaAbG8rYQH6hwcf.png)
> 比如说需要生成：`{ "name": "<MODEL_GEN_1>", "age": <MODEL_GEN_2> }` 那么交错生成过程为先去固定“前缀词”（`{ "name": "`）生成`<MODEL_GEN_1>`而后再去固定“前缀词”（`", "age":`）依次类推。

对于Compressed FSMs**具体过程描述如下**：
![](https://s2.loli.net/2025/10/29/5rDovM47VEzaYk3.png)

### 总结
## 参考
[^1]: [对比了 vLLM 和 SGLang 在两张英伟达 GPU 上的表现 - SGLang 用数据并行吊打它](https://www.reddit.com/r/LocalLLaMA/comments/1jjl45h/compared_performance_of_vllm_vs_sglang_on_2/?tl=zh-hans)
[^2]: [https://arxiv.org/pdf/2312.07104](https://arxiv.org/pdf/2312.07104)
[^3]: [vLLM和SGLang的比较（最新版本）](https://zhuanlan.zhihu.com/p/1909197327287182496)
[^4]: [https://zhuanlan.zhihu.com/p/30886364337](https://zhuanlan.zhihu.com/p/30886364337)
[^5]: [https://lmsys.org/blog/2024-02-05-compressed-fsm/](https://lmsys.org/blog/2024-02-05-compressed-fsm/)
[^6]: [guidance-Github](https://github.com/guidance-ai/guidance?tab=readme-ov-file#guidance-acceleration)