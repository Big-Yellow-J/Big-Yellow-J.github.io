---
layout: mypost
title: æ·±å…¥æµ…å‡ºäº†è§£ç”Ÿæˆæ¨¡å‹-7ï¼šç”ŸæˆåŠ é€Ÿç­–ç•¥æ¦‚è¿°
categories: ç”Ÿæˆæ¨¡å‹
extMath: true
images: true
address: é•¿æ²™ğŸŒ·
show_footer_image: true
tags:
- ç”Ÿæˆæ¨¡å‹
- diffusion model
- é‡åŒ–æŠ€æœ¯
show: true
stickie: true
description: 
---
## æ‰©æ•£æ¨¡å‹ç”ŸæˆåŠ é€Ÿç­–ç•¥
Diffusionæ¨ç†åŠ é€Ÿçš„æ–¹æ¡ˆï¼Œä¸»è¦åŒ…æ‹¬Cacheã€é‡åŒ–ã€åˆ†å¸ƒå¼æ¨ç†ã€é‡‡æ ·å™¨ä¼˜åŒ–å’Œè’¸é¦ç­‰ã€‚ä¸‹é¢å†…å®¹ä¸»è¦æ˜¯å»å¯¹Cacheã€è®¡ç®—åŠ é€Ÿæ¡†æ¶ä»¥åŠé‡åŒ–æŠ€æœ¯è¿›è¡Œä»‹ç»
> SDæ¨¡å‹åŠ é€Ÿæ–¹å¼ï¼š[https://github.com/xlite-dev/Awesome-DiT-Inference?tab=readme-ov-file#Quantization](https://github.com/xlite-dev/Awesome-DiT-Inference?tab=readme-ov-file#Quantization)

ä¸è¿‡å€¼å¾—æ³¨æ„çš„æ˜¯å¯¹äºä¸‹é¢å†…å®¹ï¼Œé¦–å…ˆä»‹ç»åŠ é€Ÿæ¡†æ¶ï¼ˆè¿™éƒ¨åˆ†å†…å®¹ä¸»è¦æ˜¯ä»‹ç»è¿›è¡ŒåŠ é€Ÿçš„ä¸€äº›å°trickï¼Œä¸»è¦æ˜¯ç›´æ¥é€šè¿‡apiå»åŠ é€Ÿï¼‰ã€cacheä»¥åŠé‡åŒ–ä¸€èˆ¬å°±ä¼šæ¶‰åŠåˆ°ä¸€äº›ç®—æ³•çš„åŸºæœ¬åŸç†ã€‚æ‰€æœ‰çš„æµ‹è¯•ä»£ç ï¼š
### ä¸€èˆ¬åŠ é€Ÿæ¡†æ¶
è¿™éƒ¨åˆ†å†…å®¹çš„è¯æ¯”è¾ƒæ‚ï¼ˆç›´æ¥æ€»ç»“[huggingface](https://huggingface.co/docs/diffusers/optimization/fp16#scaled-dot-product-attention)å†…å®¹ï¼‰ï¼Œ1ã€**ç›´æ¥ä½¿ç”¨attnè®¡ç®—åŠ é€Ÿåç«¯**ï¼Œæ¯”å¦‚è¯´ä¸€èˆ¬å°±æ˜¯ç›´æ¥ä½¿ç”¨æ¯”å¦‚è¯´`flash_attn`è¿›è¡Œattentionè®¡ç®—åŠ é€Ÿï¼Œæ¯”å¦‚è¯´ï¼š
```python
pipeline.transformer.set_attention_backend("_flash_3_hub") # å¯ç”¨flash attnè®¡ç®—åŠ é€Ÿ
pipeline.transformer.reset_attention_backend()             # å…³é—­flash attnè®¡ç®—åŠ é€Ÿ
```
ä¸è¿‡å€¼å¾—æ³¨æ„çš„æ˜¯`_flash_3_hub` åªæ”¯æŒéhopperæ¶æ„ï¼Œå› æ­¤å¯ä»¥ç›´æ¥å°±ä½¿ç”¨`set_attention_backend("flash")`ã€‚2ã€**ç›´æ¥ä½¿ç”¨**`torch.compile`è¿›è¡ŒåŠ é€Ÿï¼Œä¸è¿‡å€¼å¾—æ³¨æ„çš„æ˜¯**åœ¨å¼€å§‹ä½¿ç”¨è¿‡ç¨‹ä¸­ä¼šæ¯”è¾ƒæ…¢**ï¼Œå› ä¸ºåœ¨æ‰§è¡Œæ—¶ï¼Œå®ƒä¼šå°†æ¨¡å‹ç¼–è¯‘ä¸ºä¼˜åŒ–çš„å†…æ ¸ï¼Œæ‰€ä»¥ç›¸å¯¹ä¼šæ¯”è¾ƒæ…¢ï¼Œä½†æ˜¯å¦‚æœå¯¹ç¼–è¯‘åæ¨¡å‹è¿›è¡Œæ‰¹é‡æµ‹è¯•åœ¨æ—¶é—´ä¸Šå°±ä¼šæœ‰æ‰€æå‡æ¯”å¦‚è¯´åœ¨ä»£ç [df_acceralate.ipynb](code/Python/DFModelCode/DF_acceralate/df_acceralate.ipynb)ä¸­æµ‹è¯•ç»“æœä½¿ç”¨compileåœ¨z-imageä¸Šç”Ÿæˆ5å¼ å›¾ç‰‡è€—æ—¶ï¼š86.49sï¼ˆ**å¹³å‡ç”Ÿå›¾æ—¶é—´**4sï¼‰ä¸ä½¿ç”¨compileï¼š29.92ï¼ˆ**å¹³å‡ç”Ÿå›¾æ—¶é—´**5sï¼‰ï¼›3ã€ä½¿ç”¨`torch.channels_last`å»ä¼˜åŒ–æ•°æ®ç»“æ„ï¼ˆ[torchæ–‡æ¡£](https://docs.pytorch.org/tutorials/intermediate/memory_format_tutorial.html#performance-gains)ï¼‰ï¼šæœ€ä¸»è¦çš„ä¸€ç‚¹æ˜¯é€šè¿‡channel_lastè®© GPU åœ¨è®¡ç®—å·ç§¯ / attention æ—¶ï¼Œå†…å­˜è®¿é—®æ›´è¿ç»­ï¼Œæ¯”å¦‚è¯´ä¸€èˆ¬æ•°æ®çš„è¾“å…¥æ˜¯NCHWé‚£ä¹ˆåœ¨å†…å­˜è®¿é—®ä¸­æ ¼å¼æ˜¯ï¼š`N0C0H0W0, N0C0H0W1, ..., N0C0H1W0, ...`è¿™ä¸ªé‡Œé¢é€šé“Cå˜åŒ–æœ€æ…¢ï¼Œä½¿ç”¨channel_listæ•°æ®æ ¼å¼å˜ä¸ºNHWCåœ¨å†…å­˜ä¸­è®¿é—®é¡ºåºæ˜¯ï¼š`N0H0W0C0, N0H0W0C1, N0H0W0C2, ...`å€¼å¾—æ³¨æ„çš„æ˜¯ä¸¤éƒ¨åˆ†æ•°æ®åœ¨shapeä¸Šæ˜¯ä¸€è‡´çš„åªæ˜¯stridä¸ä¸€è‡´ã€‚ä½¿ç”¨æ–¹å¼ä¹Ÿæ¯”è¾ƒç®€å•ï¼š
```python
# ä¿®æ”¹æ¨¡å‹
model = model.to(memory_format=torch.channels_last)
# ä¿®æ”¹è¾“å…¥
input = input.to(memory_format=torch.channels_last)
output = model(input)
...
pipeline.unet.to(memory_format=torch.channels_last)
```
#### 1ã€xFormersåŠ é€Ÿ
> é¡¹ç›®åœ°å€ï¼š[https://github.com/facebookresearch/xformers](https://github.com/facebookresearch/xformers)

åœ¨SDæ¨¡å‹ä¸­å¯¹äºxformersåŸºæœ¬ä½¿ç”¨æ–¹å¼å¦‚ä¸‹æ‰€ç¤ºï¼š
```python
import torch
from diffusers import StableDiffusionXLPipeline

pipeline = StableDiffusionXLPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16,
).to("cuda")
# ä½¿ç”¨xformeråŠ é€Ÿ
pipeline.enable_xformers_memory_efficient_attention()
# å…³é—­xformeråŠ é€Ÿ
pipeline.disable_xformers_memory_efficient_attention()
```
xformersä½œç”¨åœ¨äº**åŠ é€Ÿattentionè®¡ç®—å¹¶é™ä½æ˜¾å­˜**ï¼Œé™¤æ­¤ä¹‹å¤–è¿˜æä¾›äº†å¤šç§æ³¨æ„åŠ›å®ç°æ–¹å¼ï¼Œå¦‚casual attentionç­‰ã€‚æ ¹æ®[å®˜æ–¹æ–‡æ¡£](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.fmha.cutlass.FwOp)ä¸­çš„æè¿°ï¼Œå¯¹äºå¯¹äº`xformers.ops.memory_efficient_attention`åœ¨ä½¿ç”¨ä¸Šå‚æ•°ä¸»è¦æ˜¯ï¼š1ã€è¾“å…¥æ•°æ®ä¹Ÿå°±æ˜¯QKVçš„æ ¼å¼ä¸Šå¿…é¡»æ»¡è¶³ä¸ºï¼š`[B, M, H, K]`åˆ†åˆ«è¡¨ç¤ºçš„æ˜¯å…¶ä¸­B ä¸ºbatch size, Nä¸ºåºåˆ—é•¿åº¦, num_headsä¸ºå¤šå¤´æ³¨æ„åŠ›å¤´çš„ä¸ªæ•°, dim_headåˆ™ä¸ºæ¯ä¸ªå¤´å¯¹åº”çš„embeding sizeï¼›2ã€attn_biaså®é™…ä¸Šå……å½“ä¸ºåœ¨ä½¿ç”¨mask attentionæ—¶çš„maskï¼›3ã€pä¹Ÿå°±æ˜¯dropoutå¯¹åº”å€¼ï¼›4ã€opä¸ºTupleï¼Œç”¨äºæŒ‡å®šä¼˜åŒ–self-attentionè®¡ç®—æ‰€é‡‡ç”¨çš„ç®—å­ã€‚åŸºæœ¬ä½¿ç”¨æ–¹å¼å¦‚ä¸‹ï¼š
```python
import xformers.ops as xops
y = xops.memory_efficient_attention(q, k, v)
y = xops.memory_efficient_attention(q, k, v, p=0.2) # ä½¿ç”¨dropout
y = xops.memory_efficient_attention(
    q, k, v,
    attn_bias=xops.LowerTriangularMask()
)# ä½¿ç”¨casual æ³¨æ„åŠ›
```
å€¼å¾—ç€é‡äº†è§£çš„å°±æ˜¯å…¶ä¸­`attn_bias`å‚æ•°ï¼Œç®€å•ç›´è§‚çš„ç†è§£ï¼šç”¨äºæ§åˆ¶æ³¨æ„åŠ›å¯è§æ€§å’Œç»“æ„çš„ç»Ÿä¸€æ¥å£ï¼Œ**æ—¢å¯ä»¥è¡¨ç¤º maskï¼Œä¹Ÿå¯ä»¥è¡¨ç¤ºç¨€ç–/å±€éƒ¨/å› æœç­‰é«˜çº§æ³¨æ„åŠ›æ¨¡å¼**ï¼Œå¹¶ä¸”ä»¥é«˜æ€§èƒ½æ–¹å¼èå…¥ attention å†…æ ¸ã€‚æ¯”å¦‚è¯´ï¼š
1ã€`xops.LowerTriangularMask()`ï¼šå¸¸è§„çš„causalæ³¨æ„åŠ›ä¹Ÿå°±æ˜¯ä¸‹ä¸‰è§’mask
2ã€`xops.LocalAttentionFromBottomRightMask`ï¼šå±€éƒ¨æ³¨æ„åŠ›ï¼Œæ¯ä¸ªtokenåªèƒ½çœ‹æœ€è¿‘çš„window_sizeä¸ªtoken
### cacheç­–ç•¥
cacheæŒ‡çš„æ˜¯ï¼š**ç¼“å­˜é€šè¿‡å­˜å‚¨å’Œé‡ç”¨ä¸åŒå±‚ï¼ˆä¾‹å¦‚æ³¨æ„åŠ›å±‚å’Œå‰é¦ˆå±‚ï¼‰çš„ä¸­é—´è¾“å‡ºæ¥åŠ é€Ÿæ¨ç†ï¼Œè€Œä¸æ˜¯åœ¨æ¯ä¸ªæ¨ç†æ­¥éª¤æ‰§è¡Œæ•´ä¸ªè®¡ç®—**ã€‚å®ƒä»¥æ›´å¤šå†…å­˜ä¸ºä»£ä»·æ˜¾ç€æé«˜äº†ç”Ÿæˆé€Ÿåº¦ï¼Œå¹¶ä¸”ä¸éœ€è¦é¢å¤–çš„è®­ç»ƒã€‚ä¸»è¦è¯¦ç»†ä»‹ç»ä¸¤ç§ï¼š1ã€DeepCacheï¼›2ã€FORAã€‚å¯¹äºæ›´åŠ å¤šçš„cacheç­–ç•¥å¯ä»¥çœ‹[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/711223667)ï¼Œ**æ¨èç›´æ¥ä½¿ç”¨**[CacheDit](#cachedit)æ¥è¿›è¡ŒåŠ é€Ÿã€‚
#### DeepCacheç­–ç•¥
> Paper:[https://arxiv.org/pdf/2312.00858](https://arxiv.org/pdf/2312.00858)
> Code:[https://link.zhihu.com/?target=https%3A//github.com/horseee/DeepCache](https://link.zhihu.com/?target=https%3A//github.com/horseee/DeepCache)

**ä¸»è¦é’ˆå¯¹UNetæ¶æ„**çš„Diffusionæ¨¡å‹è¿›è¡Œæ¨ç†åŠ é€Ÿã€‚DeepCache æ˜¯ä¸€ç§Training-freeçš„æ‰©æ•£æ¨¡å‹åŠ é€Ÿç®—æ³•ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯**åˆ©ç”¨æ‰©æ•£æ¨¡å‹åºåˆ—å»å™ªæ­¥éª¤ä¸­å›ºæœ‰çš„æ—¶é—´å†—ä½™æ¥å‡å°‘è®¡ç®—å¼€é”€**ã€‚
![](https://s2.loli.net/2026/01/13/7fSrYDnbHFLu6iG.png)
åŸºäº U-Net ç»“æ„ç‰¹æ€§ï¼Œå‘ç°ç›¸é‚»å»å™ªæ­¥éª¤çš„é«˜å±‚ç‰¹å¾å…·æœ‰æ˜¾è‘—æ—¶é—´ä¸€è‡´æ€§ï¼ˆAdjacent steps in the denoising process exhibit significant temporal similarity in high-level features.ï¼‰ï¼Œæ¯”å¦‚è¯´ä¸Šå›¾ä¸­ä½œè€…åœ¨æµ‹è¯•ä¸Šé‡‡ç”¨block $U_2$çš„ç‰¹å¾å’Œå…¶å®ƒæ‰€æœ‰çš„é‡‡æ ·æ­¥ä¹‹é—´ç›¸ä¼¼æ€§è®¡ç®—ï¼ˆå›¾bï¼‰ï¼Œå› æ­¤ç¼“å­˜è¿™äº›é«˜å±‚ç‰¹å¾å¹¶ä»…ä»¥ä½æˆæœ¬æ›´æ–°ä½å±‚ç‰¹å¾ï¼Œä»è€Œé¿å…é‡å¤è®¡ç®—ã€‚å…·ä½“æ–¹æ³•ä¸ºï¼š
![](https://s2.loli.net/2026/01/13/eXRHCFcdxLi2z7K.png)
æ¯”å¦‚è¯´åœ¨å®˜æ–¹çš„ä½¿ç”¨ä¸­æœ‰å‚æ•°ï¼š`helper.set_params(cache_interval=3,cache_branch_id=0,)`è¡¨ç¤ºæ˜¯æ¯3ä¸ªæ—¶é—´æ­¥è¿›è¡Œä¸€æ¬¡å®Œæˆforwardç„¶ååˆ·æ–°cacheï¼Œè€Œå…¶ä¸­å‚æ•°cache_branch_idå€¼å¾—æ˜¯ä¸€èˆ¬è€Œè¨€åœ¨UNetä¸­ä¼šå®šä¹‰`branch 0 â†’ early / down blocks`ç­‰å°±æ˜¯é€‰æ‹©å“ªäº›å±‚çš„è¾“å‡ºã€‚å…·ä½“è¿‡ç¨‹å¦‚ä¸‹ï¼št=1è¿›è¡Œè®¡ç®—ç¼“å­˜ï¼Œt=2,3éƒ½ç›´æ¥ä½¿ç”¨ç¼“å­˜ï¼Œt=4å®Œæ•´è®¡ç®—å¾—åˆ°ç¼“å­˜ã€‚
#### FORA
> Paper: [https://arxiv.org/pdf/2407.01425](https://arxiv.org/pdf/2407.01425)
> Code: [https://github.com/prathebaselva/FORA](https://github.com/prathebaselva/FORA)

**ä¸»è¦æ˜¯äº‰å¯¹Ditæ¶æ„**çš„Diffusionæ¨¡å‹è¿›è¡Œæ¨ç†åŠ é€Ÿã€‚åˆ©ç”¨ Diffusion Transformer æ‰©æ•£è¿‡ç¨‹çš„é‡å¤ç‰¹æ€§å®ç°äº†å¯ç”¨äºDiTçš„Training-freeçš„CacheåŠ é€Ÿç®—æ³•ã€‚
![](https://s2.loli.net/2026/01/13/UCOEAJDLZNHXFW5.png)
FORAçš„æ ¸å¿ƒåœ¨äºå‘ç°Ditåœ¨å»å™ªè¿‡ç¨‹ä¸­ï¼Œ**ç›¸é‚»æ—¶é—´æ­¥çš„Attnå’ŒMLPå±‚ç‰¹å¾å­˜åœ¨æ˜¾è‘—é‡å¤æ€§**ï¼ˆå¦‚ä¸Šå›¾æ‰€ç¤º:åœ¨layer0ã€9ã€18ã€27è¿™äº›å±‚ä»¥åŠ250æ­¥é‡‡æ ·ä¸­ï¼Œéšåé‡‡æ ·æ­¥çº¦å¾€åç‰¹å¾ä¹‹é—´ç›¸ä¼¼æ€§ä¹Ÿå°±è¶Šé«˜ã€‚ï¼‰ã€‚é€šè¿‡Cachingç‰¹å¾ï¼ŒFORA å°†è¿™äº›é‡å¤è®¡ç®—çš„ä¸­é—´ç‰¹å¾ä¿å­˜å¹¶åœ¨åç»­æ—¶é—´æ­¥ç›´æ¥å¤ç”¨ï¼Œé¿å…é€æ­¥é‡æ–°è®¡ç®—ã€‚
![](https://s2.loli.net/2026/01/13/dSp5Zy9zua3gjw4.png)
å…·ä½“è€Œè¨€ï¼Œæ¨¡å‹ä»¥å›ºå®šé—´éš” N é‡æ–°è®¡ç®—å¹¶ç¼“å­˜ç‰¹å¾ï¼šå½“æ—¶é—´æ­¥ t æ»¡è¶³ t mod N=0 æ—¶ï¼Œæ›´æ–°æ‰€æœ‰å±‚çš„ç¼“å­˜ï¼›åœ¨åç»­ N-1 æ­¥ä¸­ï¼Œç›´æ¥æ£€ç´¢cachedçš„ Attn å’Œ MLP ç‰¹å¾ï¼Œè·³è¿‡é‡å¤è®¡ç®—ã€‚è¿™ç§ç­–ç•¥åˆ©ç”¨äº† DiT æ¶æ„åœ¨é‚»è¿‘æ—¶é—´æ—¶é—´æ­¥çš„ç‰¹å¾ç›¸ä¼¼æ€§ï¼Œåœ¨ä¸ä¿®æ”¹DiTæ¨¡å‹ç»“æ„çš„å‰æä¸‹å®ç°åŠ é€Ÿã€‚ä¾‹å¦‚ï¼Œåœ¨ 250 æ­¥ DDIM é‡‡æ ·ä¸­ï¼Œå½“ N=3 æ—¶ï¼Œæ¨¡å‹ä»…éœ€åœ¨ç¬¬ 3ã€6ã€9... æ­¥é‡æ–°è®¡ç®—ç‰¹å¾ï¼Œå…¶ä½™æ­¥éª¤å¤ç”¨Cacheï¼Œä½¿è®¡ç®—é‡å‡å°‘çº¦ 2/3ã€‚å®éªŒè¡¨æ˜ï¼ŒFORAå¯¹åæœŸå»å™ªé˜¶æ®µçš„ç‰¹å¾ç›¸ä¼¼æ€§åˆ©ç”¨æ›´ä¸ºé«˜æ•ˆï¼Œæ­¤æ—¶ç‰¹å¾å˜åŒ–ç¼“æ…¢ï¼Œç¼“å­˜å¤ç”¨çš„æ€§ä»·æ¯”æœ€é«˜ã€‚
#### FBCache
> é¡¹ç›®åœ°å€ï¼š[https://github.com/chengzeyi/ParaAttention/blob/main/doc/fastest_flux.md](https://github.com/chengzeyi/ParaAttention/blob/main/doc/fastest_flux.md)

é€šè¿‡ç¼“å­˜å˜æ¢å™¨æ¨¡å‹ä¸­å˜æ¢å™¨å—çš„è¾“å‡ºï¼Œå¹¶åœ¨ä¸‹ä¸€æ­¥æ¨ç†ä¸­é‡æ–°ä½¿ç”¨å®ƒä»¬ï¼Œå¯ä»¥é™ä½è®¡ç®—æˆæœ¬ï¼ŒåŠ å¿«æ¨ç†é€Ÿåº¦ã€‚ç„¶è€Œï¼Œå¾ˆéš¾å†³å®šä½•æ—¶é‡æ–°ä½¿ç”¨ç¼“å­˜ä»¥ç¡®ä¿ç”Ÿæˆå›¾åƒçš„è´¨é‡ã€‚æœ€è¿‘ï¼ŒTeaCache æå‡ºï¼Œå¯ä»¥ä½¿ç”¨æ—¶é—´æ­¥åµŒå…¥æ¥è¿‘ä¼¼æ¨¡å‹è¾“å‡ºä¹‹é—´çš„å·®å¼‚ã€‚AdaCache ä¹Ÿè¡¨æ˜ï¼Œåœ¨å¤šä¸ªå›¾åƒå’Œè§†é¢‘ DiT åŸºçº¿ä¸­ï¼Œ**ç¼“å­˜å¯ä»¥åœ¨ä¸ç‰ºç‰²ç”Ÿæˆè´¨é‡çš„æƒ…å†µä¸‹æ˜¾è‘—æé«˜æ¨ç†é€Ÿåº¦**ã€‚ä¸è¿‡ï¼ŒTeaCache ä»ç„¶æœ‰ç‚¹å¤æ‚ï¼Œå› ä¸ºå®ƒéœ€è¦é‡æ–°ç¼©æ”¾ç­–ç•¥æ¥ç¡®ä¿ç¼“å­˜çš„å‡†ç¡®æ€§ã€‚åœ¨ ParaAttention ä¸­ï¼Œ**å‘ç°å¯ä»¥ç›´æ¥ä½¿ç”¨ç¬¬ä¸€ä¸ªtransformerè¾“å‡ºçš„æ®‹å·®æ¥è¿‘ä¼¼æ¨¡å‹è¾“å‡ºä¹‹é—´çš„å·®å¼‚ã€‚å½“å·®å€¼è¶³å¤Ÿå°æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥é‡å¤ä½¿ç”¨ä¹‹å‰æ¨ç†æ­¥éª¤çš„æ®‹å·®**ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å®é™…ä¸Šè·³è¿‡äº†å»å™ªæ­¥éª¤ã€‚æˆ‘ä»¬çš„å®éªŒè¯æ˜äº†è¿™ä¸€æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ FLUX.1-dev æ¨ç†ä¸Šå®ç°é«˜è¾¾ 1.5 å€çš„é€Ÿåº¦ï¼Œè€Œä¸”è´¨é‡éå¸¸å¥½[^1]ã€‚
ç®€å•æ¥è¯´å°±æ˜¯ä¸Šé¢æåˆ°çš„DeepCache/FORAåœ¨ä½¿ç”¨ä¸Šå¤ªç²—ç³™ç›´æ¥é€šè¿‡å›ºå®šæ—¶é—´æ­¥å»cacheç¼“å­˜è¿™æ ·å¿½è§†è¾“å‡ºå·®å¼‚çš„éå‡åŒ€æ€§ï¼Œå› æ­¤åç»­çš„TeaCacheå‘ç°æ¨¡å‹è¾“å…¥ä¸è¾“å‡ºçš„å¼ºç›¸å…³æ€§ï¼Œé€šè¿‡Timestep Emebddingï¼ˆè¾“å…¥ï¼‰æ¥ä¼°è®¡è¾“å‡ºå·®å¼‚ã€‚è€ŒåFBCacheåˆåšäº†æ–°çš„æ”¹è¿›ï¼š
![](https://s2.loli.net/2026/01/14/raG4jTspv1DAZzB.png)
åˆ©ç”¨residual cacheå®ç°äº†ä¸€ä¸ªåŸºäºFirst Block L1è¯¯å·®çš„Cacheæ–¹æ¡ˆï¼Œè¯¯å·®å°äºæŒ‡å®šé˜ˆå€¼ï¼Œå°±è·³è¿‡å½“å‰æ­¥è®¡ç®—ï¼Œå¤ç”¨residual cacheï¼Œå¯¹å½“å‰æ­¥çš„è¾“å‡ºè¿›è¡Œä¼°è®¡ã€‚
#### CacheDit
[cache-dit](https://github.com/vipshop/cache-dit)è¿™ä¸ªæ¡†æ¶ä¸»è¦æ˜¯é€‚ç”¨äºDitç»“æ„çš„æ‰©æ•£æ¨¡å‹ä½¿ç”¨ï¼Œå…¶å…·ä½“[æ¨¡å‹æ¡†æ¶](https://cache-dit.readthedocs.io/en/latest/user_guide/DBCACHE_DESIGN/)å¦‚ä¸‹ï¼š
![](https://s2.loli.net/2026/01/14/vw8AFh1cbpjdP2E.png)
å¯¹äºä¸Šè¿°æ¡†æ¶é¦–å…ˆäº†è§£CacheDitä¸­å‡ ä¸ªæ¦‚å¿µï¼š1ã€`Fn`ï¼šè¡¨ç¤ºéœ€è¦è®¡ç®—å‰nå±‚transformer blockåœ¨æ—¶é—´æ­¥tå¹¶ä¸”è¯¦ç»†è§£é‡Šä¸€ä¸‹CacheDitåŸç†ï¼›2ã€`Bn`:è¡¨ç¤ºè¿›ä¸€æ­¥çš„èåˆånå±‚transformer blockçš„ä¿¡æ¯å»å¼ºåŒ–é¢„æµ‹å‡†ç¡®æ€§ã€‚å…¶ä¸­n=1æ—¶å€™å°±æ˜¯FBCacheã€‚
å› æ­¤å¯¹äºCacheDitå…·ä½“è¿‡ç¨‹ä¸ºï¼š**åœ¨t-1æ­¥æ—¶å€™**ï¼Œå‰nå—blockå»è®¡ç®—ä»–ä»¬çš„ç»“æœå¾—åˆ°è¾“å‡ºç»“æœhidden stateå¹¶ä¸”å†™å…¥ç¼“å­˜ä¸­$C_{t-1}$ï¼Œè€Œååå‡ å±‚è¿›è¡Œå®Œæ•´ç»“ç®—ã€‚**åœ¨tæ­¥æ—¶å€™**ï¼Œå‰nå—blockä¸å®Œæ•´è®¡ç®—ï¼Œè€Œæ˜¯ç›´æ¥å¤ç”¨/è¿‘ä¼¼ t-1 æ­¥çš„ç¼“å­˜$C_{t-1}$å¾—åˆ°è¿‘ä¼¼çš„ç»“æœï¼Œè®¡ç®—è¿‘ä¼¼ç»“æœå’Œç¼“å­˜ç»“æœä¸­å·®å¼‚ï¼ˆL1 èŒƒæ•°ï¼‰ï¼Œå¦‚æœå·®å¼‚å°äºé˜ˆå€¼ç›´æ¥å¤ç”¨ç¼“å­˜è¾“å…¥åˆ°åç»­çš„å—ä¸­è®¡ç®—ï¼Œåä¹‹å°±é‡æ–°è®¡ç®—è¿™nå—ç»“æœã€‚
å…¶ä¸­å…·ä½“ä½¿ç”¨å¦‚ä¸‹ï¼š[df_acceralate.ipynb](code/Python/DFModelCode/DF_acceralate/df_acceralate.ipynb)
### é‡åŒ–æŠ€æœ¯æ¦‚è¿°
#TODO: 1ã€ggufs
[é‡åŒ–æŠ€æœ¯](https://www.big-yellow-j.top/posts/2025/10/11/Quantized.html)æ˜¯ä¸€ç§æ¨¡å‹å‹ç¼©çš„å¸¸è§æ–¹æ³•ï¼Œå°†æ¨¡å‹æƒé‡ä»é«˜ç²¾åº¦ï¼ˆå¦‚FP16æˆ–FP32ï¼‰é‡åŒ–ä¸ºä½æ¯”ç‰¹ä½ï¼ˆå¦‚INT8ã€INT4ï¼‰å»å®ç°**é™ä½æ˜¾å­˜+ç”ŸæˆåŠ é€Ÿ**ã€‚
> TODO: cpuå¸è½½

å¸¸è§çš„é‡åŒ–ç­–ç•¥å¯ä»¥åˆ†ä¸ºPTQå’ŒQATä¸¤å¤§ç±»ã€‚é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQuantization-Aware Trainingï¼‰ï¼šåœ¨**æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œé‡åŒ–**ï¼Œä¸€èˆ¬æ•ˆæœä¼šæ›´å¥½ä¸€äº›ï¼Œä½†éœ€è¦é¢å¤–è®­ç»ƒæ•°æ®å’Œå¤§é‡è®¡ç®—èµ„æºã€‚åé‡åŒ–ï¼ˆPost-Training Quantization, PTQï¼‰ï¼šåœ¨**æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œå¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–**ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚å¯¹äºçº¿æ€§é‡åŒ–ä¸‹ï¼Œæµ®ç‚¹æ•°ä¸å®šç‚¹æ•°ä¹‹é—´çš„è½¬æ¢å…¬å¼å¦‚ä¸‹ï¼š$Q=\frac{R}{S}+Z;R=(Q-Z)*S$ï¼Œå…¶ä¸­R è¡¨ç¤ºé‡åŒ–å‰çš„æµ®ç‚¹æ•°ã€Q è¡¨ç¤ºé‡åŒ–åçš„å®šç‚¹æ•°ã€Sï¼ˆScaleï¼‰è¡¨ç¤ºç¼©æ”¾å› å­çš„æ•°å€¼ã€Zï¼ˆZeroï¼‰è¡¨ç¤ºé›¶ç‚¹çš„æ•°å€¼ã€‚
æ¯”å¦‚è¯´åœ¨LLMä¸­å¸¸ç”¨çš„ä¸¤ç§**åé‡åŒ–æŠ€æœ¯**ï¼š1ã€**GPTQé‡åŒ–æŠ€æœ¯**ï¼šé€šè¿‡é‡åŒ–â€”â€”è¡¥å¿â€”â€”é‡åŒ–è¿­ä»£æ–¹æ³•ï¼Œé¦–å…ˆé‡åŒ–$W_{:,j}$ï¼Œè€Œåå»è®¡ç®—è¯¯å·®å¹¶ä¸”è¡¥å……åˆ° $W_{:,j:(i+B)}$è€Œåè¿›è¡Œè¿­ä»£å®ç°æ‰€æœ‰å‚æ•°çš„é‡åŒ–ï¼›2ã€**AWQé‡åŒ–æŠ€æœ¯**ï¼šæ¨¡å‹è®¡ç®—è¿‡ç¨‹ä¸­åªæœ‰å…³é”®å‚æ•°èµ·ä½œç”¨å› æ­¤å¯¹äºå…³é”®å‚æ•°ä¿æŒåŸæ¥çš„ç²¾åº¦(FP16)ï¼Œå¯¹å…¶ä»–æƒé‡è¿›è¡Œä½æ¯”ç‰¹é‡åŒ–ï¼Œä½†æ˜¯è¿™æ ·ä¸åŒè¿›åº¦å‚æ•°ä¼šå¯¼è‡´ç¡¬ä»¶é—®é¢˜ï¼Œå› æ­¤åœ¨AWQä¸­**å¯¹æ‰€æœ‰æƒé‡å‡è¿›è¡Œä½æ¯”ç‰¹é‡åŒ–ï¼Œä½†æ˜¯ï¼Œåœ¨é‡åŒ–æ—¶ï¼Œå¯¹äºæ˜¾è‘—æƒé‡ä¹˜ä»¥è¾ƒå¤§çš„scaleï¼Œç›¸å½“äºé™ä½å…¶é‡åŒ–è¯¯å·®ï¼›åŒæ—¶ï¼Œå¯¹äºéæ˜¾è‘—æƒé‡ï¼Œä¹˜ä»¥è¾ƒå°çš„scaleï¼Œç›¸å½“äºç»™äºˆæ›´å°‘çš„å…³æ³¨ã€‚**
#### Bitsandbytes é‡åŒ–
é€šè¿‡ä½¿ç”¨bitsandbytesé‡åŒ–æ¥å®ç°8-bitï¼ˆint8ï¼‰æˆ–è€…4-bitï¼ˆint4ã€Qloraä¸­ä¸€èˆ¬å°±ä¼šä½¿ç”¨ï¼‰é‡åŒ–ï¼Œä¸è¿‡åŒºåˆ«ä¸Šé¢æåˆ°çš„AWQä»¥åŠGPTQé‡åŒ–ï¼Œbitsandbyteså±äºé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼Œå‰è€…éœ€è¦é€šè¿‡æ•°æ®æ¥ä¿è¯é‡åŒ–ç²¾åº¦ï¼ˆé‡åŒ–è¿‡ç¨‹æ˜¯ç¦»çº¿ã€ä¸€æ¬¡æ€§è¿‡ç¨‹ï¼‰ï¼Œåè€…é‡åŒ–è¿‡ç¨‹æ˜¯å³æ—¶çš„å¯é€†çš„ã€‚å…¶æŠ€æœ¯åŸç†å¦‚ä¸‹ï¼š$wâ‰ˆs q$å…¶ä¸­wè¡¨ç¤ºåŸå§‹çš„FP16æƒé‡ï¼Œqä»£è¡¨int4/int8æƒé‡ï¼Œsç¼©æ”¾å› å­ï¼Œå…¶é‡åŒ–è¿‡ç¨‹ä¸ºå¯¹æ¯ä¸€ä¸ªblockæƒé‡è®¡ç®—ï¼š$\max(\text{abs}(w))$è€Œåå»è®¡ç®—scaleï¼š$s=\frac{amx(\| w\|)}{2^{b-1}-1}$è€Œåä»£å…¥å…¬å¼å°±å¯ä»¥å¾—åˆ°é‡åŒ–åæƒé‡ï¼Œæ¨ç†è¿‡ç¨‹ä¸­è¿›è¡Œï¼šåé‡åŒ– + çŸ©é˜µä¹˜æ³•èåˆåœ¨ä¸€ä¸ª CUDA kernel ä¸­å®Œæˆï¼š$Y=X(sq)$ã€‚å› æ­¤å¯¹äºå…¶ä½¿ç”¨ä¹Ÿå¾ˆç®€å•ï¼Œæ¯”å¦‚è¯´åœ¨ä»£ç ä¸­ï¼š[cache_acceralate.py](code/Python/DFModelCode/DF_acceralate/cache_acceralate.py)
```python
# åœ¨ZImagePipelineä¸­å‚æ•°ä¸ºï¼š
class ZImagePipeline(DiffusionPipeline, ZImageLoraLoaderMixin, FromSingleFileMixin):
    def __init__(,..,vae, text_encoder, tokenizr, transformer):
        ...
# å› æ­¤å¯ä»¥ç›´æ¥å¯¹é‡Œé¢çš„text_encoderä½¿ç”¨é‡åŒ–å¤„ç†

from diffusers import BitsAndBytesConfig as DiffusersBitsAndBytesConfig
quantization_config = DiffusersBitsAndBytesConfig(
    load_in_4bit=True,# åœ¨æ¨¡å‹åŠ è½½é˜¶æ®µï¼Œå°†æƒé‡ä»¥ 4-bit é‡åŒ–å½¢å¼åŠ è½½
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,# æŒ‡å®š åé‡åŒ–åå‚ä¸è®¡ç®—çš„ dtype
    bnb_4bit_use_double_quant=True,#å¯ç”¨ Double Quantizationï¼ˆåŒé‡é‡åŒ–ï¼‰ï¼Œä¹Ÿå°±æ˜¯å¯¹blockçš„scaleåœ¨è¿›è¡Œä¸€æ¬¡é‡åŒ–
    llm_int8_skip_modules=["transformer_blocks.0.img_mod"],# æŒ‡å®š ä¸å‚ä¸ bitsandbytes é‡åŒ–çš„æ¨¡å—
)
transformer = AutoModel.from_pretrained(
    model_name,
    cache_dir=cache_dir,
    subfolder="transformer",
    quantization_config=quantization_config,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    mirror='https://hf-mirror.com'
)
```
å»å¯¹ä½ çš„`model_name`é‡Œé¢çš„transformerè¿›è¡Œé‡åŒ–å¤„ç†ï¼Œé™¤æ­¤ä¹‹å¤–è¿˜æœ‰ä½¿ç”¨ä¾‹å­å°±æ˜¯è¿›è¡Œä¼˜åŒ–å™¨é‡åŒ–ï¼Œæ¯”å¦‚è¯´
```python
# å’Œä½¿ç”¨adamwæ–¹å¼ä¸€æ ·ï¼Œä½¿ç”¨qloraä½¿ç”¨ä¸€èˆ¬å¸¦ä¸Šè¿™ä¸ªä¼˜åŒ–å™¨
import bitsandbytes as bnb
optimizer_class = bnb.optim.AdamW8bit
```
#### SVDQuanté‡åŒ–
> https://github.com/nunchaku-ai/nunchaku

#TODO: é‡åŒ–ä¾¯æ¨¡å‹å¦‚ä½•è¿›è¡Œåè®­ç»ƒå¯ä»¥ç›´æ¥ä½¿ç”¨ flux1-dev-kontext_fp8_scaled.safetensors è¿›è¡Œä»‹ç»
<<<<<<< HEAD
## æ€»ç»“
æœ¬æ–‡ä¸»è¦æ˜¯ä»‹ç»ä¸€äº›åœ¨SDæ¨¡å‹ä¸­åŠ å¿«ç”Ÿå›¾çš„ç­–ç•¥ï¼Œ1ã€ç›´æ¥ä½¿ç”¨åŠ é€Ÿæ¡†æ¶è¿›è¡Œä¼˜åŒ–ï¼Œæ¯”å¦‚è¯´æŒ‡å®šattentionè®¡ç®—åç«¯æ–¹å¼ã€é€šè¿‡`torch.compile`è¿›è¡Œç¼–è¯‘ã€ä½¿ç”¨`torch.channels_last`å»ä¼˜åŒ–å†…å­˜è®¿é—®æ–¹å¼ç­‰ï¼›2ã€cacheç­–ç•¥ï¼Œå‘ç°åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­åœ¨æŸäº›å±‚/æ—¶é—´å¸ƒä¹‹é—´å›¾åƒçš„ç‰¹å¾æ¯”è¾ƒç›¸ä¼¼ï¼Œå› æ­¤å°±å¯ä»¥è€ƒè™‘å°†è¿™äº›è®¡ç®—ç»“æœè¿›è¡Œç¼“å­˜åœ¨åç»­næ­¥ä¸­ç›´æ¥åŠ è½½ç¼“å­˜å¥½çš„ç‰¹å¾æ¥å®ç°ç”ŸæˆåŠ é€Ÿï¼Œä¸»è¦ä»‹ç»æ¡†æ¶æ˜¯`cache-dit`ï¼›3ã€é‡åŒ–æŠ€æœ¯æ¦‚è¿°ï¼Œ
æœ€åç®€å•å¯¹æ¯”ä¸€ä¸‹ç”ŸæˆåŠ é€Ÿæ—¶é—´
> æµ‹è¯•prompt: `è¶…å†™å®äºšæ´²ä¸­å¹´ç”·æ€§ï¼Œå¹´é¾„çº¦45-55å²ã€‚é¢å®¹åšæ¯…ã€æ†”æ‚´ï¼Œå¸¦æœ‰ç”Ÿæ´»é˜…å†çš„ç—•è¿¹ï¼ˆå¦‚çœ¼è§’çš„ç»†çº¹ï¼‰ã€‚ä»–ç©¿ç€è´¨æ„ŸæŸ”è½¯çš„æ·±ç°è‰²é«˜é¢†æ¯›è¡£ï¼Œå¤–æ­ä¸€ä»¶ç»å…¸çš„å¡å…¶è‰²é£è¡£ï¼Œç«™åœ¨å¯’é£ä¸­å‘¨å›´æ˜¯é«˜æ¥¼å¤§å¦`

| æ­£å¸¸ç”Ÿå›¾ | +ä½¿ç”¨channel+ flash_attn| +ä½¿ç”¨cachedit |
|:--:|:--:|:--:|
|![](https://s2.loli.net/2026/01/14/DJYyBdQAEqK9hg2.png) |![](https://s2.loli.net/2026/01/14/z9NApexJEwfagqm.png)| ![](https://s2.loli.net/2026/01/14/3J1pKEb4GaMRlIe.png)|
| `5.97` | `5.67` | `5.48` |

## å‚è€ƒ
[^1]: [https://github.com/chengzeyi/ParaAttention/blob/main/doc/fastest_flux.md](https://github.com/chengzeyi/ParaAttention/blob/main/doc/fastest_flux.md)
=======
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTg3NDMyNDk4XX0=
-->
>>>>>>> aae1c479fcd9da4bf82dcac628ed8ffd45004776
