---
layout: mypost
title: DPOã€PPOã€GRPOä»£ç ä¹±è®°
categories: å¼ºåŒ–å­¦ä¹ 
address: æ­¦æ±‰ğŸ¯
extMath: true
show_footer_image: true
tags:
- å¼ºåŒ–å­¦ä¹ 
description: æœ¬æ–‡è¯¦ç»†ä»‹ç»DPOTrainerã€GRPOTrainerç­‰è®­ç»ƒå™¨çš„æ•°æ®å¤„ç†ã€æ¨¡å‹è®­ç»ƒåŠç»“æœåˆ†æã€‚DPOTraineréœ€å‡†å¤‡3å…ƒç»„æ•°æ®é›†ï¼Œé€šè¿‡_prepare_datasetå‡½æ•°ç¼–ç ç”Ÿæˆpromptã€chosen_input_idsç­‰ï¼Œæ ¸å¿ƒé€šè¿‡get_batch_loss_metricsåŠdpo_lossè®¡ç®—lossï¼Œå…³æ³¨å¥–åŠ±ä¿¡å·ï¼ˆrewards/chosenç­‰ï¼‰ã€å‡†ç¡®æ€§åŠå¯¹æ•°æ¦‚ç‡ï¼›GRPOTraineråˆ™ç»_prepare_inputså¤„ç†æ•°æ®ï¼Œè®¡ç®—old_per_token_logpsã€ref_per_token_logpsï¼ˆç”¨äºKLå€¼ï¼‰åŠadvantagesï¼Œè®­ç»ƒæ—¶ä½¿ç”¨æœ€æ–°æ¨¡å‹çš„_get_per_token_logps_and_entropiesè®¡ç®—lossï¼ŒåŠ©åŠ›æ¨¡å‹ä¼˜åŒ–ã€‚
---

## DPOTrainer
### æ•°æ®å¤„ç†
åœ¨ä½¿ç”¨DPOTrainerç¬¬ä¸€æ­¥å°±æ˜¯å‡†å¤‡æ•°æ®é›†ï¼Œå¯¹äºæ•°æ®é›†å‡†å¤‡è¿‡ç¨‹ä¸ºï¼Œæ”¶é›†3å…ƒç»„æ•°æ®é›†ï¼š
![](https://s2.loli.net/2025/11/12/SCpUYFAOeHfbs4R.webp)
> [trl-lib/ultrafeedback_binarized](https://huggingface.co/datasets/trl-lib/ultrafeedback_binarized/viewer?row=0&views%5B%5D=train)

ä¸€èˆ¬è€Œè¨€æ„å»ºçš„å…·ä½“çš„**æ•°æ®æ ¼å¼**å¦‚ä¸‹ï¼š
```python
[
    {
        "role": "system",
        "content": [{xxxxx}],
    },
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": image,
            },
            {
                "type": "text",
                "text": (xxxxxx)
            },
        ],
    },
]
```
åœ¨å¾—åˆ°æ•°æ®é›†ä¹‹åï¼Œåœ¨DPOTrainerå†…éƒ¨ä¼šç›´æ¥é€šè¿‡`_prepare_dataset`å‡½æ•°é€šè¿‡è¾“å…¥çš„`tokenizer`è¿›è¡Œç¼–ç å¤„ç†ï¼Œå¾—åˆ°ä¸»è¦å†…å®¹ä¸ºï¼š`'prompt', 'prompt_input_ids', 'chosen_input_ids', 'rejected_input_ids'`ã€‚å…¶ä¸­`prompt`ä¸»è¦å°±æ˜¯ä¸Šé¢æ•°æ®æ ¼å¼ä¸­çš„ userçš„content-textå†…å®¹ã€‚è€Œ`chosen`å’Œ`rejected`åˆ™æ˜¯åˆ†åˆ«å¯¹åº”`system`çš„å†…å®¹ã€‚é€šè¿‡tokenizerå°†æ‰€æœ‰çš„æ–‡æœ¬è¿›è¡Œç¼–ç å¤„ç†ï¼Œæ¥ä¸‹æ¥å°±æ˜¯æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ã€‚
### æ¨¡å‹è®­ç»ƒ
åœ¨`DPOTrainer`è®­ç»ƒä¸­ä¸»è¦æ˜¯ç»§æ‰¿`Trainer`ç±»ï¼Œå› æ­¤å¯¹äº`DPOTrainer`æ ¸å¿ƒéœ€è¦å…³æ³¨çš„ç‚¹å°±æ˜¯lossè®¡ç®—è¿‡ç¨‹ï¼Œåœ¨ä»£ç ä¸­ä¸»è¦æ˜¯é€šè¿‡å‡½æ•°`get_batch_loss_metrics`æ¥è®¡ç®—losså…·ä½“å€¼
```python
def get_batch_loss_metrics(
    self,
    model: PreTrainedModel | nn.Module,
    batch: dict[str, list | torch.LongTensor],
    train_eval: Literal["train", "eval"] = "train",
) -> tuple[torch.Tensor, dict[str, float]]:
    metrics = {}

    if self.args.use_liger_kernel:
        model_output = self._compute_loss_liger(model, batch)
        losses = model_output["loss"]
        chosen_rewards = model_output["chosen_rewards"]
        rejected_rewards = model_output["rejected_rewards"]
    else:
        model_output = self.concatenated_forward(model, batch)

        # if ref_chosen_logps and ref_rejected_logps in batch use them, otherwise use the reference model
        if "ref_chosen_logps" in batch and "ref_rejected_logps" in batch:
            ref_chosen_logps = batch["ref_chosen_logps"]
            ref_rejected_logps = batch["ref_rejected_logps"]
        else:
            ref_chosen_logps, ref_rejected_logps = self.compute_ref_log_probs(batch)

        # Initialize combined losses
        losses = 0
        chosen_rewards = 0
        rejected_rewards = 0

        # Compute losses for each loss type
        for idx, loss_type in enumerate(self.loss_type):
            # Compute individual loss using standard DPO loss function
            _losses, _chosen_rewards, _rejected_rewards = self.dpo_loss(
                model_output["chosen_logps"],
                model_output["rejected_logps"],
                ref_chosen_logps,
                ref_rejected_logps,
                loss_type,
                model_output,
            )

            # Add weighted contributions
            weight = self.loss_weights[idx] if self.loss_weights else 1.0
            losses = losses + _losses * weight
            chosen_rewards = chosen_rewards + _chosen_rewards * weight
            rejected_rewards = rejected_rewards + _rejected_rewards * weight

    reward_accuracies = (chosen_rewards > rejected_rewards).float()

    if self.args.rpo_alpha is not None:
        losses = losses + self.args.rpo_alpha * model_output["nll_loss"]  # RPO loss from V3 of the paper

    if self.use_weighting:
        losses = losses * model_output["policy_weights"]

    if self.aux_loss_enabled:
        losses = losses + self.aux_loss_coef * model_output["aux_loss"]

    ...
    return losses.mean(), metrics
```
çœç•¥æ‰æ‰€æœ‰çš„å¤„ç†è¿‡ç¨‹ç›´æ¥çœ‹æ¨¡å‹åˆ†åˆ«å¾—åˆ°ä»€ä¹ˆæ ·çš„ç»“æœï¼Œé¦–å…ˆåœ¨`self.concatenated_forward`ä¸­ä¸»è¦ä¹å››å°†æ•°æ®ä¸¢åˆ°modelä¸­`outputs = model(input_ids, **model_kwargs) logits = outputs.logits`å¾—åˆ°å¤„ç†çš„æ¯ä¸€ä¸ªtokençš„æ¦‚ç‡è€Œåé€šè¿‡`logits = logits[:, -seq_len:]`æ¥æˆªå–æ¨¡å‹å›ç­”çš„tokenæ¦‚ç‡ï¼Œå› æ­¤æœ€åè¿”å›ï¼š`model_output`ï¼Œæœ€åå¾—åˆ°çš„`model_output`ä¸»è¦æ˜¯ç”±å¦‚ä¸‹ç»„æˆ
```python
...
outputs = model(input_ids, **model_kwargs)
logits = outputs.logits             # [dim_1, dim_2, dim_3]
logits = logits[:, -seq_len:]       # [dim_1, dim_2, dim_4]
...
all_logps = per_token_logps[:, 1:].sum(-1) # å…¶ä¸­per_token_logpsç›´æ¥é€šè¿‡è®¡ç®— log softmaxè®¡ç®—å¾—åˆ°tokenæ¦‚ç‡å€¼
...
output = {
    'chosen_logps': log_probs[:B, L_p:L_p+L_c].mean(dim=-1),
    'rejected_logps': log_probs[B:, L_p:L_p+L_r].mean(dim=-1),
    'mean_chosen_logits': logits[:B, L_p:L_p+L_c, :].mean(),
    'mean_rejected_logits': logits[B:, L_p:L_p+L_r, :].mean(),
} # ä¸Šé¢ 4ä¸ªéƒ½æ˜¯å…·ä½“çš„æ•°å€¼ï¼Œå…¶ä¸­ chosen_logps > rejected_logps ä»£è¡¨ DPO æ­£åœ¨å­¦ä¹ ï¼
```
è€Œåå°±æ˜¯å…·ä½“è®¡ç®—lossè¿‡ç¨‹`self.dpo_loss`ï¼ˆè¿™éƒ¨åˆ†ä»£ç æ¯”è¾ƒç®€å•ï¼‰
### æ¨¡å‹ç»“æœ
æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡`report_to`å‚æ•°ç¡®å®šè®°å½•è®­ç»ƒè¿‡ç¨‹ï¼Œåœ¨DPOTrainerä¸­ä¸»è¦æ˜¯å¦‚ä¸‹å‡ ä¸ªç»“æœå€¼éœ€è¦å…³æ³¨ï¼š1ã€**å¥–åŠ±ä¿¡å·**ï¼š`rewards/chosen`ã€`rewards/rejected`ï¼Œ`rewards/margins`ï¼›2ã€**å‡†ç¡®æ€§**ï¼š`rewards/accuracies`ï¼ˆæ¨¡å‹æ˜¯å¦æ­£ç¡®åå¥½ chosenï¼‰ï¼›3ã€**å¯¹æ•°æ¦‚ç‡**ï¼š`logps/chosen`ï¼Œ`logps/rejected`ï¼ˆpolicy æ¨¡å‹å¯¹å“åº”çš„æ‰“åˆ†ï¼‰ï¼›4ã€å…¶ä»–æŒ‡æ ‡ï¼Œå¦‚`grad_norm`ï¼ˆè®­ç»ƒç¨³å®šæ€§å®šé‡è¯„ä¼°ï¼‰
## GRPOTrainer
### æ•°æ®å¤„ç†
åœ¨GRPOTrainerä¸­å¯¹äºæ•°æ®å¤„ç†è¿‡ç¨‹ä¼šç›´æ¥é€šè¿‡`_prepare_inputs`ï¼Œ`_generate_and_score_completions`æ¥å‡†å¤‡æ¨¡å‹æ‰€éœ€è¦çš„è¾“å…¥ï¼Œå…¶ä¸­æ¨¡å‹ä¸»è¦å¾—åˆ°çš„è¾“å…¥ï¼ˆè¿™éƒ¨åˆ†å…¶å®å°±å’Œæ™®é€šçš„SFTè¿‡ç¨‹æ˜¯ç›¸åŒçš„å°†æ•°æ®é€šè¿‡tokenizerè¿›è¡Œå¤„ç†ï¼‰ä¸ºæ¯”å¦‚è¯´æœ€åŸºæœ¬çš„å‡ ä¸ªå€¼ï¼š
```python
output = {
    "prompt_ids": prompt_ids,                # torch.Size([8, 256])
    "prompt_mask": prompt_mask,              # torch.Size([8, 256])
    "completion_ids": completion_ids,        # torch.Size([8, 68])
    "completion_mask": completion_mask,      # torch.Size([8, 68])
    "advantages": advantages,                # torch.Size([8])
    "num_items_in_batch": num_items_in_batch,
    "ref_per_token_logps": ref_per_token_logps # torch.Size([8, 68]) 
}
# å¯¹äºå¤šæ¨¡æ€æ•°æ® å°±ä¼šå¤šå‡ ä¸ªå‚æ•°å¦‚ pixel_valuesç­‰
```
å…¶ä¸­å¯¹äºoutputä¸­å„ä¸ªéƒ¨åˆ†ä»£è¡¨å«ä¹‰ï¼š`prompt_` ä¸»è¦æŒ‡çš„æ˜¯promptå†…å®¹ï¼Œ`completion_` ä¸»è¦æŒ‡çš„æ˜¯æ¨¡å‹çš„å›ç­”å†…å®¹ï¼ˆ**å€¼å¾—æ³¨æ„çš„æ˜¯è¿™ä¸¤é¡¹è¿˜éƒ½æ˜¯æˆ‘çš„æ•°æ®ä¸­å†…å®¹è€Œéæ¨¡å‹ç”Ÿæˆå†…å®¹**ï¼‰ã€‚é™¤æ­¤ä¹‹å¤–è¿˜æœ‰ä¸¤ä¸ªå€¼å¾—å…³æ³¨çš„ç»“æœï¼š1ã€`old_per_token_logps`ï¼šä¸»è¦é€šè¿‡ modelç”Ÿæˆï¼ˆè¿™ä¸ªmodelä¸»è¦æ˜¯â€**æ—§çš„æ²¡æœ‰æ›´æ–°**â€œçš„æ¨¡å‹ï¼‰ï¼›2ã€`ref_per_token_logps`ï¼ˆ`beta!=0`ï¼‰ä¸»è¦é€šè¿‡ ref_modelç”Ÿæˆã€‚ä¹‹æ‰€ä»¥éœ€è¦è®¡ç®—è¿™ä¸¤ä¸ªå€¼ **ä¸ºäº†è®¡ç®—KLå€¼**ã€‚
> éƒ½æ˜¯ç›´æ¥é€šè¿‡ `self._get_per_token_logps_and_entropies` è®¡ç®—å¾—åˆ°

é™¤æ­¤ä¹‹å¤–éœ€è¦å…³æ³¨çš„æ˜¯`advantages`çš„è®¡ç®—è¿‡ç¨‹ï¼š
```python
# é¦–å…ˆã€è®¡ç®—rewardå€¼
rewards_per_func = self._calculate_rewards(inputs, prompts, completions,completion_ids_list)
rewards = (rewards_per_func * self.reward_weights.to(device).unsqueeze(0)).nansum(dim=1) # å¦‚æœæœ‰å¤šä¸ª rewards è®¡ç®—å‡½æ•° é‚£ä¹ˆé€šè¿‡ä½¿ç”¨ self.reward_weights å»å¹³è¡¡ ä¸åŒè®¡ç®—å‡½æ•°
# è€Œåè®¡ç®— (r- mean)/std
mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1)
mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(self.num_generations, dim=0)
advantages = rewards - mean_grouped_rewards # r- mean
...
elif self.scale_rewards == "batch":
    std_rewards = rewards.std().expand_as(rewards)
advantages = advantages / (std_rewards + 1e-4) 
```
### æ¨¡å‹è®­ç»ƒ
åœ¨GRPOTrainerä¸­è®¡ç®—lossè¿‡ç¨‹ä¸ºï¼š
```python
def _compute_loss(self, model, inputs):
    prompt_ids, prompt_mask = inputs["prompt_ids"], inputs["prompt_mask"]
    completion_ids, completion_mask = inputs["completion_ids"], inputs["completion_mask"]
    input_ids = torch.cat([prompt_ids, completion_ids], dim=1)
    attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)
    logits_to_keep = completion_ids.size(1)  # we only need to compute the logits for the completion tokens

    # è®¡ç®—æ¨¡å‹è¾“å‡ºç»“æœ logits = model(**model_inputs).logits logps = selective_log_softmax(logits, completion_ids)
    per_token_logps, entropies = self._get_per_token_logps_and_entropies(model,input_ids,attention_mask,logits_to_keep,.....)
    ...
    # è®¡ç®—KLæ•£åº¦å€¼
    if self.beta != 0.0:
        ref_per_token_logps = inputs["ref_per_token_logps"]
        per_token_kl = (
            torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1
        )

    # Compute the loss
    advantages = inputs["advantages"]
    old_per_token_logps = inputs.get("old_per_token_logps")
    old_per_token_logps = per_token_logps.detach() if old_per_token_logps is None else old_per_token_logps

    log_ratio = per_token_logps - old_per_token_logps
    if self.importance_sampling_level == "token":
        log_importance_weights = log_ratio
    elif self.importance_sampling_level == "sequence":
        log_importance_weights = (log_ratio * completion_mask).sum(-1) / completion_mask.sum(-1).clamp(min=1.0)
        log_importance_weights = log_importance_weights.unsqueeze(-1)
    
    # From here, log_importance_weights (and all subsequent tensors, coef_1, coef_2, etc.) shape depends on
    # importance_sampling_level: "token" level: (B, T); "sequence" level: (B, 1)

    coef_1 = torch.exp(log_importance_weights)
    coef_2 = torch.clamp(coef_1, 1 - self.epsilon_low, 1 + self.epsilon_high)

    # Two-sided clipping
    if self.args.delta is not None:
        coef_1 = torch.clamp(coef_1, max=self.args.delta)

    per_token_loss1 = coef_1 * advantages.unsqueeze(1)
    per_token_loss2 = coef_2 * advantages.unsqueeze(1)
    per_token_loss = -torch.min(per_token_loss1, per_token_loss2)
    if entropy_mask is not None:
        per_token_loss = per_token_loss * entropy_mask

    if self.use_vllm and self.vllm_importance_sampling_correction:
        per_token_loss = per_token_loss * inputs["importance_sampling_ratio"]

    if self.beta != 0.0:
        per_token_loss = per_token_loss + self.beta * per_token_kl

    if self.loss_type == "grpo":
        loss = ((per_token_loss * completion_mask).sum(-1) / completion_mask.sum(-1).clamp(min=1.0)).mean()
        loss = loss / self.current_gradient_accumulation_steps
    ...
    # è®°å½•è®¡ç®—è¿‡ç¨‹ç»“æœ
    ...
    return loss
```
åœ¨è®¡ç®—lossè¿‡ç¨‹ä¸­éœ€è¦å†æ¬¡ä½¿ç”¨ `self._get_per_token_logps_and_entropies`ï¼ˆåœ¨æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­ä¼šåˆ†åˆ«ä½¿ç”¨ modelã€ref_modeléƒ½è¿›è¡Œè®¡ç®—ï¼‰ä¸è¿‡è¿™é‡Œä½¿ç”¨modelæ˜¯â€**æœ€æ–°çš„éœ€è¦æ›´æ–°**â€œçš„æ¨¡å‹ã€‚
### æ¨¡å‹ç»“æœ
> å‚è€ƒGPTå¯¹äºè¾“å‡ºç»“æœçš„è§£é‡Š

![](https://s2.loli.net/2025/11/12/1VCFJ3YXxaZIpBc.webp)
## PPOTrainer
## è‡ªå®šä¹‰Trainer