---
layout: mypost
title: 深入探讨Attention变种与内存优化-2：Lightning Attention
categories: Attention优化
extMath: true
images: true
address: changsha
show_footer_image: true
tags: [attention, lightning attention]
description: 本文主要深入探讨Attention变种与内存优化-2：Lightning Attention。其基本原理和操作
---

众所周知attention的算法复杂度是 $O(n^2)$，在前面的blog中有介绍可以通过flash-attention（简称fla）来提高算法的计算效率，但是使用fla带来的就是在IO上的消耗增加，因此

## 参考
1、[Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models](https://arxiv.org/pdf/2401.04658)