---
layout: mypost
title: LLM中的RLHF优化方法：GRPO、DPO与PPO解析
categories: 深度学习基础理论
extMath: true
images: true
address: changsha
show_footer_image: true
description: 本文详细解析了LLM训练过程及强化学习相关技术原理与实践。LLM训练主要分为预训练（使模型能“说人话”的自回归学习）和后训练，后者包含监督微调（SFT，通过专家数据让模型模仿推理）与人类反馈强化学习（RLHF，利用人类反馈训练奖励模型并引导模型对齐人类偏好）。文章还介绍了强化学习基本概念，如Agent（LLM）、State（模型状态）、Action（输出内容）、Reward
  Model（评分模型）、Policy（输出策略），并梳理了其发展历程，包括Policy Model优化、baseline、actor-critic、advantage
  actor critic等策略，以及TRPO、PPO、GRPO等算法的演进。重点讲解了DPO（直接偏好优化）通过人类偏好数据微调模型，PPO（近端策略优化）利用Clipping机制限制策略更新幅度以保证稳定性，GRPO（组相对偏好优化）作为DPO扩展处理组级别偏好优化，通过组内最优响应和KL项防止模型过度偏离初始策略。内容涵盖各技术核心公式、关键机制及应用场景，为LLM强化学习实践提供理论指导。
tags:
- GRPO
- DPO
- PPO
- LLM
- 强化学习
---

本文主要介绍几个强化学习中的几个重要模型，包括DPO、PPO、GRPO的主要算法原理。
## 简单回顾LLM训练过程
在[Blog](https://www.big-yellow-j.top/posts/2025/02/15/LLM.html)里面讨论过LLM框架这里简单讨论一下LLM训练过程，一般而言在LLM中训练主要分为如下几个阶段：**1、预训练阶段**：这部分简单理解就是让LLM能够说“人话”，自回归模型通过前一段文本然后预测下一个文本，并且让模型能够较好的“说话”（比如说：大语言模，下一个字可以正确输出 “型”）；**2、后训练阶段**：在得到一个能够说人话的模型之后，就需要让模型能够“思考”，这部分主要分为两部分：1、监督微调（**SFT** Supervised Training）；2、人类反馈强化学习（**RLHF** Reinforcement Learning from Human Feedback）。前者：顾名思义，我们首先使用监督学习方法，在少量高质量的专家推理数据上对 LLM 进行微调，例如指令跟踪、问题解答和/或思维链。希望在训练阶段结束时，模型已经学会如何模仿专家演示。后者：RLHF 利用人类反馈来训练奖励模型，然后通过 RL 引导 LLM 学习。这就使模型与人类的细微偏好保持一致
## DPO（Direct Preference Optimization）
[DPO](https://github.com/hkproj/dpo-notes/blob/main/DPO_Final.pdf)直接根据人类偏好数据对模型进行微调，使其生成更符合人类期望的输出，损失函数为：

$$\mathcal{L}_{\text{DPO}}(\pi_{\theta};\pi_{\text{ref}})=-\mathbb{E}_{(x,y_c,y_r)\sim\mathcal{D}}\left[\log\sigma\left(\beta\log\frac{\pi_{\theta}(y_c|x)}{\pi_{\text{ref}}(y_c|x)}-\beta\log\frac{\pi_{\theta}(y_r|x)}{\pi_{\text{ref}}(y_r|x)}\right)\right]$$

其中：
$\pi_{\theta}$：当前优化的语言模型（策略模型）。
$\pi_{\text{ref}}$：参考模型，通常是监督微调后的模型（SFT 模型），用于稳定训练。
$\sigma$：Sigmoid 函数，将偏好分数映射到 (0, 1)。
$\beta$：一个超参数，控制偏好强度的缩放（通常取值在 0.1 到 1 之间）。
$y_c,y_r$：优选和劣选回答。
$\mathcal{D}$：偏好数据集。

直观理解上面过程，DPO模型是一种纯粹数据驱动的训练范式，比如说下面例子：
![](https://s2.loli.net/2025/06/21/C2QjJtryViueONf.webp)
模型的优化目标就是需要让 **Bad answer**经可能的贴近我们的 **Good answer**其实从上面损失函数也可以发现。假设存在数据集$\mathcal{D}=(\text{Prompt}, \text{GoodAnswer}, \text{BadAnswer})$（简化为：$D=(x, y_c, y_r)$）；模型：$\pi_{\theta}$、$\pi_{\text{ref}}$。那么DPO处理过程为：
**第一步**、将输入数据通过tokenizer处理然后进行拼接得到两个输入队列： $[x,y_c]$ 以及 $[x,y_r]$
**第二步**、计算对数概率，直接将组合的数据传到上面定义的两个模型中也就是说：$out_c=\pi_{\theta}(x,y_c)$ 依次类推然后直接获取输出的概率：$out_c.logits$而后计算他们的 $softmax$不过在这里需要注意一点，输入是：$[x,y_c]$ 拼接起来的序列，但是我们需要的是 $y_c$ 部分词的概率！！，这样一来上面损失函数中所有需要计算的值都获取了。更加数学的表示：输入：$[x,y_c]$而后通过 $\pi_\theta$进行处理得到：$logits= R^{1\times (n+m)\times V}$（都是代表长度以及词汇表大小）但是我们需要符合$y_c$ 部分内容，因此需要筛选：$logits[0,n+t-1]$（要用n+t-1是因为llm大多为自回归$y_c$的第一个词由$x$ 最后一个词得到）
**第三步**、反向传播。
## PPO（Proximal Policy Optimization）
![](https://s2.loli.net/2025/06/21/9WSPbIR7rmjTsoe.webp)
PPO是一种基于策略梯度的强化学习算法，核心思想是通过**限制策略更新的幅度**来保持训练的稳定性。其目标函数（通过KL散度处理）为：

$$
L^{CLIP}(\theta)=\hat{\mathbb{E}}_{t}\left[\min(r_{t}(\theta)\hat{A}_{t},\operatorname{clip}(r_{t}(\theta),1-\epsilon,1+\epsilon)\hat{A}_{t})\right]
$$

$$
r_t(\theta)=\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
$$


$\pi_\theta$: 当前策略参数化的策略函数
$A_t$: 优势函数，衡量动作$a_t$相对于平均水平的优势
$\epsilon$: 超参数（通常0.1-0.2），限制策略更新的最大幅度
​**Clipping机制**：通过截断重要性采样比率，防止策略更新过大导致训练不稳定

对于上述公式里面优势函数$A_t$（用来衡量的是某个动作相对于平均水平的优势，也就是说，这个动作比平均情况好多少）具体计算公式为：$A_t=Q(s_t, a_t)-V(s_t)$，分别表示：1、$Q(s_t, a_t)$：在状态$s_t$下执行动作$a_t$得到的期望汇报；2、$V(s_t)$：状态$s_t$的平均累计期望。对于其计算可以通过GAE（广义优势估计）来进行计算。
![](https://s2.loli.net/2025/09/05/AvLeinFOo5lPV6z.webp)
仅从上面流程图（trl中的训练策略）可以大致看出PPO中模型的训练优化过程。**首先**对于我们的输入$q=[x_1,...,x_n]$ 对于 policy model（也就是我们强化学习需要优化的模型 或者称之为：actor model）会将所有内容输出我们的结果：$o=[y_1,...,y_n]$。 **而后**将我们的输出和回答进行拼接，接下来通过reward model，它需要做的就是对于我们全部的输出进行“评分”得到 *scores*。 

内部逻辑就是（RewardModel(o)而后通过一层线性映射出最后评分而后用 **最后一个词代表输出的效果如何** ）
## GRPO（Group Relative Preference Optimization）
GRPO是DPO的扩展形式，处理**组级别**的偏好优化问题，其核心公式：
![](https://s2.loli.net/2025/06/21/vN4YOEhCwpo5Mys.webp)
$r^*$: 组内最优响应
$\mathcal{R}$: 包含k个响应的候选集
KL项：防止模型过度偏离初始策略，缓解模式坍塌
除去上述复杂公式直接通过下面PPO/GRPO过程进行理解
![](https://s2.loli.net/2025/06/21/9WSPbIR7rmjTsoe.webp)
> 引用论文（https://arxiv.org/pdf/2402.03300） 中对于PPO和GRPO的对比分析

上图中几个比较关键词：1、**Policy Model**：即我们需要通过强化学习优化的模型；2、**Reward Model**：奖励模型，即对模型做出的决策所给出的反馈（分类打分的）；3、**Value Model**：估计状态的价值，帮助指导策略优化（分类打分的）；4、**Reference Model**：提供历史策略的参考，确保优化过程中策略变化不过度。
> 假设为优化Llama（假设参数为1B）模型，那么上述4个模型分别代表（结合hugging face训练过程中使用解释,参考如下代码）：
> 1、**Policy Model**：Llama模型本身；2、**Reward Model**：通常是一个更加强的模型（比如说Qwen2.5-13B）;3、**Value Model**以及 **Reference Model**：可以直接使用 Llama本身。

```python
value_model = AutoModelForSequenceClassification.from_pretrained(
    training_args.reward_model_path, trust_remote_code=model_args.trust_remote_code, num_labels=1
)
reward_model = AutoModelForSequenceClassification.from_pretrained(
    training_args.reward_model_path, trust_remote_code=model_args.trust_remote_code, num_labels=1
)
policy = AutoModelForCausalLM.from_pretrained(
    training_args.sft_model_path, trust_remote_code=model_args.trust_remote_code
)

peft_config = get_peft_config(model_args)
if peft_config is None:
    ref_policy = AutoModelForCausalLM.from_pretrained(
        training_args.sft_model_path, trust_remote_code=model_args.trust_remote_code
    )
else:
    ref_policy = None
trainer = PPOTrainer(
        args=training_args,
        processing_class=tokenizer,
        model=policy,
        ref_model=ref_policy,
        reward_model=reward_model,
        value_model=value_model,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        peft_config=peft_config,
    )
Code From:Code:https://github.com/shibing624/MedicalGPT/blob/main/ppo_training.py
```

那么在PPO/GRPO整体优化过程中的处理思路为：
**首先**，通过最开始的模型输出我们需要的“答案”也就是公式中的$\pi_{\theta_{old}}(o_i|q)$，而PPO和GRPO区别就在于输出的结果数量上的差异，而上述公式中所提到的 **优势函数**（$A_t$）则是对于模型输出结果的评分，在PPO中知道计算过程为：$A_t=Q(s_t, a_t)-V(s_t)$而在GRPO中处理的方式为：$A_i=\frac{r_i- \text{mean}(r_1,...,r_n)}{\text{std}(r_1,...,r_n)}$（其中的$r$代表对每一个回答给出的评分），GRPO的优势函数很好理解，对于PPO计算公式理解：$Q(s_t, a_t)$就是LLM在当前状态下对输出所给出的评分和GRPO中相似，$V(s_t)$则是表示未来所有可能生成的 token 序列的加权回报期望值。

> 如何理解$V(s_t)$呢？
> 通过PPO训练LLM时候，会通过 **Pilicy Head**以及 **Value Head**来进行调节，第一个很好理解就是用来输出下一个词，第二个就是用来负责评估当前生成状态的价值 $V(s_t)$

**而后**，通过计算KL散度来保证更新模型“偏离”不会太远，通过计算优化模型与Reference Model之间KL散度来进行限制。

## 参考
1、https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/
2、https://yugeten.github.io/posts/2025/01/ppogrpo/
3、https://arxiv.org/pdf/2501.12948
4、https://arxiv.org/pdf/2402.03300
5、https://arxiv.org/pdf/2305.18290
6、https://github.com/hkproj/dpo-notes/blob/main/DPO_Final.pdf
7、[李宏毅-介绍RL课程](https://www.youtube.com/watch?v=W8XF3ME8G2I)
8、[李宏毅-RL系列课程](https://www.youtube.com/playlist?list=PLJV_el3uVTsODxQFgzMzPLa16h6B8kWM_)
9、https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow
10、https://youtu.be/JZZgBu8MV4Q?si=Tr7QC6srxkZJPdsI
11、TRPO：https://arxiv.org/pdf/1502.05477
12、PPO：https://arxiv.org/pdf/1707.06347