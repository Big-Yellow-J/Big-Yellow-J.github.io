---
layout: mypost
title: LLM大杀器GRPO/PPO
categories: paper
extMath: true
images: true
address: changsha
show_footer_image: true
description: 在之前blog中简单介绍了一下DeepSeek-R1可以不使用任何监督数据让模型拥有推理能力，其使用的GRPO技术这里再次具体理解一下他的具体原理，以及代码实践操作
tags: [GRPO, PPO, LLM]
---

在之前blog中简单介绍了一下[DeepSeek-R1](https://arxiv.org/pdf/2501.12948)可以不使用任何监督数据让模型拥有推理能力，其使用的[GRPO](https://www.big-yellow-j.top/posts/2025/02/15/LLM.html#:~:text=%E6%8C%87%E6%A0%87%EF%BC%89%E6%83%A9%E7%BD%9A%E5%81%8F%E5%B7%AE-,PPO%E5%92%8C%20GRPO,-%E4%B8%8A%E9%9D%A2%E6%8F%90%E5%88%B0%E7%9A%84)技术这里再次具体理解一下他的具体原理，以及代码实践操作。对于GRPO参考论文（DeepSeek）：https://arxiv.org/pdf/2402.03300；对于PPO参考论文（OpenAI）：https://arxiv.org/pdf/1707.06347

## 简单回顾LLM训练过程

在[Blog](https://www.big-yellow-j.top/posts/2025/02/15/LLM.html)里面讨论过LLM框架这里简单讨论一下LLM训练过程，一般而言在LLM中训练主要分为如下几个阶段：

* 1、预训练（**Pre-Training**）

这部分简单理解就是让LLM能够说“人话”，自回归模型通过前一段文本然后预测下一个文本，并且让模型能够较好的“说话”（比如说：大语言模，下一个字可以正确输出 “型”）

* 2、后训练（**Post-Training**）

在得到一个能够说人话的模型之后，就需要让模型能够“思考”，这部分主要分为两部分：1、监督微调（**SFT** Supervised Training）；2、人类反馈强化学习（**RLHF** Reinforcement Learning from Human Feedback）。前者：顾名思义，我们首先使用监督学习方法，在少量高质量的专家推理数据上对 LLM 进行微调，例如指令跟踪、问题解答和/或思维链。希望在训练阶段结束时，模型已经学会如何模仿专家演示。后者：RLHF 利用人类反馈来训练奖励模型，然后通过 RL 引导 LLM 学习。这就使模型与人类的细微偏好保持一致

## 简单介绍Reinforcement Learning

了解LLM训练过程之后，在介绍DPO/PPO之前简单了解一下简单了解一下奖励模型（**Reward Model**），还是以DS-R1的为例，如何让他产出高质量/正确的思维链，一个很简单（废人）过程就是直接让人生成很多思维过程（比如比较 7.1和7.11大小，应该先怎么样，再怎么样）但是这样就会有问题：**人不可能将所有问题都写出一个思维链！**那就有一个更加直接办法：**直接让训练好的LLM自己生成思维链**。参考[Blog](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/)的解释对于一个奖励模型$R_{\phi}$定义一个这样的优化过程：

$$
\mathrm{L}(\phi)=-log \sigma(R_{\phi}(p, r_i)- R_{\phi}(p, r_j))
$$

其中$p$代表输入问题，$r$代表LLM输出的的结果。其中假设$r_i$的结果优于$r_j$，那么优化过程就是让模型输出内容更加的“贴合”$r_i$

![](https://s2.loli.net/2025/03/25/FkLG7VDmEpg4hKS.png)

引用论文（https://arxiv.org/pdf/2402.03300）中对于PPO和GRPO的对比分析

https://www.jianshu.com/p/9f113adc0c50

## 简单介绍RL

1、[介绍RL](https://www.youtube.com/watch?v=W8XF3ME8G2I)
2、[RL系列](https://www.youtube.com/playlist?list=PLJV_el3uVTsODxQFgzMzPLa16h6B8kWM_)
3、https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow


## PPO优化

## 参考
1、https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/
2、https://yugeten.github.io/posts/2025/01/ppogrpo/
3、https://arxiv.org/pdf/2501.12948
4、https://arxiv.org/pdf/2402.03300