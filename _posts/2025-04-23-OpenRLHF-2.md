---
layout: mypost
title: 强化学习框架：OpenRLHF源码解读，模型训练
categories: OpenRLHF框架解读
address: changsha
extMath: true
show_footer_image: true
description: 强化学习框架：OpenRLHF源码解读，模型训练模块解读
---

前文已经介绍了：
* [**强化学习框架：OpenRLHF源码解读，模型处理模块解读**](https://www.big-yellow-j.top/posts/2025/04/22/OpenRLHF-1.html)

本文主要介绍 **强化学习框架：OpenRLHF源码解读，模型**。因为RL由DPO、GRPO、PPO等几种类别，因此本文住哟介绍PPO范式训练。在OpenRLHF训练框架中，主要还会应用到DeepSpeed以及vLLM，因此在介绍PPO训练之前需要回顾一下：**1、DeepSpeed的配置**；**2、vLLM配置**。
> 在之前Blog已经对[DeepSpeed](https://www.big-yellow-j.top/posts/2025/02/24/deepspeed.html)以及[vLLM](https://www.big-yellow-j.top/posts/2025/02/17/Attention.html)原理进行了解释，因此只需要介绍在OpenRLHF如何去对这两部分进行配置

参数参考脚本：https://github.com/OpenRLHF/OpenRLHF/blob/main/examples/scripts/train_ppo_llama_ray.sh 中的设置
* **1、vLLM配置**

> From:https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/trainer/ray/vllm_engine.py

```python
def create_vllm_engines(
    num_engines: int, # 推理引擎数量
    tensor_parallel_size: int, # 张量并行大小
    pretrain: str,
    seed: int,
    full_determinism: bool,
    enable_prefix_caching: bool,
    enforce_eager: bool,
    max_model_len: int,
    num_total_actors: int,
    shared_pg=None,
    gpu_memory_utilization=None,
    vllm_enable_sleep=False,):
    ...
    # 1、资源调度配置。配置参数设置为：num_engines= tensor_parallel_size= 2
    use_hybrid_engine = shared_pg is not None
    num_gpus = int(tensor_parallel_size == 1)
    if use_hybrid_engine and tensor_parallel_size == 1:
        num_gpus = 0.2

    if not use_hybrid_engine:
        bundles = [{"GPU": 1, "CPU": 1} for _ in range(num_engines * tensor_parallel_size)]
        shared_pg = placement_group(bundles, strategy="PACK")
        ray.get(shared_pg.ready())
    ...
    # 2、构建每一个vLLM（=2）
    for i in range(num_engins):
        ...
        scheduling_strategy = PlacementGroupSchedulingStrategy(...) # 调度策略
        ...
        vllm_engines.append(
            LLMRayActor.options(
            num_cpus=num_gpus,
            num_gpus=num_gpus,
            scheduling_strategy=scheduling_strategy,
        ).remote(...)
        )
```

1、**资源调度配置**：第一种Hybrid模式（多个引擎共同占用GPU）；第二种标准模式（每个引擎都单独占用一个GPU和CPU）；
2、**构建vLLM**：首先是建立资源调度策略，以及使用vLLM。有必要了解一下就是在OpenRLHF中使用的是 [**ray**](https://www.ray.io/) 分布式架构进行训练。简单了解一下在这个里面他是怎么做的。通过 ray封装了一个vLLM推理架构（过程和python中多进程很相像）

> **补充-1**：ray简单使用，代码：[🔗](../code/ray_test.py.txt)
> Ray核心概念：1、任务（Task）：无状态的并行函数调用。
2、Actor：有状态的计算单元，适合需要持久状态的场景（如模型推理）。3、远程调用（remote）：通过 .remote() 异步调度任务或 Actor 方法。比如说上面代码，初始化我需要的节点 
```python
@ray.remote
class PrintActor:
```
> 在vLLM中可能就需要对资源进行分配：`PrintActor.options(...).remote(...)`其中 `remote`就是每个“进程”需要输出的任务参数，而 `options` 则是资源分配策略，比如GPU（`num_gpus`）/CPU（`num_cpus`）数量。在后面获取进程结果可以直接通过：
```python
ray.init()
print_engines = create_print_engines(4)
results = [engine.execture_print.remote(i) for i, engine in enumerate(print_engines)]
print(ray.get(results))
```

## PPO训练范式

完全了解PPO训练范式之前需要了解一下在OpenRLHF中如何定义 PPO训练器的。

* 1、`ActorPPOTrainer`



### train.sh

模型训练脚本：[🔗](https://github.com/OpenRLHF/OpenRLHF/blob/main/examples/scripts/train_ppo_llama_ray.sh) 脚本中主要涉及到参数：1、模型脚本：`openrlhf.cli.train_ppo_ray`；2、

### `train_ppo_ray.py`
> From: https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/cli/train_ppo_ray.py

前面已经介绍了在OpenRLHF中是如何将Ray和vLLM进行结合了，直接回顾 `train_ppo_ray.py`代码：

```python
def train(...):
    ...
    # 1、创建vLLM
    vllm_engines = None
    if ...:
        vllm_engines = create_vllm_engines(...)
```
第一步、创建vLLM，上面已经介绍不做赘述
第二步、


## 代码测试


## 总结
