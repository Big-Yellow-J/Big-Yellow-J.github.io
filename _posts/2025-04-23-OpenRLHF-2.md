---
layout: mypost
title: å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼šOpenRLHFæºç è§£è¯»ï¼Œæ¨¡å‹è®­ç»ƒ
categories: OpenRLHFæ¡†æ¶è§£è¯»
address: changsha
extMath: true
show_footer_image: true
description: å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼šOpenRLHFæºç è§£è¯»ï¼Œæ¨¡å‹è®­ç»ƒæ¨¡å—è§£è¯»
---

å‰æ–‡å·²ç»ä»‹ç»äº†ï¼š
* [**å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼šOpenRLHFæºç è§£è¯»ï¼Œæ¨¡å‹å¤„ç†æ¨¡å—è§£è¯»**](https://www.big-yellow-j.top/posts/2025/04/22/OpenRLHF-1.html)

æœ¬æ–‡ä¸»è¦ä»‹ç» **å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼šOpenRLHFæºç è§£è¯»ï¼Œæ¨¡å‹**ã€‚å› ä¸ºRLç”±DPOã€GRPOã€PPOç­‰å‡ ç§ç±»åˆ«ï¼Œå› æ­¤æœ¬æ–‡ä½å“Ÿä»‹ç»PPOèŒƒå¼è®­ç»ƒã€‚åœ¨OpenRLHFè®­ç»ƒæ¡†æ¶ä¸­ï¼Œä¸»è¦è¿˜ä¼šåº”ç”¨åˆ°DeepSpeedä»¥åŠvLLMï¼Œå› æ­¤åœ¨ä»‹ç»PPOè®­ç»ƒä¹‹å‰éœ€è¦å›é¡¾ä¸€ä¸‹ï¼š1ã€DeepSpeedçš„é…ç½®ï¼›2ã€vLLMé…ç½®ã€‚
> åœ¨ä¹‹å‰Blogå·²ç»å¯¹DeepSpeedä»¥åŠvLLMåŸç†è¿›è¡Œäº†è§£é‡Šï¼Œå› æ­¤åªéœ€è¦ä»‹ç»åœ¨OpenRLHFå¦‚ä½•å»å¯¹è¿™ä¸¤éƒ¨åˆ†è¿›è¡Œé…ç½®

å‚æ•°å‚è€ƒè„šæœ¬ï¼šhttps://github.com/OpenRLHF/OpenRLHF/blob/main/examples/scripts/train_ppo_llama_ray.sh ä¸­çš„è®¾ç½®
* 1ã€vLLMé…ç½®

> From:https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/trainer/ray/vllm_engine.py

```python
def create_vllm_engines(
    num_engines: int, # æ¨ç†å¼•æ“æ•°é‡
    tensor_parallel_size: int, # å¼ é‡å¹¶è¡Œå¤§å°
    pretrain: str,
    seed: int,
    full_determinism: bool,
    enable_prefix_caching: bool,
    enforce_eager: bool,
    max_model_len: int,
    num_total_actors: int,
    shared_pg=None,
    gpu_memory_utilization=None,
    vllm_enable_sleep=False,):
    ...
    # 1ã€èµ„æºè°ƒåº¦é…ç½®ã€‚é…ç½®å‚æ•°è®¾ç½®ä¸ºï¼šnum_engines= tensor_parallel_size= 2
    use_hybrid_engine = shared_pg is not None
    num_gpus = int(tensor_parallel_size == 1)
    if use_hybrid_engine and tensor_parallel_size == 1:
        num_gpus = 0.2

    if not use_hybrid_engine:
        bundles = [{"GPU": 1, "CPU": 1} for _ in range(num_engines * tensor_parallel_size)]
        shared_pg = placement_group(bundles, strategy="PACK")
        ray.get(shared_pg.ready())
    ...
    # 2ã€æ„å»ºæ¯ä¸€ä¸ªvLLMï¼ˆ=2ï¼‰
    for i in range(num_engins):
        ...
        scheduling_strategy = PlacementGroupSchedulingStrategy(...) # è°ƒåº¦ç­–ç•¥
        ...
        vllm_engines.append(
            LLMRayActor.options(
            num_cpus=num_gpus,
            num_gpus=num_gpus,
            scheduling_strategy=scheduling_strategy,
        ).remote(...)
        )
```

1ã€**èµ„æºè°ƒåº¦é…ç½®**ï¼šç¬¬ä¸€ç§Hybridæ¨¡å¼ï¼ˆå¤šä¸ªå¼•æ“å…±åŒå ç”¨GPUï¼‰ï¼›ç¬¬äºŒç§æ ‡å‡†æ¨¡å¼ï¼ˆæ¯ä¸ªå¼•æ“éƒ½å•ç‹¬å ç”¨ä¸€ä¸ªGPUå’ŒCPUï¼‰ï¼›
2ã€**æ„å»ºvLLM**ï¼š

## 1ã€vLLMé…ç½®


## PPOè®­ç»ƒèŒƒå¼

### train.sh

æ¨¡å‹è®­ç»ƒè„šæœ¬ï¼š[ğŸ”—](https://github.com/OpenRLHF/OpenRLHF/blob/main/examples/scripts/train_ppo_llama_ray.sh) è„šæœ¬ä¸­ä¸»è¦æ¶‰åŠåˆ°å‚æ•°ï¼š1ã€æ¨¡å‹è„šæœ¬ï¼š`openrlhf.cli.train_ppo_ray`ï¼›2ã€

### `train_ppo_ray.py`
> From: https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/cli/train_ppo_ray.py

å› ä¸ºè¦å®ç°åˆ†å¸ƒå¼è®­ç»ƒ

## ä»£ç æµ‹è¯•


## æ€»ç»“
