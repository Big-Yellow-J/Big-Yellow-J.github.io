---
layout: mypost
title: 深度学习基础理论————各类LLM模型技术汇总
categories: 深度学习基础理论
extMath: true
images: true
address: changsha
show_footer_image: true
---

## 各类LLM模型技术汇总

![](https://s2.loli.net/2025/02/15/HZiC6RpbqnkNVBm.png)

只去对比整体框架，对所采用的激活函数，归一化处理，位置编码等参考：
**1、位置编码**：https://www.big-yellow-j.top/posts/2025/02/03/pos-embedding.html
**2、归一化处理**：https://www.big-yellow-j.top/posts/2025/01/05/dl-norm.html
**3、分布式训练**：https://www.big-yellow-j.top/posts/2025/01/03/DistributeTraining.html

## `GPT`

### 1.`GPT v1`

对于大部分的深度学习任务，需要大量的**标记数据**（labeled data），但是如果使用大量的标记数据就会导致一个问题：构建得到的模型缺少适用性（可以理解为模型的泛化性能可能不佳）。那么就尝试使用**非标记的数据**（unlabelled data）但是这样一来又会有一个新的问题：时间消费大（time-consuming and expensive）。所以目前学者提出：使用预训练的词嵌入来提高任务性能。使用 *未标注的文本信息*（word-level information from unlabelled text）可能会：1、不清楚那种**优化目标**（optimization objective）在学习对迁移有用的文本表示时最有效；2、如何将这些学习到的表征有效的迁移到**目标任务**（target task）中。
作者提出：1、**无监督的预训练**（unsupervised pre-training）；2、**监督的微调**（supervised fine-tuning）

> 1、**Unsupervised pre-training**

给定一些列的的 **无标签**的 ``token``：$U=\{u_1,...,u_n\}$，构建自回归的模型：$L_1(U)= \sum_{i}logP(u_i|u_{i-k},...,u_{i-1}; \theta)$，其中$\theta$为模型的参数。作者在模型中使用 ``Transformer``作为 ``decoder``，在最后的模型上作者构建得到为：

$$
h_0= UW_e+W_p \\
h_l = transformer\_block(h_{l-1})\forall i \in [1,n]\\
P(u)=softmax(h_nW_e^T)
$$

其中$n$代表神经网路层的数目，$W_e$代表 ``token embedding matrix``，$W_p$代表 ``position embedding matrix``。对于无监督下的预训练：通过构建的数据集，去对模型的参数进行训练，得到模型的参数。

> 2、**Supervised fine-tunning**

作者在此部分提到：通过第一步得到的模型参数去对监督任务进行训练（采用的模型结构是没有变化的）。给定标签数据集$C$，给定输入：$\{x^1,...,x^m \}$以及其标签$y$。将数据投入到预训练得到的模型参数里面得到：$h_l^m$，然后添加一个线性输出层（参数为：$W_y$）去对$y$进行预测。

$$
P(y|x^1,...,x^m)=softmax(h_l^wW_y)
$$

> 对于上述两部分步骤直观上理解：人首先从外界获取大量信息：网络，书本等，把这些信息了解之后，然后去写作文或者去回答问题。

模型结构：

![](https://s2.loli.net/2025/02/15/siGWQOaH9e6znYm.png)

### 2.`GPT v2`

### 3.`GPT v3`

## `Qwen`

## `DeepSeek`

## `LLama`

### 1.`LLama v1`

LLaMA 所采用的 Transformer 结构和细节，与标准的 Transformer 架构不同的地方包括采用了前置层归一化（Pre-normalization）并使用 **RMSNorm 归一化函数 （Normalizing Function）**、激活函数更换为 SwiGLU，并使用了旋转位置嵌入（RoP），整体 Transformer 架构与 GPT-2 类似

### 2.`LLama v2`

区别于上一代的`LLama v1`改进如下几点（主要参考[论文](https://arxiv.org/pdf/2307.09288)中的A.2.1中的描述）：
**1、序列长度**：由原来的2048 tokens变化为4096 tokens
**2、使用GQA**：通过使用KV-cache可以加快模型生成速度，但是也会造成过大的显存占用，因此`LLama v2`使用`GQA`来减少这个过程中的显存占用。
> **GQA原理**：https://www.big-yellow-j.top/posts/2025/01/29/Attention.html

### 3.`LLama v3`

https://blog.csdn.net/qq_57063846/article/details/145612742

## `GLM`

## `BERT`

预训练阶段任务：
1、``Masked LM(MLM)``

MLM是一种预训练任务，通过随机掩蔽输入序列中的部分词元，模型根据上下文预测被掩蔽的词元，从而学习双向语言表示。在模型中作者按照：80：10：10的比例进行处理（80：将词元替换为[MASK]；10：将词元替换为词汇表中随机选取的其他词。；10：保持原词元不变）
**例子：**
输入：“今天天气真好，我打算去[MASK]。”
模型的任务是根据上下文预测“[MASK]”应该是“公园”。


2、``Next Sentence Prediction(NSP)``

NSP是一种预训练任务，模型接收两个句子并预测第二个句子是否是第一个句子的后续。该任务帮助模型理解句子间的逻辑关系。
例子：
句子对：
句子1：“我喜欢去公园散步。”
句子2：“今天下午我会去跑步。”
模型的任务是判断第二个句子是否是第一个句子的自然延续，答案是“是”。

* **缺点**
  1、BERT neglects dependency between the masked positions and suffers from a **pretrain-finetune discrepancy**（忽略了屏蔽位置之间的依赖性，并遭受预训练微调差异的影响）

> 这是因为在 ``BERT``模型中，在预训练阶段会添加 ``[MASK]``，但是在 ``下游任务(downsteram tasks)``中并不会使用 ``[MASK]``

## 评估方式

## 参考
1、https://arxiv.org/pdf/2307.06435
2、https://arxiv.org/abs/2302.13971
3、[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/pdf/1810.04805)
4、[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288)
5、[The Llama 3 Herd of Models](https://arxiv.org/pdf/2407.21783v3)
