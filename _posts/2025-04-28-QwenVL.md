---
layout: mypost
title: Qwen多模态系列论文
categories: paper
extMath: true
images: true
address: changsha
show_footer_image: true
---
TODO: 
1、整理完毕具体的原理：具体位置编码的原理
2、从代码进行出发对比模型：https://zhuanlan.zhihu.com/p/24986805514

本文主要介绍Qwen-vl系列模型包括：Qwen-vl、[Qwen2-vl](#qwen2-vl)、[Qwen2.5-vl](#qwen25-vl)。做一个大混讲

## Qwen2-vl
> http://arxiv.org/abs/2409.12191

模型结构：
![](https://s2.loli.net/2025/04/28/53eVNaWtfglvKus.png)

**Qwen2-vl**主要的改进点在于：1、使用动态分辨率（也就是说输入图像不需要再去改变图像尺寸到一个固定值），于此同时为了减少 **visual-token**数量，将2x2的的相邻的token进行拼接到一个token而后通过MLP层进行处理。2、使用多模态的旋转位置编码（M-RoPE）,也就是将原来位置编码所携带的信息处理为：时序（temporal）、高度（height）、宽度（width）。比如下图中对于文本处理直接初始化为：$(i,i,i)$。但是对于图片而言就是：$(i,x,y)$ 其中 $i$ 是恒定的，而对于视频就会将 $i$ 换成视频中图像的顺序

![image.png](https://s2.loli.net/2025/04/28/1gV5Ci4tTfu6OIB.png)


## Qwen2.5-vl
> http://arxiv.org/abs/2502.13923

模型结构：
![](https://s2.loli.net/2025/04/28/s8WU5wFNngaMHDm.png)

从模型结构上而言在 **Qwen2.5-vl** 中主要改进点在于：
* **视觉编码器上**

1、改进的ViT模型（window-attention+ full-attention）；2、2D-RoPE

* **MLP处理**

通过ViT得到所有的patch之后，直接将这些patch**解析分组**（4个一组）然后继续拼接在输入到两层MLP中进行处理

---

**1、window-attention**
> https://arxiv.org/abs/2004.05150v2

前面有介绍在Kimi和DeepSeek中如何处理稀疏注意力的（[🔗](https://www.big-yellow-j.top/posts/2025/02/21/Kimi-DS-Paper.html)），他们都是通过额外的网络结构来处理注意力计算问题，而在上面提到的注意力计算则是直接通过规则范式计算注意力。

![](https://s2.loli.net/2025/04/28/UNmfSGYikJLVhHe.png)

上面 **window-attention** 处理范式就和卷积操作类似，直接通过移动“步长”然后对“采集”得到的内容进行计算注意力。代码：[⚙](../code/WindowAttention.py.txt)。代码核心点就在于划分，而后对划分结果计算注意力：

```python
q_window = q[:, :, t:window_end, :]  # (B, num_heads, window_size, head_dim)
k_window = k[:, :, t:window_end, :]
v_window = v[:, :, t:window_end, :]
```

**2、2D-RoPE**
#TODO: 待补充

---

介绍完这部分有必要了解一下他是如何处理数据的（毕竟说实在话，模型（无论为LLM还是MLLM在结构上创新远不如数据集重要）都是数据驱动的）以及他是如何训练模型的。
* **1、模型预训练**
![](https://s2.loli.net/2025/04/28/utC4ABwe65Wmqbg.png)

从论文里面作者提到如下几种数据以及处理范式如下：
**1、Image-Text Data**（图片-文本匹配数据集）：保留较高评分匹配对（这里也就是说文本对于图片描述要丰富）、信息互补（图像和文本各自提供独特信息）、信息密度平衡
**2、Video Data**（视频数据）：首先是通过动态采用方式获取视频帧；
**3、图像坐标分辨率处理**：直接将原始图像进行输入不去修改分辨率（固定每个patch为112x112对于不足的不去做填补，总共8x8个patches），对于里面的坐标直接使用Grounding DINO 或者SAM进行获取。
**4、Omni-Parsing Data**：对于文档数据集直接解析为html格式

* **3、模型后训练**




## 参考
1、https://arxiv.org/abs/2004.05150v2
2、http://arxiv.org/abs/2309.16609
3、http://arxiv.org/abs/2409.12191
4、http://arxiv.org/abs/2502.13923