---
layout: mypost
title: LLM 生成策略全解析：从 Beam Search 到 Top-p 采样
categories: 深度学习基础理论
extMath: true
images: true
address: changsha
show_footer_image: true
tags:
- LLM
- beam search
- Greedy Search
description: LLM生成过程包含Prefill阶段和Decoding阶段，Prefill输入整个prompt的所有token输出KV Cache及首个输出token的logits，Decoding利用KV
  Cache和上一步token自回归生成。Beam Search通过束宽（k）选择k个最优输出作为第t步，t+1步结合前k步输出构成新输出，区别于Greedy search仅选最优。温度调节通过温度系数影响概率分布，温度0时选logits最大值对应token（确定性），较高温度使分布均匀增加随机性，较低使分布尖锐减少多样性，数值精度可能导致temperature=0时输出差异，温度仅控制采样随机性。Lookahead
  Search前瞻评估多步生成结果质量，平衡连贯性和多样性。
---

我们都知道在使用dl模型（比如图像分类）最后的结果都是一个概率值（比如100种类别，输出就是每种类别的概率），常见的作法就是直接取概率最大的作为最终预测结果，但是[LLM](https://www.big-yellow-j.top/posts/2025/02/15/LLM.html)里面也用这种方式合理吗（毕竟文本也需要考虑整体的不单单就是让下一个字最佳即可）。本文主要介绍：Beam search、Greedy search等LLM生成策略方式。
![](https://s2.loli.net/2025/06/21/MH1wFkitDJ3Oy7p.webp)
## 模型生成过程
都知道模型输出过程就是将prompt丢给模型，然后模型输出回答，但是实际上在模型输出的内部中输出过程是：**Prefill阶段+Decoding阶段**两部分组成首先对于这两部分简单介绍如下：1、Prefill阶段该阶段中直接将整个 prompt（用户输入 + system prompt + 历史对话等）的所有token进行输入而后去输出完整的KV Cache以及第一步生成logits（也就是**预测第一个输出token**）；2、Decoding阶段直接将Prefill阶段中的KV Cache以及上一步生成的token进行输入而后进行自回归逐个生成。
## Beam Search
区别于Greedy search每次都会选择一个最优的输出，Beam search则是会选择一个 *束宽（beam size）*（k）也就是在生成过程中会从生成的内容中选择k个最为第t步的输出，而后在t+1步中会将前k步的输出结合起来构成新的输出。
## 温度调节（Temperature Scaling）
比如说温度调节使用：
```python
@torch.no_grad()
def generate(self, idx, eos, max_new_tokens, temperature=1.0, top_k=None):
    for _ in range(max_new_tokens):
        # if the sequence context is growing too long we must crop it at block_size
        idx_cond = idx if idx.size(1) <= self.params.max_seq_len else idx[:, -self.params.max_seq_len:]
        # forward the model to get the logits for the index in the sequence
        logits = self(idx_cond)
        logits = logits[:, -1, :] # crop to just the final time step
        if temperature == 0.0:
            # "sample" the single most likely index
            _, idx_next = torch.topk(logits, k=1, dim=-1)
        else:
            # pluck the logits at the final step and scale by desired temperature
            logits = logits / temperature
            # optionally crop the logits to only the top k options
            if top_k is not None:
                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))
                logits[logits < v[:, [-1]]] = -float('Inf')
            # apply softmax to convert logits to (normalized) probabilities
            probs = F.softmax(logits, dim=-1)
            idx_next = torch.multinomial(probs, num_samples=1)
        # append sampled index to the running sequence and continue
        idx = torch.cat((idx, idx_next), dim=1)
        if idx_next==eos:
            break

    return idx
```
上面生成器中，设定最大生成长度`max_new_tokens`，通过前馈计算（`self(idx_cond)`）生成之后通过选取最后时间步的概率（$b,t,vocab\_size \rightarrow b, 1, vocab\_szie$）这样就相当于得到模型新的输出（每个词的概率）当选择的温度参数为0时，模型会选择最可能的 token（即选择 logits 中最大值对应的索引）。这是确定性的选择。反之就会，logits 会被 temperature 除以，从而影响选择的多样性。较高的温度值会使得概率分布变得更均匀，增加随机性；较低的温度值会让概率分布更加尖锐，使得选择更有偏向性。
从上面代码其实也很容易发现 **温度系数**是如何影响输出的：

$$
P(x)=\frac{e^{logit_i/T}}{\sum e^{logit_j/T}}
$$

通过温度系数$T$来平滑最后输出：
当$T=1$ 时，公式恢复为标准的softmax计算，logits 被直接用来计算概率。
当$T>1$ 时，logits 被缩小，概率分布变得更加平滑。这个时候，**低概率的选项也可能被采样到**，从而增加生成的多样性。温度较高时，模型的输出更加随机，生成的文本更加多样化。
当$T<1$时，logits 被放大，概率分布更加尖锐。这个时候，高概率的选项会变得更加突出，低概率的选项几乎被完全抑制，从而使模型的输出更加确定性。低温度时，模型倾向于生成高概率的单词，**减少了生成的多样性**。
> 那么按照理论上当设置temperature=0时候输出应该是相同的，因为次数每一步都是使用greedy decoding进行处理，但是实际上不是的这是因为数值精度原因导致发生差异。总的来说temperature只能起到控制采用的随机性。

## Lookahead Search
在生成过程中，它不仅考虑当前 token，还预先评估多步生成结果的质量，以此来选择最合适的当前步骤。它是一种对未来进行**前瞻评估**的策略，能够平衡生成的连贯性和多样性。使用方法比较简单
```python
probs = F.softmax(logits, dim=-1)
candidate_probs = []
for _ in range(lookahead_depth):
    idx_next = torch.multinomial(probs, num_samples=1)
    candidate_score = self.evaluate_candidate(idx, idx_next, eos)
    candidate_probs.append((candidate_score, idx_next))

candidate_probs.sort(reverse=True, key=lambda x: x[0])
best_candidate = candidate_probs[0][1]
idx = torch.cat((idx, best_candidate), dim=1)
if best_candidate == eos:
    break
```