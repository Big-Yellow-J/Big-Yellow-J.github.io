---
layout: mypost
title: å¤šæ¨¡æ€ç®—æ³•Clipã€Albefã€Blipç­‰ç®—æ³•åŸç†
categories: æ·±åº¦å­¦ä¹ åŸºç¡€ç†è®º
extMath: true
images: true
address: æ­¦æ±‰ğŸ¯
tags:
- cv-backbone
- å¤šæ¨¡æ€
- multimodal
show_footer_image: true
description: è§†è§‰å¤šæ¨¡æ€æ¨¡å‹ï¼ˆå¦‚CLIPã€ALBEFã€BLIPv1/v2ï¼‰æ ¸å¿ƒæŒ‘æˆ˜åœ¨äºæ¨¡æ€ä¿¡æ¯ç»“åˆï¼Œé€šå¸¸é‡‡ç”¨Vit/Resnetç­‰è§†è§‰ç¼–ç å™¨ä¸æ–‡æœ¬ç¼–ç å™¨å¤„ç†å›¾åƒå’Œæ–‡æœ¬ï¼Œé€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼ˆå¦‚InfoNCEæŸå¤±ï¼‰å®ç°è·¨æ¨¡æ€å¯¹é½ã€‚CLIPä¾§é‡å­¦ä¹ è·¨æ¨¡æ€ç›¸ä¼¼åº¦è¡¨ç¤ºï¼Œå…·å¤‡é›¶æ ·æœ¬èƒ½åŠ›ï¼›ALBEFå’ŒBLIPç³»åˆ—é€šè¿‡æ¨¡æ€å¯¹é½ï¼ˆITCï¼‰ã€å›¾æ–‡åŒ¹é…ï¼ˆITMï¼‰ç­‰ä»»åŠ¡ä¼˜åŒ–ï¼ŒBLIPv2æ›´å¼•å…¥Q-Formerå°†å›¾åƒç‰¹å¾æ˜ å°„è‡³LLMç©ºé—´ï¼Œç»“åˆå†»ç»“LLMæå‡ç”Ÿæˆä¸é›¶æ ·æœ¬æ€§èƒ½ï¼Œé€‚ç”¨äºå›¾æ–‡æ£€ç´¢ã€åˆ†ç±»ç­‰å¤šä»»åŠ¡ã€‚
---

è§†è§‰å¤šæ¨¡æ€æ¨¡å‹åœ¨ç»“æ„ä¸Šæ¯”è¾ƒç»Ÿä¸€ï¼Œä¸€ä¸ªè§†è§‰ç¼–ç å™¨ï¼ˆè¾ƒå¤šä½¿ç”¨çš„æ˜¯Vit/Resnetç­‰ï¼‰å¯¹å›¾åƒä¿¡æ¯è¿›è¡Œå¤„ç†ï¼Œç„¶åå°†å…¶å’Œæ–‡æœ¬ä¿¡æ¯ä¸€èµ·ç»“åˆç„¶åè¾“å…¥åˆ°LLMæ¨¡å‹ä¸­å¾—åˆ°æœ€åçš„ç»“æœï¼Œå› æ­¤åœ¨æ­¤è¿‡ç¨‹ä¸­ä¸€ä¸ªæœ€å¤§çš„æŒ‘æˆ˜å°±æ˜¯ï¼š**å¦‚æœå°†ä¸åŒæ¨¡æ€ä¿¡æ¯è¿›è¡Œç»“åˆ**ï¼ˆå½“ç„¶æœ‰äº›å¯èƒ½è¿˜éœ€è¦è€ƒè™‘å¦‚ä½•å°†å›¾åƒè¿›è¡Œå‹ç¼©ï¼Œè¿™é‡Œä¸»è¦æ˜¯è€ƒè™‘æœ‰äº›å›¾åƒçš„åˆ†è¾¨ç‡æ¯”è¾ƒé«˜ï¼‰ã€‚

## Clip

![](https://s2.loli.net/2025/06/22/H6kEoxgzYAWNhXp.webp)

ä»£è¡¨æ¨¡å‹ [**CLIP**](https://arxiv.org/pdf/2103.00020)[^2]ï¼Œæ›´åŠ åƒä¸€ç§ **å›¾åƒ-æ–‡æœ¬**å¯¹é½æ¨¡å‹ï¼ŒæŒ‰ç…§è®ºæ–‡é‡Œé¢ä»–è‡ªå·±æåˆ°çš„è®¡ç®—èŒƒå¼ï¼š

```python
# image_encoder - ResNet or Vision Transformer
# text_encoder - CBOW or Text Transformer
# I[n, h, w, c] - minibatch of aligned images
# T[n, l] - minibatch of aligned texts
# W_i[d_i, d_e] - learned proj of image to embed
# W_t[d_t, d_e] - learned proj of text to embed
# t - learned temperature parameter
# extract feature representations of each modality
I_f = image_encoder(I) #[n, d_i]
T_f = text_encoder(T) #[n, d_t]
# joint multimodal embedding [n, d_e]
I_e = l2_normalize(np.dot(I_f, W_i), axis=1)
T_e = l2_normalize(np.dot(T_f, W_t), axis=1)
# scaled pairwise cosine similarities [n, n]
logits = np.dot(I_e, T_e.T) * np.exp(t)
# symmetric loss function
labels = np.arange(n)
loss_i = cross_entropy_loss(logits, labels, axis=0)
loss_t = cross_entropy_loss(logits, labels, axis=1)
loss = (loss_i + loss_t)/2
```

åœ¨å°† Image å’Œ Text ç¼–ç å®Œæˆä¹‹åï¼Œç›´æ¥è®¡ç®—å®ƒä»¬ä¹‹é—´çš„**ç›¸ä¼¼åº¦**ï¼Œå®ç°æ¨¡æ€ä¹‹é—´çš„å¯¹é½ã€‚ä¼˜åŒ–è¿‡ç¨‹çš„ç›®æ ‡æ˜¯è®©åŒ¹é…çš„å›¾æ–‡å¯¹çš„ç›¸ä¼¼åº¦å°½å¯èƒ½å¤§ï¼ŒåŒæ—¶è®©ä¸åŒ¹é…å¯¹çš„ç›¸ä¼¼åº¦å°½å¯èƒ½å°ã€‚æ¢è¨€ä¹‹ï¼ŒCLIP çš„å¯¹æ¯”å­¦ä¹ æœºåˆ¶æœ¬è´¨ä¸Šæ˜¯åœ¨å­¦ä¹ ä¸€ç§è·¨æ¨¡æ€çš„ç›¸ä¼¼åº¦è¡¨ç¤ºã€‚å…¶æ ¸å¿ƒæœºåˆ¶æ˜¯é€šè¿‡å¯¹æ¯”å­¦ä¹ å’ŒåµŒå…¥ç©ºé—´å¯¹é½ï¼Œå°†å›¾åƒå’Œæ–‡æœ¬æ˜ å°„åˆ°ä¸€ä¸ªå…±äº«çš„è¯­ä¹‰ç©ºé—´ä¸­ã€‚
å°½ç®¡ CLIP æœ¬èº«å¹¶ä¸ç›´æ¥åŒ…å«å¤æ‚çš„æ¨ç†èƒ½åŠ›æˆ–ä»»åŠ¡ç‰¹å®šçš„çŸ¥è¯†ï¼Œä½†å®ƒé€šè¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œå±•ç°å‡ºäº†å¼ºå¤§çš„é€šç”¨æ€§å’Œé›¶æ ·æœ¬å­¦ä¹ èƒ½åŠ›ã€‚åœ¨è®ºæ–‡ä¸­ï¼ŒCLIP è¡¨ç°å‡ºäº†ä¸ä¿—çš„é›¶æ ·æœ¬æ€§èƒ½ï¼Œä½†éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒCLIP çš„ä¸»è¦ç›®æ ‡æ˜¯å­¦ä¹ è·¨æ¨¡æ€çš„å¯¹é½è¡¨ç¤ºï¼Œè¿™ä½¿å¾—å®ƒèƒ½å¤Ÿèƒœä»»å¤šç§ä»»åŠ¡ï¼ˆå¦‚å›¾æ–‡æ£€ç´¢ã€é›¶æ ·æœ¬åˆ†ç±»ç­‰ï¼‰ã€‚ç›¸æ¯”äºä¼ ç»Ÿçš„ç›®æ ‡è¯†åˆ«æ¨¡å‹ï¼ŒCLIP æ›´åƒæ˜¯ä¸€ä¸ªå¤šæ¨¡æ€çš„åŸºç¡€æ¨¡å‹ï¼Œå…·å¤‡æ›´å¹¿æ³›çš„é€‚ç”¨æ€§å’Œçµæ´»æ€§ã€‚
## ALBEF
Albef[^1]æ¨¡å‹åŸºæœ¬ç»“æ„å¦‚ä¸‹ï¼š
![](https://s2.loli.net/2025/09/19/wCK5MxvBQITkuhE.png)
**æ¨¡å‹ç»“æ„**ï¼š1ã€å›¾åƒç¼–ç å™¨ï¼ˆ12å±‚çš„Vit-B/16ï¼‰ï¼›2ã€æ–‡æœ¬ç¼–ç å™¨ï¼ˆ6å±‚ $\text{BERT}_{\text{base}}$ ï¼‰ï¼›3ã€å¤šæ¨¡æ€ç¼–ç å™¨ï¼ˆ6å±‚ $\text{BERT}_{\text{base}}$ï¼‰ã€‚å¯¹äºæ–‡æœ¬å’Œå›¾åƒéƒ½ä¼šç¼–ç ä¸ºå¸¦å‰ç¼€çš„å‘é‡ï¼Œå›¾åƒï¼š${v_{cls},v_1,...,v_N}$ï¼Œæ–‡æœ¬ï¼š${w_{cls},w_1,...,w_N}$ã€‚
**è®­ç»ƒè¿‡ç¨‹**ï¼š**1ã€æ¨¡æ€å¯¹é½ï¼ˆITCï¼‰**ï¼šè¿™ä¸ªè¿‡ç¨‹ä¸»è¦æ˜¯è®¡ç®—image-to-textä»¥åŠtext-to-imageç›¸ä¼¼æ€§è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š
![](https://s2.loli.net/2025/09/19/SqxarzjPtbegiQZ.png)
å…¶ä¸­ $s(I, T_m)=g_v(v_{cls})^Tg^â€²(w^â€²_{cls})$ï¼Œç›¸ä¼¼æ€§è®¡ç®—å…¬å¼ä¸­ $g$ä¸»è¦æ˜¯å°† `[CLS]`é€šè¿‡çº¿æ€§å¤„ç†å¤„ç†åˆ°256ç»´ï¼Œè€Œ $g^â€²$åˆ™æ˜¯é€šè¿‡åŠ¨é‡ç¼–ç å™¨çš„è§„èŒƒåŒ–ç‰¹å¾è¡¨ç¤ºã€‚$y$ä»£è¡¨GTã€‚
> å¯¹äºè¿™ä¸ªlossè®¡ç®—è¿‡ç¨‹å†Albefä¸­ä¼šæ”¹å†™ä¸ºï¼š
> ![](https://s2.loli.net/2025/09/19/X9I8ZEzxOyeg3GS.png)
> å…¶ä¸­$s$ä»£è¡¨score functionï¼ˆæ¯”å¦‚è¯´ç›´æ¥è®¡ç®—ç‚¹ä¹˜ï¼‰ï¼Œ$\tau$æ¸©åº¦ç¨€ç–

**2ã€é®è”½è¯­è¨€æ¨¡å‹( MLM )**ï¼šç›´æ¥é¢„æµ‹è¢«MASKæ‰çš„è¯ï¼›**3ã€å›¾æ–‡åŒ¹é…ï¼ˆITMï¼‰**ï¼šä¸»è¦æ˜¯åˆ¤æ–­å›¾æ–‡ä¹‹é—´åŒ¹é…ï¼Œå¯¹äºè¿™ä¸¤ä¸ªè¿‡ç¨‹æ•°æ®å¤„ç†ä¸ºï¼š
![](https://s2.loli.net/2025/09/19/eVdW7hRcSwn3Ial.png)
## BLIP
### BLIPv1
BLIP-1[^3]æ¨¡å‹ç»“æ„å¦‚ä¸‹
![](https://s2.loli.net/2025/09/19/vOkf7aWluqItKEh.png)
å¯¹äºæ¨¡å‹ä½¿ç”¨å¯¹äº**è§†è§‰ç¼–ç å™¨ç›´æ¥ä½¿ç”¨Vit**ï¼Œå¯¹äº**æ–‡æœ¬ç¼–ç å™¨ç›´æ¥ä½¿ç”¨BERT**ï¼Œä¸è¿‡å€¼å¾—æ³¨æ„çš„æ˜¯å’ŒAlbefä¸­å¤„ç†ç›¸åŒçš„æ˜¯åœ¨ç‰¹å¾å‰é¢éƒ½ä¼šé€‰æ‹©æ·»åŠ ä¸€ä¸ª`[CLS]`æ ‡è®°ç„¶åå…¶ä»–ç»“æ„é›†åˆä¸Šé¢çš„ä¸€è‡´ã€‚åœ¨æ¨¡å‹ç»“æ„ä¸Šä¸»è¦åˆ†ä¸º3å—ï¼š1ã€Text Encderï¼›2ã€Image grounded Text encoderï¼›3ã€Image-grouned Text decoderï¼›å¯¹äºè¿™3å—éƒ½åˆ†åˆ«å¯¹åº”çš„å»è®¡ç®—ITCã€ITMä»¥åŠLM3ä¸ªæŸå¤±ï¼Œå…¶ä¸­å‰ä¸¤ä¸ªå’ŒAlbefä¸­è®¡ç®—æ–¹å¼ç›¸åŒã€‚é™¤æ­¤ä¹‹å¤–è™½ç„¶è®¾è®¡äº†3ä¸ªæ¨¡å—ä½†æ˜¯æ¨¡å—ä¹‹é—´å‚æ•°æ˜¯å…±äº«çš„ï¼ˆ**é¢œè‰²ç›¸åŒé‚£ä¹ˆå‚æ•°å°±æ˜¯ç›¸åŒçš„**ï¼‰
![](https://s2.loli.net/2025/09/19/kvuBxLI18JtEjAC.png)
è®ºæ–‡ä¸­æ•°æ®åˆæˆæ–¹æ³•ï¼Œå…¶å®è¿˜æ˜¯åŸºäºBLIPè‡ªèº«çš„encoder-decoderç»“æ„ï¼Œé¦–å…ˆæ˜¯é€šè¿‡æ ‡æ³¨çš„æ•°æ®ï¼ˆ$I_h,T_h$ï¼‰è¿›è¡Œè®­ç»ƒæ¨¡å‹åœ¨å¾—åˆ°å¾ˆå¥½çš„æ•ˆæœä¹‹åï¼Œå°†æœªæ ‡æ³¨çš„å›¾ç‰‡ $I_w$ç›´æ¥è¾“å…¥åˆ°æ¨¡å‹ä¸­ç”Ÿæˆå›¾-æ–‡å¯¹ï¼ˆ$I_w,T_s$ï¼‰ä»¥åŠä»ç½‘ç»œä¸Šæœç´¢å¾—åˆ°çš„å›¾-æ–‡å¯¹ï¼ˆ$I_w,T_w$ï¼‰æ­¤æ—¶è¿™ä¸¤éƒ¨åˆ†å›¾æ–‡å¯¹ä¸æ˜¯å¾ˆâ€œæ°å½“çš„â€é€šè¿‡filterå»è¿‡æ»¤æ‰ä¸åˆé€‚çš„é…å¯¹è¿™æ ·ä¸€æ¥æœ€åå°±å¯ä»¥å¾—åˆ°ç›¸å¯¹å¹²å‡€çš„å›¾-æ–‡å¯¹ã€‚
> å…¶ä¸­`filter`è®¾è®¡å°±æ˜¯ç›´æ¥ä½¿ç”¨ image-ground text encoderé€šè¿‡ç›´æ¥å¾®è°ƒæ¥è®©æ¨¡å‹çŸ¥é“ å›¾-æ–‡åŒ¹é…æ•ˆæœ

### BLIPv2
**BLIP-2**[^4]æ¨¡å‹ç»“æ„å¦‚ä¸‹ï¼š
![](https://s2.loli.net/2025/09/20/MidSCm4Ioev3ULT.png)
åœ¨ BLIP-2ä¸­**åŒæ—¶å†»ç»“äº†Image Encoderä»¥åŠLLM**å› æ­¤ä¸ºäº†å¼¥è¡¥ä¸åŒæ¨¡æ€ä¹‹é—´çš„å·®å¼‚ï¼Œå°±éœ€è¦è®¾è®¡ä¸€ä¸ªâ€œæ¨¡å—â€æ¥è¿›è¡Œè¡¨ç¤ºï¼ˆåœ¨è®ºæ–‡ä¸­åšæ³•æ˜¯ï¼šé€šè¿‡è®¾è®¡ä¸€ä¸ª[Q-Former](https://github.com/salesforce/LAVIS/blob/main/lavis/models/blip2_models/blip2_qformer.py)**å°†Image/Textä¸Šçš„ä¿¡æ¯éƒ½â€åæ˜ â€œåˆ°ä¸€ä¸ªLearned-Queriesä¸Š**ï¼‰ã€‚
> **Q-Former**é€šè¿‡åˆå§‹åŒ–çš„queryç„¶åå°†å›¾ç‰‡å’Œæ–‡æœ¬ç‰¹å¾éƒ½åæ˜ åˆ°queryä¸Šï¼Œå…¶ç»“æ„å°±æ˜¯ç›´æ¥ä½¿ç”¨BERTä½œä¸ºä¸»ä½“ç»“æ„ï¼Œé€šè¿‡æ”¹å˜BERTçš„è¾“å…¥æ•°æ®æ¥ä¿è¯å¯¹äºå›¾ç‰‡å’Œæ–‡æœ¬çš„ç‰¹å¾â€œåæ˜ â€

å…·ä½“æ“ä½œåˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š
![](https://s2.loli.net/2025/09/20/CXk69glF2qhIrRK.png)
**ç¬¬ä¸€é˜¶æ®µ**ï¼šç»“æ„å›¾å¦‚ä¸Šæ‰€è¿°é€šè¿‡å†»ç»“image-encoderï¼Œæ¨¡å‹å¯¹äºè¾“å‡ºé¦–å…ˆè¿›è¡Œå¤„ç†[è¿‡ç¨‹](https://github.com/salesforce/LAVIS/blob/506965b9c4a18c1e565bd32acaccabe0198433f7/lavis/models/blip2_models/blip2_qformer.py#L91C9-L127C10)ï¼š
```python
image_embeds = self.ln_vision(self.visual_encoder(image))
query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)
query_output = self.Qformer.bert(query_embeds=query_tokens,encoder_hidden_states=image_embeds,encoder_attention_mask=image_atts,...)
image_feats = F.normalize(self.vision_proj(query_output.last_hidden_state), dim=-1)
text_tokens = self.tokenizer(text,...)
text_output = self.Qformer.bert(text_tokens.input_ids,...)
```

å¯¹äº`self.query_tokens`åˆå§‹åŒ–ç›´æ¥é€šè¿‡[ç”Ÿæˆå…¨0çš„å‘é‡](https://github.com/salesforce/LAVIS/blob/506965b9c4a18c1e565bd32acaccabe0198433f7/lavis/models/blip2_models/blip2.py#L57)ã€‚é™¤æ­¤ä¹‹å¤–å¯¹äºåˆå§‹åŒ–åçš„ `query_tokens`ä¹‹åä¼šé€šè¿‡ `self.Qformer.bert`ï¼ˆ**Qformeré‡‡ç”¨çš„è¿˜æ˜¯BERTç»“æ„**ï¼Œå› æ­¤æ‰€æœ‰çš„ä¸Šé¢ç»“æ„å›¾ä¸­æ¶‰åŠåˆ°çš„å„ç§attention maskæ“ä½œä¹Ÿéƒ½æ˜¯å†bertä¸­è®¡ç®—åªæ˜¯é€šè¿‡å‚æ•°ï¼š`attention_mask`æ§åˆ¶ï¼‰å°†å…¶æ ¸å›¾åƒç‰¹å¾è¿›è¡Œâ€œäº¤äº’â€æœ€åå¾—åˆ° `image_feats`ï¼Œè€Œå¯¹äºæ–‡æœ¬å¤„ç†è¿‡ç¨‹å°±æ¯”è¾ƒç®€å•ç›´æ¥tokenizerå¤„ç†ä¹‹åå†å»æœ‰bertç¼–ç å³å¯å¾—åˆ°`text_feat`
åœ¨å¾—åˆ°3éƒ¨åˆ†è¾“å…¥ä¹‹åå†Qformerä¸­è€Œåè¿›è¡Œ3ä¸ªè®­ç»ƒä»»åŠ¡ï¼š
**1ã€å›¾ç‰‡å¯¹æ¯”æŸå¤±ITC**ï¼ˆ[ä»£ç ](https://github.com/salesforce/LAVIS/blob/506965b9c4a18c1e565bd32acaccabe0198433f7/lavis/models/blip2_models/blip2_qformer.py#L129C9-L174C1)ï¼‰ï¼š
```python
sim_q2t = torch.matmul(image_feats.unsqueeze(1), text_feat_all.unsqueeze(-1)).squeeze()
sim_i2t, _ = sim_q2t.max(-1)
sim_i2t = sim_i2t / self.temp
sim_t2q = torch.matmul(text_feat.unsqueeze(1).unsqueeze(1), image_feats_all.permute(0, 2, 1)).squeeze()
sim_t2i, _ = sim_t2q.max(-1)
sim_t2i = sim_t2i / self.temp
...
loss_itc = (
  F.cross_entropy(sim_i2t, targets, label_smoothing=0.1)+ 
  F.cross_entropy(sim_t2i, targets, label_smoothing=0.1)) / 2
```
å¯¹äºITCä¸­è®¡ç®— **InfoNCE**
**2ã€å›¾ç‰‡æ–‡æœ¬é…å¯¹ITM**ï¼ˆ[ä»£ç ](https://github.com/salesforce/LAVIS/blob/506965b9c4a18c1e565bd32acaccabe0198433f7/lavis/models/blip2_models/blip2_qformer.py#L176C9-L247C55)ï¼‰ï¼šè¿™ä¸ªè¿‡ç¨‹é¦–å…ˆå†ITCä¸­ä¼šå¾—åˆ° `sim_t2i` å’Œ `sim_i2t`è¿™ä¸¤ä¸ªçŸ©é˜µï¼ˆåˆ†åˆ«ä»£è¡¨å›¾ç‰‡æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µï¼‰ï¼Œè¿™æ ·ä¸€æ¥å°±å¯ä»¥ç›´æ¥æ›´å…·è¿™ä¸ªç›¸ä¼¼åº¦çŸ©é˜µå»ä¸åŒ¹é…çš„å›¾æ–‡å¯¹å’Œæ–‡å›¾å¯¹ã€‚æœ€ç»ˆï¼Œ**ä½œä¸ºæ­£æ ·æœ¬çš„å›¾æ–‡å¯¹å°±æ˜¯åŸå§‹è¾“å…¥çš„å›¾æ–‡å¯¹ï¼Œè€Œä½œä¸ºè´Ÿæ ·æœ¬çš„å›¾æ–‡å¯¹å’Œæ–‡å›¾å¯¹å°±æ˜¯é€šè¿‡ç›¸ä¼¼çŸ©é˜µé‡‡æ ·å‡ºæ¥çš„**ã€‚
```python
image_embeds_all = torch.cat([image_embeds, image_embeds_neg, image_embeds], dim=0)  # pos, neg, pos
image_atts_all = torch.ones(image_embeds_all.size()[:-1], dtype=torch.long).to(image.device)
...
text_ids_all = torch.cat([text_tokens.input_ids, text_tokens.input_ids, text_ids_neg], dim=0)  # pos, pos, neg
query_tokens_itm = self.query_tokens.expand(text_ids_all.shape[0], -1, -1)
...
output_itm = self.Qformer.bert(
  text_ids_all,
  query_embeds=query_tokens_itm,
  attention_mask=attention_mask_all,
  encoder_hidden_states=image_embeds_all,
  encoder_attention_mask=image_atts_all,...)

vl_embeddings = output_itm.last_hidden_state[:, : query_tokens_itm.size(1), :]
vl_output = self.itm_head(vl_embeddings)
logits = vl_output.mean(dim=1)

itm_labels = torch.cat([torch.ones(bs, dtype=torch.long), torch.zeros(2 * bs, dtype=torch.long)],dim=0,).to(image.device)
loss_itm = F.cross_entropy(logits, itm_labels)
```
**3ã€å›¾ç‰‡ç”Ÿæˆæ–‡æœ¬ITG**ï¼š
```python
lm_output = self.Qformer(
            decoder_input_ids,
            attention_mask=attention_mask,
            past_key_values=query_output.past_key_values,
            return_dict=True,
            labels=labels,
        )

loss_lm = lm_output.loss
```
é€šè¿‡ä¸Šé¢ä¸‰ä¸ªä»»åŠ¡ï¼Œè®­ç»ƒå¥½çš„query tokenså’ŒQ-Formerå°±èƒ½å¤Ÿå°†image encoderæå–çš„åŸå§‹å›¾åƒç‰¹å¾å’Œæ–‡æœ¬ç‰¹å¾è¿›è¡Œæ‹‰è¿‘ã€‚**ç†è®ºä¸Šï¼Œè¿™ä¸ªé˜¶æ®µçš„æ¨¡å‹ï¼Œå°±æ˜¯ä¸€ä¸ªè®­ç»ƒå®Œæˆçš„å›¾æ–‡å¤šæ¨¡æ€æ¨¡å‹ã€‚è¯¥æ¨¡å‹èƒ½å¤Ÿå®Œæˆå›¾æ–‡retrievalã€å›¾æ–‡åŒ¹é…ã€å›¾ç”Ÿæ–‡çš„ä»»åŠ¡**[^5]ã€‚ä¸ºäº†è¿›ä¸€æ­¥åˆ©ç”¨LLMsçš„ç”Ÿæˆèƒ½åŠ›å’Œzero-shotèƒ½åŠ›ï¼Œè®­ç»ƒè¿›å…¥ç¬¬äºŒé˜¶æ®µã€‚
**ç¬¬äºŒé˜¶æ®µ**
![](https://s2.loli.net/2025/09/20/bnY4dVvi2I1Cf6O.png)
LLMsæ˜¯ä¸€ä¸ªç”Ÿæˆå¼æ¨¡å‹ï¼Œæ•´ä¸ªæµç¨‹æ˜¯ï¼šå†»ç»“çš„Image Encoderç”ŸæˆåŸå§‹çš„å›¾åƒç‰¹å¾ï¼Œè€Œquery tokenså’ŒQ-Formerä»åŸå§‹å›¾åƒç‰¹å¾ä¸­ç”Ÿæˆè½¬åŒ–å¥½çš„å›¾åƒç‰¹å¾ï¼Œç„¶åè¯¥å›¾åƒç‰¹å¾ç»è¿‡å…¨è¿æ¥å±‚æ˜ å°„åˆ°LLMsçš„æ–‡æœ¬embeddingç©ºé—´ä¸­ã€‚ç„¶åè¿™äº›æ˜ å°„åçš„å›¾åƒç‰¹å¾ï¼Œå°±ç›¸å½“äºè§†è§‰promptsï¼Œå’Œæ–‡æœ¬embeddingä¸€èµ·ï¼Œè¾“å…¥åˆ°å†»ç»“çš„LLMsä¸­ï¼Œæœ€åç”Ÿæˆç›®æ ‡æ–‡æœ¬ã€‚
## æ€»ç»“
ä¸Šé¢æåˆ°å‡ ä¸ªæ¨¡å‹Clipã€Albefã€Blipv1ã€Blipv2é¦–å…ˆå†æ–‡æœ¬ä»¥åŠå›¾ç‰‡ç¼–ç ä¸Šå·®å¼‚ä¸å¤§ï¼Œç‰¹å¾å¯¹é½ä¸Šä¹Ÿéƒ½æ˜¯é€‰æ‹© **å¯¹æ¯”å­¦ä¹ æ–¹å¼**å»å¯¹é½å›¾ç‰‡å’Œæ–‡æœ¬ä¹‹é—´çš„æ¨¡æ€ä¿¡æ¯ï¼Œåé¢3ä¸ªæ¨¡å‹åœ¨æ¨¡æ€å¯¹é½ä¸Šé€‰æ‹©è®¡ç®—æ–¹å¼éƒ½æ˜¯ **InfoNCE**
$$
\mathcal{L}_{\text{itc}} = -\frac{1}{2} \mathbb{E}_{p(I,T)} \left[ \log \frac{\exp(s(I,T)/\tau)}{\sum_{m=1}^M \exp(s(I,T_m)/\tau)} + \log \frac{\exp(s(T,I)/\tau)}{\sum_{m=1}^M \exp(s(T,I_m)/\tau)} \right]
$$
ä¸è¿‡åœ¨Blipv2ä¸­æ˜¯å°†æ–‡æœ¬ï¼Œå›¾ç‰‡ä¿¡æ¯éƒ½åæ˜ åˆ°ä¸€ä¸ªåˆå§‹åŒ–çš„queryä¸Šã€‚
## å‚è€ƒ
[^1]: [https://arxiv.org/pdf/2107.07651](https://arxiv.org/pdf/2107.07651)
[^2]: [https://arxiv.org/pdf/2103.00020](https://arxiv.org/pdf/2103.00020)
[^3]: [https://arxiv.org/pdf/2201.12086](https://arxiv.org/pdf/2201.12086)
[^4]: [https://arxiv.org/pdf/2301.12597](https://arxiv.org/pdf/2301.12597)
[^5]: [https://zhuanlan.zhihu.com/p/664601983](https://zhuanlan.zhihu.com/p/664601983)