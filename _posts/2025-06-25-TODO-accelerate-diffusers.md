---
layout: mypost
title: æ·±å…¥æµ…å‡ºäº†è§£ç”Ÿæˆæ¨¡åž‹-5ï¼šdiffuser/accelerateåº“å­¦ä¹ 
categories: python
extMath: true
images: true
address: æ­¦æ±‰ðŸ¯
show_footer_image: true
tags: [ç”Ÿæˆæ¨¡åž‹,diffusion model,python]
show: true
stickie: true
description: å·¥æ¬²å–„å…¶äº‹å¿…å…ˆåˆ©å…¶å™¨ï¼Œä»‹ç»å†å¤šçš„ç”Ÿæˆæ¨¡åž‹æ²¡æœ‰ä¸€ä¸ªå¥½çš„å·¥å…·æ˜¯ä¸è¡Œçš„ï¼Œå› æ­¤æœ¬ä½ä¸»è¦ä»‹ç»å‡ ä¸ªåœ¨ç”Ÿæˆæ¨¡åž‹ä¸­å¸¸ç”¨çš„pythonåº“ï¼šdiffuser/accelerateçš„åŸºæœ¬ä½¿ç”¨ä»¥åŠä»£ç æ“ä½œã€‚
---

å·¥æ¬²å–„å…¶äº‹ï¼Œå¿…å…ˆåˆ©å…¶å™¨ã€‚å³ä¾¿ä»‹ç»äº†å†å¤šç”Ÿæˆæ¨¡åž‹ï¼Œæ²¡æœ‰è¶æ‰‹çš„å·¥å…·ä¹Ÿéš¾ä»¥æ–½å±•æ‰åŽã€‚å› æ­¤ï¼Œæœ¬æ–‡å°†é‡ç‚¹ä»‹ç»å‡ ä¸ªåœ¨ç”Ÿæˆæ¨¡åž‹å¼€å‘ä¸­å¸¸ç”¨çš„ Python åº“ï¼Œç€é‡è®²è§£ **Diffusers** å’Œ **Accelerate** çš„åŸºæœ¬ä½¿ç”¨ã€‚æ„Ÿè°¢ Hugging Face ä¸ºæ— æ•°ç®—æ³•å·¥ç¨‹å¸ˆæä¾›äº†å¼ºå¤§çš„å¼€æºæ”¯æŒï¼éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå®˜æ–¹æ–‡æ¡£å¯¹è¿™ä¸¤ä¸ªåº“å·²æœ‰è¯¦å°½çš„è¯´æ˜Žï¼Œæœ¬æ–‡ä»…ä½œä¸ºä¸€ç¯‡ç®€æ˜Žçš„ä½¿ç”¨ç¬”è®°ï¼ŒæŠ›ç –å¼•çŽ‰ï¼Œä¾›å‚è€ƒå’Œäº¤æµã€‚

## accelerate
> æŽ¨èç›´æŽ¥é˜…è¯»å®˜æ–¹æ–‡æ¡£ï¼š[https://huggingface.co/docs/accelerate/index](https://huggingface.co/docs/accelerate/index)
> [`pip install accelerate`](https://huggingface.co/docs/accelerate/basic_tutorials/install)

ä»‹ç»ä¹‹å‰äº†è§£ä¸€ä¸‹è¿™ä¸ªåº“æ˜¯å¹²ä»€ä¹ˆçš„ï¼šè¿™ä¸ªåº“ä¸»è¦æä¾›ä¸€ä¸ªå¿«é€Ÿçš„åˆ†å¸ƒå¼è®­ç»ƒï¼ˆé¿å…äº†ç›´æŽ¥ç”¨torchè¿›è¡Œæ‰‹æ“ï¼‰å¹¶ä¸”æ”¯æŒå„ç±»åŠ é€Ÿæ–¹æ³•ï¼š[æ··åˆç²¾åº¦è®­ç»ƒ](https://www.big-yellow-j.top/posts/2025/01/01/mixed-precision.html)ã€[Deepspeed](https://www.big-yellow-j.top/posts/2025/02/24/deepspeed.html)ã€æ¢¯åº¦ç´¯è®¡ç­‰

## ä¸€ä¸ªåŸºæœ¬ä½¿ç”¨åœºæ™¯
ä¸€èˆ¬ä»»åŠ¡ä¸­ä¸€ä¸ªå¸¸è§çš„åº”ç”¨åœºæ™¯æ˜¯ï¼šéœ€è¦å®žçŽ°ä¸€ä¸ªå¤šæ˜¾å¡ï¼ˆè¿™é‡Œå‡è®¾ä¸ºåŒæ˜¾å¡ï¼‰åˆ†å¸ƒå¼è®­ç»ƒï¼Œå¹¶ä¸”ä½¿ç”¨æ¢¯åº¦ç´¯è®¡ã€æ··åˆç²¾åº¦è®­ç»ƒï¼Œå¹¶ä¸”è®­ç»ƒå¾—åˆ°çš„ç»“æžœé€šè¿‡tensorboard/wandbè¿›è¡Œè®°å½•ï¼Œé™¤æ­¤ä¹‹å¤–è¿˜éœ€è¦ä½¿ç”¨warm-upå­¦ä¹ çŽ‡è°ƒæ•´ç­–ç•¥ï¼Œå¹¶ä¸”æˆ‘çš„æ¨¡åž‹ä¸åŒæ¨¡å—ä½¿ç”¨çš„å­¦ä¹ çŽ‡ä¸åŒï¼Œè®­ç»ƒå®Œæˆä¹‹åŽæ‰€æœ‰çš„æ¨¡åž‹æƒé‡è¦è¿›è¡Œä¿å­˜/è¯»å–æƒé‡è¿›è¡Œæµ‹è¯•ã€‚é‚£ä¹ˆå¯ä»¥ç›´æŽ¥é€šè¿‡ä¸‹é¢ä»£ç è¿›è¡Œå®žçŽ°ï¼ˆéƒ¨åˆ†åº“çš„å¯¼å…¥ä»¥åŠä¸€äº›å‚æ•°æ¯”å¦‚è¯´configç›´æŽ¥å¿½ç•¥ï¼‰

```python
from accelerate import Accelerator
kwargs_handlers=[DistributedDataParallelKwargs(find_unused_parameters=True)] # ä¸æ˜¯å¿…é¡»çš„
# Step-1 é¦–å…ˆåˆå§‹åŒ– accelerate
accelerator = Accelerator(mixed_precision= 'fp16', 
                            gradient_accumulation_steps= 2,
                            log_with= ['tensorboard', 'wandb'], # ä¸€èˆ¬æ¥è¯´ç”¨ä¸€ä¸ªå³å¯
                            project_dir=os.path.join(config.output_dir, "logs"),
                            kwargs_handlers= kwargs_handlers
                            )
# ä»…åœ¨ä¸»çº¿ç¨‹ä¸Šåˆ›å»ºæ–‡ä»¶å¤¹
if accelerator.is_main_process: 
    os.makedirs(config.output_dir, exist_ok=True)
    # åˆå§‹åŒ–ä¸€ä¸ªå®žéªŒè®°å½•å™¨ï¼ˆæ­¤å¤„å†…å®¹éœ€è¦æ³¨æ„â­ï¼‰
    # accelerator.init_trackers(f"Train-{config.training}")
    log_name = 'Model-Test'
    accelerator.init_trackers(
        project_name= f"Page-Layout-Analysis-{config.pred_heads}",
        init_kwargs={
            "wandb": {
                "name": log_name,
                "dir": os.path.join(config.output_dir, "logs"),
                "config": vars(config)
            }
        }
        )
 
# Step-2 åˆå§‹åŒ–å®Œæˆä¹‹åŽå¯ä»¥ç›´æŽ¥å°†æˆ‘ä»¬éœ€è¦çš„å†…å®¹é€šè¿‡ accelerator.prepare è¿›è¡Œå¤„ç†
optimizer = torch.optim.AdamW([
        {'params': model.image_model.parameters(), 'lr': 2e-5, 'weight_decay': 1e-4},
        {'params': model.text_model.parameters(), 'lr': 4e-5},
        {'params': [p for n, p in model.named_parameters() 
                    if 'image_model' not in n and 'text_model' not in n], 
        'lr': config.learning_rate, 'weight_decay': 1e-4}, 
    ])
total_steps = config.epochs * len(train_dataloader)
warmup_steps = int(0.15 * total_steps)

# Warmup è°ƒåº¦å™¨ï¼šä»Ž 0.1*lr çº¿æ€§å¢žåŠ åˆ° lr
warmup_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, 
                                                        start_factor=0.1, 
                                                        total_iters=warmup_steps
)

# ä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼šæ·»åŠ  eta_min é˜²æ­¢å­¦ä¹ çŽ‡è¿‡ä½Ž
cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 
                                                                T_max=total_steps - warmup_steps, 
                                                                eta_min=1e-6
)

cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 
                                                                T_max=total_steps - warmup_steps)
lr_scheduler = torch.optim.lr_scheduler.SequentialLR(
    optimizer,
    schedulers=[warmup_scheduler, cosine_scheduler],
    milestones=[warmup_steps] 
)
dataloader, model, optimizer, scheduler = accelerator.prepare(dataloader, model, optimizer, scheduler)

# Step-3 æ¨¡åž‹è®­ç»ƒä»¥åŠæ¨¡åž‹ä¼˜åŒ–
total_data = len(dataloader)
for i, batch in enumerate(dataloader):
    with accelerator.accumulate(model): # æ¢¯åº¦ç´¯è®¡
        inputs, targets = batch

        # ä¸‹é¢ä¸¤å¥å¯ä»¥ä¸ç”¨ï¼Œä½†æ˜¯ä¹ æƒ¯è¿˜æ˜¯ç›´æŽ¥ä½¿ç”¨
        inputs = inputs.to(accelerator.device)
        targets = targets.to(accelerator.device)

        outputs = model(inputs)
        loss = loss_function(outputs, targets)
        accelerator.backward(loss)
        if accelerator.sync_gradients: # è¿›è¡Œæ¢¯åº¦è£å‰ª
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

        # è®°å½•ä¸€ä¸‹å®žéªŒç»“æžœ
        logs = {
                "Train/loss": loss.item(),
                "Train/lr": optimizer.param_groups[0]['lr'], # è¿™é‡Œæ˜¯å‡è®¾æ¨¡åž‹ä½¿ç”¨çš„ä¼˜åŒ–å­¦ä¹ çŽ‡ä¸åŒ æˆ–è€…ç›´æŽ¥ä½¿ç”¨ scheduler.get_last_lr()[0]
                "Train/ACC": acc,
            }
            progress_bar.set_postfix(
                loss=loss.item(),
                acc=acc, f1=f1)
            accelerator.log(logs, step= epoch* total_data+ i)

# Step-3 åŒæ­¥ä¸åŒè¿›ç¨‹
accelerator.wait_for_everyone()
if accelerator.is_main_process:
    model = accelerator.unwrap_model(model)
    model.save_pretrained(os.path.join(args.output_dir, "model"))
accelerator.end_training()
```

ä¸è¿‡å¯¹äºŽä¸Šé¢çš„ä»£ç éœ€è¦æ³¨æ„å¦‚ä¸‹å‡ ä¸ªå†…å®¹
1ã€è¿½è¸ªå™¨ä½¿ç”¨ï¼šä¸€èˆ¬å¤šæ˜¾å¡ä½¿ç”¨è¿‡ç¨‹ä¸­é€šè¿‡ä½¿ç”¨ `accelerator.end_training()` åŽ»ç»“æŸ `tracker`
2ã€tqdmä½¿ç”¨ï¼šä¸€èˆ¬åªéœ€è¦ä¸»è¿›ç¨‹è¿›è¡Œæ˜¾ç¤ºè¿›åº¦æ¡ï¼Œå› æ­¤ä¸€èˆ¬ç›´æŽ¥ï¼š`tqdm(..., disable=not accelerator.is_local_main_process)`

## diffuser
> æŽ¨èç›´æŽ¥é˜…è¯»å®˜æ–¹æ–‡æ¡£ï¼š[https://huggingface.co/docs/diffusers/main/en/index](https://huggingface.co/docs/diffusers/main/en/index)
> [`pip install git+https://github.com/huggingface/diffusers`](https://huggingface.co/docs/diffusers/main/en/installation?install=Python)

https://github.com/huggingface/diffusers/blob/main/src/diffusers/schedulers/scheduling_ddim.py#L342