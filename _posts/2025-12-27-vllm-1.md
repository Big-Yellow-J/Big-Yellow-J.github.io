---
layout: mypost
title: æ¨¡å‹æ¨ç†æ¡†æ¶â€”â€”vllmåŸç†åŠæ•´ä½“æ¡†æ¶
categories: é‡åŒ–éƒ¨ç½²
address: æ­¦æ±‰ğŸ¯
extMath: true
show_footer_image: true
tags:
- æ¨¡å‹æ¨ç†
- æ¨¡å‹éƒ¨ç½²
description: Page Attentionæ˜¯ä¼˜åŒ–KV-cacheå†…å­˜ç®¡ç†çš„æ–¹æ³•ï¼Œå¯è§£å†³é¢„ç•™æµªè´¹ã€å†…éƒ¨åŠå¤–éƒ¨å†…å­˜ç¢ç‰‡åŒ–é—®é¢˜ï¼Œé€šè¿‡å°†KV-cacheåˆ’åˆ†ä¸ºå›ºå®šå¤§å°Blockï¼Œåˆ©ç”¨Block
  Tableç»´æŠ¤é€»è¾‘ä¸ç‰©ç†æ˜ å°„ï¼Œæœ‰æ•ˆç®¡ç†éè¿ç»­å†…å­˜ï¼›åŒæ—¶å¤„ç†Softmaxè®¡ç®—ä¸­çš„æ•°å€¼æº¢å‡ºä¸å…¨å±€ä¿¡æ¯é—®é¢˜ã€‚vllmæ¡†æ¶æ”¯æŒç¦»çº¿ä¸åœ¨çº¿è°ƒç”¨ï¼Œåˆå§‹åŒ–è¿‡ç¨‹åŒ…æ‹¬æ¨¡å‹åŠ è½½ã€KV
  Cacheæ˜¾å­˜é¢„åˆ†é…ï¼ˆè®¡ç®—å¯ç”¨å†…å­˜ã€num_blocksåŠæ€»tokenæ•°ï¼‰å¹¶åŠ è½½åˆ°GPUï¼Œæå‡å†…å­˜ä½¿ç”¨æ•ˆç‡ã€‚
---
## PageAttentionåŸç†åˆ†æ
`Page Attention`ä¹Ÿæ˜¯ä¸€ç§ä¼˜åŒ–æ–¹æ³•ï¼ˆåŒºåˆ«äº`MLA`ï¼Œ`page attention`æ˜¯å¯¹å†…å­˜è¿›è¡Œåˆ†é…ç®¡ç†ï¼‰ã€‚å‚è€ƒè®ºæ–‡[^1]ä¸­æè¿°ï¼Œå¯¹äº`KV-cache`å­˜åœ¨3ä¸ªé—®é¢˜ï¼š
![](https://s2.loli.net/2025/06/21/9QpfhleHvRPxLmW.webp)
1ã€**é¢„ç•™æµªè´¹ (Reserved)**ï¼šä¸ºå°†æ¥å¯èƒ½çš„ token é¢„ç•™çš„ç©ºé—´ï¼Œè¿™äº›ç©ºé—´è¢«ä¿ç•™ä½†æš‚æœªä½¿ç”¨ï¼Œå…¶ä»–è¯·æ±‚æ— æ³•ä½¿ç”¨è¿™äº›é¢„ç•™ç©ºé—´ï¼›
2ã€**å†…éƒ¨å†…å­˜ç¢ç‰‡åŒ–é—®é¢˜ï¼ˆinternal memory fragmentationï¼‰**ï¼šç³»ç»Ÿä¼šä¸ºæ¯ä¸ªè¯·æ±‚é¢„å…ˆåˆ†é…ä¸€å—è¿ç»­çš„å†…å­˜ç©ºé—´ï¼Œå¤§å°åŸºäºæœ€å¤§å¯èƒ½é•¿åº¦(æ¯”å¦‚2048ä¸ªtoken)ï¼Œä½†å®é™…è¯·æ±‚é•¿åº¦å¾€å¾€è¿œå°äºæœ€å¤§é•¿åº¦ï¼Œè¿™å¯¼è‡´é¢„åˆ†é…çš„å†…å­˜æœ‰å¤§é‡ç©ºé—´è¢«æµªè´¹ã€‚
3ã€**å¤–éƒ¨å†…å­˜ç¢ç‰‡åŒ–é—®é¢˜ï¼ˆexternal memory fragmentationï¼‰**ï¼šä¸åŒå†…å­˜å—ä¹‹é—´çš„é›¶æ•£ç©ºé—²ç©ºé—´ï¼Œè™½ç„¶æ€»ç©ºé—²ç©ºé—´è¶³å¤Ÿï¼Œä½†å› ä¸è¿ç»­è€Œéš¾ä»¥ä½¿ç”¨ã€‚
![](https://s2.loli.net/2025/06/21/ryt7kgaGZSw32HN.webp)
åªæœ‰ **20.4%-38.2%** çš„tokenæ˜¯è¢«ä½¿ç”¨çš„ï¼Œå¤§éƒ¨åˆ†éƒ½è¢«æµªè´¹æ‰äº†ã€‚`Page Attention`å…è®¸åœ¨éè¿ç»­çš„å†…å­˜ç©ºé—´ä¸­å­˜å‚¨è¿ç»­çš„ key å’Œ value ã€‚å…·ä½“æ¥è¯´ï¼Œ`Page Attention`å°†æ¯ä¸ªåºåˆ—çš„ `KV-cache` åˆ’åˆ†ä¸ºå—ï¼Œæ¯ä¸ªå—åŒ…å«å›ºå®šæ•°é‡ token çš„é”®å’Œå€¼ã€‚åœ¨æ³¨æ„åŠ›è®¡ç®—æœŸé—´ï¼Œ`Page Attention`å†…æ ¸å¯ä»¥æœ‰æ•ˆåœ°è¯†åˆ«å’Œè·å–è¿™äº›å—ã€‚å¦‚ä½•ç†è§£ä¸Šé¢æè¿°å‘¢ï¼Ÿè¿˜æ˜¯å€Ÿç”¨è®ºæ–‡ä¸­çš„æè¿°ï¼š
![](https://s2.loli.net/2025/06/21/sZ1uOlYStP3ehDb.webp)
æ¯”å¦‚è¯´æŒ‰ç…§ä¸Šé¢Promptè¦è¾“å‡ºï¼ˆå‡è®¾åªè¾“å‡ºè¿™äº›å†…å®¹ï¼‰ï¼šâ€œfathers brought a carâ€ï¼Œä¸€èˆ¬çš„å¥—è·¯å¯èƒ½æ˜¯ï¼šæ¯”å¦‚è¯´ï¼šâ€œFour score and seven years ago our xxxxxâ€ï¼ˆxxxä»£è¡¨é¢„ç•™ç©ºé—´ï¼‰å› ä¸ºå®é™…ä¸çŸ¥é“åˆ°åº•è¦è¾“å‡ºå¤šå°‘æ–‡æœ¬ï¼Œå› æ­¤ä¼šæå‰é¢„ç•™å¾ˆé•¿çš„ä¸€éƒ¨åˆ†ç©ºé—´ï¼ˆä½†æ˜¯å¦‚æœåªè¾“å‡º4ä¸ªå­—ç¬¦ï¼Œè¿™é¢„ç•™ç©ºé—´å°±è¢«æµªè´¹äº†ï¼‰ï¼Œå› æ­¤åœ¨`page attention`é‡Œé¢å°±åˆ°ç”¨ä¸€ç§â€œåˆ†å—â€çš„æ€æƒ³å¤„ç†ï¼Œä»¥ä¸Šå›¾ä¸ºä¾‹ï¼Œåˆ†ä¸º8ä¸ªBlockæ¯ä¸ªBlockåªèƒ½å­˜å‚¨4ä¸ªå†…å®¹ï¼Œå› æ­¤å°±å¯ä»¥é€šè¿‡ä¸€ä¸ª`Block Table`æ¥å»ºç«‹ä¸€ä¸ªè¡¨æ ¼å‘Šè¯‰é‚£äº›Blockå­˜å‚¨äº†å¤šå°‘ï¼Œå­˜å‚¨æ»¡äº†å°±å»å…¶ä»–Blobkç»§ç»­å­˜å‚¨ã€‚æ•´ä¸ªè¿‡ç¨‹å¦‚ä¸‹ï¼š
![](https://s2.loli.net/2025/02/02/3lWpNMUQyLojhP9.webp)
ä¸Šè¿°è¿‡ç¨‹æè¿°å¦‚ä¸‹ï¼šå…·ä½“è€Œè¨€ï¼ŒPage Attention é¦–å…ˆå°† Key/Value çš„è¿ç»­æ˜¾å­˜ç©ºé—´åˆ’åˆ†ä¸ºå›ºå®šå¤§å°çš„ Blockï¼ˆé¡µï¼‰ï¼Œæ¯ä¸ª Block ä½œä¸ºæœ€å°çš„å†…å­˜åˆ†é…ä¸è°ƒåº¦å•å…ƒã€‚éšåï¼Œå¼•å…¥ä¸€ä¸ª Block Tableï¼ˆé¡µè¡¨ï¼‰ æ¥ç»´æŠ¤é€»è¾‘åºåˆ—ä½ç½®ä¸ç‰©ç† Block ä¹‹é—´çš„æ˜ å°„å…³ç³»ï¼Œç”¨äºè®°å½•æ¯ä¸ª Block å½“å‰çš„å­˜å‚¨çŠ¶æ€ä¸å¯ç”¨å®¹é‡ã€‚
**ä¸€ä¸ªå°é—®é¢˜**ï¼šåˆ†å—ä¹‹åæ³¨æ„åŠ›è®¡ç®—è¿‡ç¨‹ï¼Œå› ä¸ºæˆ‘çš„KVè¢«å­˜å‚¨åœ¨ä¸åŒçš„blockä¸­ï¼Œç”±äºBlock tableå­˜åœ¨å¯ä»¥ç›´æ¥å»ç´¢å¼•ä¸åŒBlcokä¸­KVå€¼ï¼Œè¿™æ ·ä¸€æ¥å¯¹äºQã€Kã€Vä¸‰è€…è®¡ç®—ä¸æˆé—®é¢˜ï¼Œä¸è¿‡å…³é”®é—®é¢˜å°±æ˜¯ï¼šSoftmax çš„åˆ†æ¯éœ€è¦å…¨å±€ä¿¡æ¯ï¼ŒBlock ï¼ˆä¸ç®¡æ˜¯Flash Attnè¿˜æ˜¯Page Attnéƒ½éœ€è¦é¢å¯¹è¿™ä¸ªé—®é¢˜ï¼‰æ˜¯åˆ†å¼€çš„ï¼Œæ€ä¹ˆåŠï¼Ÿ
åœ¨softmaxè®¡ç®—è¿‡ç¨‹ä¸­ï¼š$\sigma= \frac{e^{z_i}}{\sum_{j=1}^K e^{z_j}}$ ç”±äºåˆ†å—å¯èƒ½å¯¼è‡´å€¼è¿‡å°è¿›è€Œå¯¼è‡´æ•°å€¼æº¢å‡ºé—®é¢˜ï¼Œé™¤æ­¤ä¹‹å¤–è®¡ç®—éœ€è¦æ‰€æœ‰tokençš„åˆ†æ•°ä¸€èµ·å½’ä¸€åŒ–ï¼Œå› æ­¤é¦–å…ˆä¼šå¯¹ä¸Šé¢çš„å…¬å¼æ”¹è¿›ä¸ºï¼š$\sigma= \frac{e^{z_i-m}}{\sum_{j=1}^K e^{z_j-m}}$ä¹Ÿå°±æ˜¯å°†æ¯å—éƒ½å»å‡å»å½“å‰çš„æœ€å¤§å€¼ï¼ˆ**é¿å…æº¢å‡ºé—®é¢˜**ï¼‰ã€‚**åœ¨å¤„ç†å…¨å±€é—®é¢˜ä¸Š**ï¼šåªéœ€è¦è€ƒè™‘ä¸¤ä¸ªå€¼çš„æ›´æ–°ï¼š1ã€å½“å‰æœ€å¤§å€¼ï¼›2ã€å½’ä¸€åŒ–å› å­ï¼ˆ$\sum_{j=1}^K e^{z_j-m}$ï¼‰å› æ­¤è¿™ä¸ªè¿‡ç¨‹å°±å¯ä»¥å¤„ç†ä¸ºï¼š

$$
l_{t+1}=\sum_{i\in B_{â‰¤t+1}} e^{z_j- m_{t+1}}=\sum_{i\in B_{â‰¤t}}e^{z_j- m_{t+1}}+ \sum_{i\in B_{t+1}}e^{z_j- m_{t+1}}\\ =\sum_{i\in B_{â‰¤t}}e^{z_j- m_{t}} e^{m_t-m_{t+1}}+ \sum_{i\in B_{t+1}}e^{z_j- m_{t+1}}
$$

è¿™æ ·ä¸€æ¥å°±å¯ä»¥è½¬åŒ–ä¸ºï¼š$l_{t+1}=l_t e^{m_t-m_{t+1}}+ \sum_{i\in B_{t+1}}e^{z_j- m_{t+1}}$
## åŸºæœ¬ä½¿ç”¨æ–¹å¼
åœ¨ä½¿ç”¨vllmä¸Šæœ‰ä¸¤ç§æ–¹å¼ï¼š1ã€ç¦»çº¿ä½¿ç”¨ï¼›2ã€åœ¨çº¿ä½¿ç”¨ï¼ˆç›´æ¥å°†ä½¿ç”¨è¿‡ç¨‹è½¬åŒ–ä¸ºè°ƒç”¨APIæ–¹å¼ï¼‰ï¼š
```python
from vllm import LLM, SamplingParams
prompts = ["Hello, my name is",
           "The president of the United States is",
           "The capital of France is",
           "The future of AI is",]
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
llm = LLM(model="facebook/opt-125m")
outputs = llm.generate(prompts, sampling_params)
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

## vllmæ•´ä½“æ¡†æ¶åˆ†æ
> åŸºäºï¼š`Version: 0.11.0`

åœ¨vllmä¸­ä¸»è¦æ˜¯ä¸¤ç§è°ƒç”¨æ–¹å¼ï¼š1ã€ç¦»çº¿è°ƒç”¨ï¼›2ã€åœ¨çº¿è°ƒç”¨ï¼ˆè¿™ä¸ªå°±ç±»ä¼¼åœ¨æœ¬åœ°å¯åŠ¨ä¸€ä¸ªæœåŠ¡ï¼Œè€Œåå…¶ä»–åŠå…¶ç›´æ¥è®¿é—®ipç«¯å£ç­‰è¿›è¡Œè®¿é—®å¤„ç†ï¼‰
![](https://s2.loli.net/2026/01/15/aDyb2iSmqhuPInK.webp)
ä¸Šå›¾ä¸­åœ¨çº¿è°ƒç”¨æ–¹å¼ï¼ˆAsyï¼‰å’Œç¦»çº¿è°ƒç”¨ï¼ˆSynï¼‰
> å¯¹äºå…·ä½“çš„`LLMEngine`çš„ç»“æ„æè¿°è§åé¢çš„æè¿°

ä»¥ç¦»çº¿è°ƒç”¨æ–¹å¼è¿›è¡Œè§£é‡Šï¼Œç›´æ¥ä½¿ç”¨å®˜æ–¹ä»£ç ä¸ºä¾‹ï¼š
```python
from vllm import LLM, SamplingParams
prompts = ["Hello, my name is",
           "The president of the United States is",
           "The capital of France is",
           "The future of AI is",]
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)
llm = LLM(model="facebook/opt-125m")
outputs = llm.generate(prompts, sampling_params)
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```
ä»ä¸Šé¢ä»£ç åˆ†æå‘ç°æ„Ÿè§‰å’Œå¹³æ—¶ä½¿ç”¨Transformeræ¡†æ¶å’Œç›¸ä¼¼ï¼šåŠ è½½æ¨¡å‹-->ç¼–ç è¾“å…¥-->è¾“å…¥æ¨¡å‹-->æ¨¡å‹è¾“å‡ºå¹¶ä¸”è§£ç ã€‚å·®å¼‚åœ¨äºä½¿ç”¨vllmé¦–å…ˆä¼šä½¿ç”¨ä¸€ä¸ª`LLM`å»å¤„ç†ä½ çš„æ¨¡å‹ï¼Œè€Œåä½ å…¶ä»–çš„æ–¹å¼éƒ½æ˜¯åœ¨è¿™ä¸ª`LLM`ä¸­ï¼Œå› æ­¤äº†è§£ä¸€ä¸‹åœ¨æ¨¡å‹æ¥å—åˆ°æˆ‘çš„`prompt`ä¹‹å‰æ¨¡å‹éƒ½åœ¨åšä»€ä¹ˆã€‚
### vllmåˆå§‹åŒ–è¿‡ç¨‹
æŒ‰ç…§PPT[^2]ä¸­å¯¹äºæ¨¡å‹åŠ è½½çš„æè¿°ï¼š
![](https://s2.loli.net/2026/01/15/ynDbOIQ5zJxHrtZ.webp)
åœ¨æ¨¡å‹è¿›è¡Œè¾“å‡ºä¹‹å‰ä¸»è¦æ˜¯è¿›è¡Œ3æ­¥ï¼š1ã€åˆå§‹åŒ–å¹¶ä¸”åŠ è½½æ¨¡å‹ï¼›2ã€é¢„åˆ†é…æ˜¾å­˜è¿‡ç¨‹ï¼›3ã€å°†é¢„åˆ†é…çš„KV CacheåŠ è½½åˆ°gpuä¸Šã€‚
#### æ¨¡å‹åˆå§‹åŒ–è¿‡ç¨‹
åœ¨vllmä¸­å®šä¹‰ä¸€ä¸ªllmè¿‡ç¨‹ä¸ºï¼š
```python
# vllm/entrypoints/llm.py
class LLM:
    ...
    self.llm_engine = LLMEngine.from_engine_args(...)

# vllm/v1/engine/llm_engine.py
class LLMEngine:
    def __init__(...):
      self.engine_core = EngineCoreClient.make_client(...)
    def generate(...):
        ...
    def add_request(...):
        ...
# /vllm/v1/engine/core_client.py ä¸­ EngineCoreClienté€šè¿‡å¤šç§ï¼ˆå¼‚æ­¥/å¤šè¿›ç¨‹ï¼Œè¿™ä¹Ÿå°±æ„å‘³è¿™åœ¨linuxæœ‰äº›å¯èƒ½éœ€è¦ä½¿ç”¨`multiprocessing.set_start_method('spawn', force=True)`ï¼‰æ–¹å¼è¿›è¡ŒåŠ è½½æ¨¡å‹
```
åœ¨[LLMEngineä»£ç ](https://github.com/vllm-project/vllm/blob/8711b216766bb5d3cbe15161061c3a7d9fffe59c/vllm/v1/engine/llm_engine.py#L46)ä¸­å®šä¹‰äº†åŸºæœ¬æ‰€æœ‰å‡½æ•°åŠŸèƒ½ï¼Œå¦‚ç”Ÿæˆç­‰ï¼ˆ**åç»­è§£é‡Šå…·ä½“è¿‡ç¨‹**ï¼‰ã€‚
#### é¢„åˆ†é…æ˜¾å­˜è¿‡ç¨‹
è¿™ä¸ªç»™è¿‡ç¨‹çš„çš„è¯é¦–å…ˆæ˜¯å»è®¡ç®—é¢„åˆ†é…çš„KV Cacheå¤§å°ï¼Œè€Œåå°†é¢„åˆ†é…çš„KV CacheåŠ è½½ï¼ˆä¸€èˆ¬å°±æ˜¯åˆå§‹åŒ–ä¸º0çš„å‘é‡ï¼‰åˆ°gpuä¸Š
* **è®¡ç®—é¢„åˆ†é…çš„KV Cache**

> è®¡ç®—é¢„åˆ†é…çš„KV Cache[^3]ï¼š**å¯ç”¨æ˜¾å­˜å¤§å°Ã—é¢„åˆ†é…vllmæ¯”ç‡- ékv cacheå ç”¨å¤§å°**å¾—åˆ°kv cacheçš„å¯ç”¨ï¼ˆå­—èŠ‚ï¼‰å¤§å°ï¼Œè€Œåé€šè¿‡æ€»å…±å¯ç”¨å¤§å°è®¡ç®—å¯ç”¨åˆ†å¤šå°‘ä¸ªblockï¼š**å¯åˆ†é…å¤§å°//KV cache block çš„å­—èŠ‚å¤§å°//æ‰€æœ‰ kv_cache_groupsä¸­å±‚æ•°çš„æœ€å¤§å€¼**

![](https://s2.loli.net/2026/01/15/qDBNMLpzWOFkVHG.webp)
åœ¨è°ƒç”¨ä»£ç `LLM(model="facebook/opt-125m")`å®é™…è¿‡ç¨‹ä¸­ä¼šä½¿ç”¨`load_model`è¿›è¡Œæ¨¡å‹åŠ è½½ï¼ˆä»£ç ï¼š`vllm/v1/worker/gpu_model_runner.py`ï¼‰åœ¨åŠ è½½æ¨¡å‹ä¹‹åï¼Œæ¨¡å‹ä¼šè¿›è¡Œä¸€ä¸ªæ˜¾å­˜çš„é¢„åˆ†é…å¤„ç†ï¼Œè¿™ä¸ªè¿‡ç¨‹ï¼ˆä»£ç ï¼š`vllm/v1/core/kv_cache_utils.py`ï¼‰æè¿°å¦‚ä¸‹ï¼š
1ã€è®¡ç®—éœ€è¦åˆ†é…å¤šå°‘æ˜¾å­˜ç»™vllmï¼š**å¯ç”¨æ˜¾å­˜å¤§å°*åˆå§‹åŒ–åˆ†é…å¤§å°**ï¼ˆ`self.requested_memory=self.init_snapshot.total_memory * self.cache_config.gpu_memory_utilization`ï¼Œæ¯”è¯´24Gï¼ˆå®é™…å¯èƒ½æ¯”24Gè¦å°ï¼Œå› ä¸ºè¿˜æœ‰æ¨¡å‹å ç”¨ï¼‰æ˜¾å¡é‚£ä¹ˆçš„ç¬¬ä¸€é¡¹ç»“æœå°±æ˜¯ï¼š24\*1024^3ï¼Œåé¢ä¸€ä¸‹å°±æ˜¯æœ€å¼€å§‹çš„å‚æ•°ï¼‰
2ã€è®¡ç®—åˆ†é…ç»™kv cacheçš„æ˜¾å­˜å ç”¨å­—èŠ‚å¤§å°ï¼š**å¯ä»¥æ˜¾å­˜å¤§å°-é™¤å»KV cacheæ˜¾å­˜å¤–å…¶ä»–å¤§å°**ï¼ˆ`self.available_kv_cache_memory_bytes = self.requested_memory - profile_result.non_kv_cache_memory`ï¼‰
åœ¨è®¡ç®—å®Œæ¯•ä¹‹åï¼ˆä»¥ä¸Šé¢æ¨¡å‹åŠ è½½ä¸ºä¾‹ï¼Œå¾—åˆ°KV cacheå¤§å°ä¸ºï¼š20.44GiBï¼‰æ¥ä¸‹æ¥å°±æ˜¯è®¡ç®—GPUä¸Š KV Cache å†…**æ€»tokenæ•°é‡**ï¼š`num_tokens = num_blocks // len(kv_cache_groups) * min_block_size`
1ã€`num_blocks`è®¡ç®—è¿‡ç¨‹ï¼š`int(available_memory // page_size // num_layers)`ï¼Œå…¶ä¸­`page_size`ä»£è¡¨æ˜¯ä¸€ä¸ª KV cache block çš„å­—èŠ‚å¤§å°ï¼ˆ`page_size` = 2(K+V) * 16(block_size) * 12(num_kv_heads) * 64(head_size) * 2(dtype_bytes å…¶ä¸­fp16å¯¹åº”2)=49152ï¼Œé‡Œé¢num_kv_headså¯¹åº”ä½ çš„æ¨¡å‹ç»“æ„ä½¿ç”¨æ•°é‡ï¼‰ï¼›`num_layers`ï¼šæ‰€æœ‰ kv_cache_groupsä¸­å±‚æ•°çš„æœ€å¤§å€¼ï¼Œæ¯”å¦‚è¯´åœ¨æ¨¡å‹facebook/opt-125mä¸­æ€»å…±æœ‰12å±‚decodeï¼ˆå³ 12 å±‚è¿›è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼‰å¹¶ä¸”è¿™äº›attnè®¡ç®—æ–¹å¼å®Œå…¨ç›¸åŒé‚£ä¹ˆå°±æ˜¯1ä¸ªgroupåˆ†ç»„ï¼ˆå¦‚æœè¿˜æœ‰å…¶ä»–attné‚£ä¹ˆå¯èƒ½å°±æ˜¯å¤šä¸ªgroupä½†æ˜¯æœ€åè¿˜æ˜¯å–æœ€å¤§å€¼ï¼š`group_size = max(len(group.layer_names) for group in kv_cache_groups)`ã€‚æœ€åè®¡ç®—å¾—åˆ°ç»“æœä¸ºï¼šnum_blocks = 21946158284//49152// 12=37207
2ã€`min_block_size = min([group.kv_cache_spec.block_size for group in kv_cache_groups])` è®¡ç®—å¾—åˆ°:16ã€‚
> å®é™…è°ƒè¯•è¿‡ç¨‹ä¸­ï¼ˆç›´æ¥åœ¨éœ€è¦è°ƒè¯•ä½ç½®ä½¿ç”¨`logger.info`ï¼‰ï¼Œè¾“å‡º`kv_cache_groups`çœ‹åˆ°çš„æ¯”å¦‚ï¼ˆå…¶å®è¿™ä¸ªå‚æ•°ä¹Ÿå°±æ˜¯è®°å½•cacheéœ€è¦å‘ç”Ÿä½ç½®ï¼Œä¸€èˆ¬å°±æ˜¯attnè®¡ç®—ï¼Œä¸è¿‡å¯èƒ½å¯¹äºä¸åŒçš„attnå­˜åœ¨å·®å¼‚ï¼Œæœ‰äº›æ˜¯å¸¸è§„æœ‰äº›æœ‰å¯èƒ½æ˜¯window-attnç­‰ï¼‰ï¼š`[KVCacheGroupSpec(layer_names=['model.decoder.layers.0.self_attn.attn', ..., 'model.decoder.layers.11.self_attn.attn'], kv_cache_spec=FullAttentionSpec(block_size=16, num_kv_heads=12, head_size=64, dtype=torch.float16, sliding_window=None, attention_chunk_size=None))]` é™¤æ­¤ä¹‹å¤–è¿™éƒ¨åˆ†ç»“æœä¼šç›´æ¥å­˜å…¥`KVCacheConfig`ä¸­ã€‚åœ¨åç»­ä»£ç ï¼ˆ`vllm/v1/worker/gpu_model_runner.py`ï¼‰ä¸­å¯¹äº`initialize_kv_cache`ï¼ˆå…·ä½“è§£é‡Šè§ä¸‹é¢ï¼‰è¿˜ä¼šä¸ºæ¯ä¸€å—`model.decoder.layers.0.self_attn.attn`å–åˆ†é…ä¸€ä¸ªåˆå§‹åŒ–ï¼ˆå…·ä½“å‡½æ•°ï¼š`initialize_kv_cache_tensors`ï¼‰ä¸º0çš„å‘é‡å¤§å°ä¸ºï¼š[2, num_blocks, block_size, num_kv_heads, head_size]

å› æ­¤æœ€åå°±å¯ä»¥ç›´æ¥å¾—åˆ°ï¼šnum_tokens = 37207// 1*16 = 595,312ã€‚
* **å°†é¢„åˆ†é…çš„KV CacheåŠ è½½åˆ°gpuä¸Š**

![](https://s2.loli.net/2026/01/15/kQK5LmEcjA4VeUJ.webp)
åœ¨ä¸Šè¿°æ­¥éª¤ä¸­è®¡ç®—å¾—åˆ°äº†é¢„åˆ†é…çš„KV cacheå¤§å°ä»¥åŠnum blocksï¼Œæ¥ä¸‹æ¥å°±æ˜¯ç›´æ¥å°†å…¶å…ˆæ”¾ç½®åˆ°gpuä¸Šï¼Œå®ç°æ˜¾å­˜çš„é¢„åˆ†é…ï¼Œä»¥åè¿™å—æ˜¾å­˜å°±æ˜¯ä¸“é—¨ç”¨æ¥åšKV Cacheã€‚å…·ä½“è¿‡ç¨‹ä¸­è¿˜æ˜¯ä½¿ç”¨ä¸Šé¢å¾—åˆ°çš„`kv_cache_groups`è¿™ä¸ªå‚æ•°
```python
# vllm/v1/worker/gpu_model_runner.py
def initialize_kv_cache_tensors(self, kv_cache_config: KVCacheConfig):
    # Initialize the memory buffer for KV cache
    kv_cache_raw_tensors = self._allocate_kv_cache_tensors(kv_cache_config)
    # Change the memory buffer to the desired shape
    kv_caches = self._reshape_kv_cache_tensors(kv_cache_config, kv_cache_raw_tensors)
    ...
    num_attn_module = 2 if self.model_config.hf_config.model_type == "longcat_flash" else 1
    bind_kv_cache(kv_caches,
                  self.compilation_config.static_forward_context,
                  self.kv_caches, num_attn_module)
    return kv_caches

def _allocate_kv_cache_tensors(self, kv_cache_config: KVCacheConfig):
    kv_cache_raw_tensors: dict[str, torch.Tensor] = {}
    logger.info(kv_cache_config)
    for kv_cache_tensor in kv_cache_config.kv_cache_tensors:
        tensor = torch.zeros(kv_cache_tensor.size,
                             dtype=torch.int8,
                             device=self.device)
        for layer_name in kv_cache_tensor.shared_by:
            kv_cache_raw_tensors[layer_name] = tensor
    ...
    return kv_cache_raw_tensors

```
> å¯¹äºå‚æ•°`KVCacheConfig`å°±æ˜¯ä¸Šé¢çš„`kv_cache_groups`ç»“æœï¼Œåªä¸è¿‡è¿˜ä¼šå–è®¡ç®—æ¯å±‚çš„å¤§å°ä¹Ÿå°±æ˜¯ä¼šæ›´æ–°ä¸ºï¼š`KVCacheConfig(num_blocks=37207, kv_cache_tensors=[KVCacheTensor(size=1828798464, shared_by=['model.decoder.layers.0.self_attn.attn']), ..., KVCacheTensor(size=1828798464, shared_by=['model.decoder.layers.11.self_attn.attn'])], kv_cache_groups=[KVCacheGroupSpec(layer_names=['model.decoder.layers.0.self_attn.attn', ...,'model.decoder.layers.10.self_attn.attn', 'model.decoder.layers.11.self_attn.attn'], kv_cache_spec=FullAttentionSpec(block_size=16, num_kv_heads=12, head_size=64, dtype=torch.float16, sliding_window=None, attention_chunk_size=None))])`

åœ¨å‡½æ•°`self._allocate_kv_cache_tensors`ä¸­å¾ˆå®¹æ˜“ç†è§£ç›´æ¥åˆå§‹åŒ–ä¸€ä¸ªå…¨éƒ¨ä¸º0çš„å¼ é‡ï¼Œè€Œåå†å»é€šè¿‡å‡½æ•°`_reshape_kv_cache_tensors`å°†å¼ é‡çš„å½¢çŠ¶æ”¹ä¸º`[num_blocks, block_size, num_kv_heads, head_size]`
## æ€»ç»“
æœ¬æ–‡ä¸»è¦æ˜¯ç®€å•ä»‹ç»äº†ä¸€äº›vllmçš„æ˜¾å­˜åˆ†é…è¿‡ç¨‹ä¸­ï¼Œé¢„åˆ†é…æ˜¾å­˜æ¯”è¾ƒç®€å•:**å¯ç”¨æ˜¾å­˜å¤§å°Ã—é¢„åˆ†é…vllmæ¯”ç‡- ékv cacheå ç”¨å¤§å°**ï¼Œè€Œåå°±æ˜¯ç›´æ¥å»è®¡ç®—KV Cacheä¸­æ€»tokenæ•°é‡è¿™éƒ¨åˆ†è®¡ç®—è¿‡ç¨‹æ˜¯ï¼š`num_tokens = num_blocks // len(kv_cache_groups) * min_block_size`ã€‚
## å‚è€ƒ
[^1]: [https://dl.acm.org/doi/pdf/10.1145/3600006.3613165](https://dl.acm.org/doi/pdf/10.1145/3600006.3613165)
[^2]: [vllm-ppt](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?pli=1&slide=id.g24ad94a0065_0_162#slide=id.g24ad94a0065_0_162)
[^3]: [https://zhuanlan.zhihu.com/p/691045737](https://zhuanlan.zhihu.com/p/691045737)