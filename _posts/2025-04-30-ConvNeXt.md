---
layout: mypost
title: CV中常用Backbone-2：ConvNeXt模型详解
categories: Backbone
extMath: true
images: true
address: changsha
show_footer_image: true
tags: [cv-backbone,卷积网络]
description: 本文主要介绍A ConvNet for the 2020s和ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders
---
之前介绍了CV常用Backbon：
[CV中常用Backbone-1：Resnet/Unet/Vit系列/多模态系列等)以及代码](https://www.big-yellow-j.top/posts/2025/01/18/CV-Backbone.html)
这里介绍新的一个Backbone：ConvNeXt，主要来自两篇比较老的来自Meta论文：
1、《**A ConvNet for the 2020s**》
> arXiv:2201.03545

2、《**ConvNeXt V2: Co-designing and Scaling ConvNets with Masked Autoencoders**》
> arXiv:2301.00808
两篇论文讲的都是一个模型：*ConvNeXt*。Vit模型盛行的条件下使用卷积可以说是卷积网络又一春。

## ConvNeXt v1
> A ConvNet for the 2020s
> ⚙-官方代码：[https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py](https://github.com/facebookresearch/ConvNeXt/blob/main/models/convnext.py)
> ⚙-自己修改

![](https://s2.loli.net/2025/04/30/Lc1QrH4UO8EkBPN.png)
值得注意的一点是在 *ConvNeXt* 其实就是一个大型的模型调参（不断调节网络参数取得不错效果，于此同时作者对于模型为什么这么做也都是：对比其他模型做法而后而后借鉴到自己做法中），首先作者在论文中做了如下的一些对比（和采用 swin-transformer的resnet进行对比）：
![](https://s2.loli.net/2025/04/30/4LSIF9YiveZ8kR1.png)

模型在改进上主要是如下几点：1、macro design；2、 ResNeXt；3、inverted bottleneck；4、large kernel size；5、various layer-wise micro designs。
* **1、Macro design**
> 这一点主要是对模型的参数结构做了调整在准确率的提升上起到的效果还是比较有限的

在这里作者主要是做了如下几点修改：1、**修改堆叠数量**。将ResNet-50中的block堆叠数量从：$(3,4,6,3)$ 改为：$(3,3,9,3)$。之所以这样设计作者对比Swin Transformers中主要的比率为：$(1,1,9,1)$ 通过这样调整对于准确率提升还是比较有限的（78.8%-->79.4%），resnet中堆叠数量
![](https://s2.loli.net/2025/04/30/2eyJKI645LubWlm.png)
2、**修改卷积核**。这点没有过多解释直接使用：步长为4，大小也为4的卷积操作（这里是因为：在Vit网络架构中通常使用一个步长为4，大小也为4的卷积），准确率有79.4%-->79.5%
除此之外作者还有一点修改就是将最初的通道数由64调整成96和Swin Transformer保持一致，准确率：80.5%

* **2、Inverted bottleneck**
> 上面第1点是做模型宏观参数（卷积核大小等）做修改，而在这里作者做得主要修改网络结构顺序

![](https://s2.loli.net/2025/04/30/Sgo7D5ptwfsFKJb.png)
> a：resnet；b：MobileNetV2；c：ConvNeXt

这里作者给出的解释是：在Vit中的MLP做的处理和上图中的（b）操作很相像（代码：[⚙](https://www.big-yellow-j.top/code/CVBackbone/Vit.py.txt)）
```python
...
self.linear1 = nn.Linear(embed_dim, dim_feedforward)
self.dropout = nn.Dropout(dropout)
self.linear2 = nn.Linear(dim_feedforward, embed_dim)
...
src2 = self.linear2(self.dropout(F.relu(self.linear1(src2))))
```
因此作者给出的做法是：先降低后提高。在较小的模型上准确率由80.5%提升到了80.6%，在较大的模型上准确率由81.9%提升到82.6%。

* **3、Large Kernel Sizes**
> 换成更加大的卷积核操作

这里就比较简单直接将最开始的3x3卷积核改为7x7卷积核，它将模型的准确率提升至80.6%

*  **4、Micro Design**

**激活函数替换**：将Relu改为GELU（对结果影响不是很大）；
**减少激活函数**：之前网络结构可能对每一个卷积处理之后都会使用一个激活函数处理，这里的话只在 两个 $1\times1$ 卷积后面添加一个激活函数进行处理；
**减少归一化层**：因此在ConvNeXt中也使用了更少的归一化操作，它仅在第一个$1\times1$卷积之前添加了一个BN
**替换归一化层**：像之前的描述最开始在卷积网络中都是用BN作为归一化层，这里作者使用LN也取得不错效果；
**拆分采样层**：在残差网络中，它通常使用的是步长为 2的3x3卷积或者1x1卷积来进行降采样，这使得降采样层和其它层保持了基本相同的计算策略。但是Swin Transformer将降采样层从其它运算中剥离开来，即使用一个步长为2的2x2卷积插入到不同的Stage之间。ConvNeXt也是采用了这个策略，而且在降采样前后各加入了一个LN，而且在全局均值池化之后也加入了一个LN，这些归一化用来保持模型的稳定性。这个策略将模型的准确率提升至82.0%
![](https://s2.loli.net/2025/04/30/BAhd4MrF81iQURN.png)

总的来说这篇论文主要还是集中在玄学调参：1、对于ResNet去修改他的堆叠数量（1,1,9,1）；2、换更加大的卷积核（4x4，4）；3、更加少的激活函数核归一化处理（都只在1x1卷积之后进行操作）。提供如下几种模型
![](https://s2.loli.net/2025/04/30/nYJZOmq4CjlHi5K.png)
其中C代表4个stage中输入的通道数，B代表每个stage重复堆叠block的次数

## ConvNeXt v2


## 总结
提到的论文中可能在学术上可以提供的参考意义不大，毕竟都是拿来主义，先不管他为什么这样只要能够起到好的作用那他就是好的模型（🤪🤪🤪🤪🤪）