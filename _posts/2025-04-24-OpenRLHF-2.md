---
layout: mypost
title: å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼šOpenRLHFæºç è§£è¯»ï¼Œæ¨¡å‹è®­ç»ƒ-1
categories: OpenRLHFæ¡†æ¶è§£è¯»
address: é•¿æ²™
extMath: true
show_footer_image: true
tags:
- å¼ºåŒ–å­¦ä¹ 
- OpenRLHF
description: æœ¬æ–‡è§£è¯»å¼ºåŒ–å­¦ä¹ æ¡†æ¶OpenRLHFçš„æºç ä¸æ¨¡å‹è®­ç»ƒï¼Œèšç„¦PPOèŒƒå¼è®­ç»ƒå‰çš„åˆå§‹åŒ–é…ç½®ï¼ŒåŒ…æ‹¬DeepSpeedä¸vLLMé…ç½®ï¼Œä»¥åŠrayåˆ†å¸ƒå¼æ¶æ„çš„åº”ç”¨ï¼Œæ¶‰åŠexperience_makerã€ppo_actorç­‰æ¨¡å—çš„æ ¸å¿ƒå®ç°ã€‚
---

å‰æ–‡å·²ç»ä»‹ç»äº†ï¼š
* [**å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼šOpenRLHFæºç è§£è¯»ï¼Œæ¨¡å‹å¤„ç†æ¨¡å—è§£è¯»**](https://www.big-yellow-j.top/posts/2025/04/22/OpenRLHF-1.html)

æœ¬æ–‡ä¸»è¦ä»‹ç» **å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼šOpenRLHFæºç è§£è¯»ï¼Œæ¨¡å‹è®­ç»ƒ**ã€‚å› ä¸ºåœ¨ **OpenRLHF** ä¸­æ•´ä¸ªæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä»£ç æ¯”è¾ƒå¤šå› æ­¤åˆ†æˆå¤šæ¬¡è¿›è¡Œè§£è¯»ï¼Œæ­¤éƒ¨åˆ†ä¸»è¦ä»‹ç»ä¸€äº›è®­ç»ƒå‰çš„åˆå§‹åŒ–é…ç½®ã€‚å› ä¸ºRLç”±DPOã€GRPOã€PPOç­‰å‡ ç§ç±»åˆ«ï¼Œå› æ­¤æœ¬æ–‡ä¸»è¦ä»‹ç»PPOèŒƒå¼è®­ç»ƒã€‚åœ¨OpenRLHFè®­ç»ƒæ¡†æ¶ä¸­ï¼Œä¸»è¦è¿˜ä¼šåº”ç”¨åˆ°DeepSpeedä»¥åŠvLLMï¼Œå› æ­¤åœ¨ä»‹ç»PPOè®­ç»ƒä¹‹å‰éœ€è¦å›é¡¾ä¸€ä¸‹ï¼š**1ã€DeepSpeedçš„é…ç½®**ï¼›**2ã€vLLMé…ç½®**ã€‚
> åœ¨ä¹‹å‰Blogå·²ç»å¯¹[DeepSpeed](https://www.big-yellow-j.top/posts/2025/02/24/deepspeed.html)ä»¥åŠ[vLLM](https://www.big-yellow-j.top/posts/2025/02/17/Attention.html)åŸç†è¿›è¡Œäº†è§£é‡Šï¼Œå› æ­¤åªéœ€è¦ä»‹ç»åœ¨OpenRLHFå¦‚ä½•å»å¯¹è¿™ä¸¤éƒ¨åˆ†è¿›è¡Œé…ç½®

å‚æ•°å‚è€ƒè„šæœ¬ï¼šhttps://github.com/OpenRLHF/OpenRLHF/blob/main/examples/scripts/train_ppo_llama_ray.sh ä¸­çš„è®¾ç½®
* **1ã€vLLMé…ç½®**

> From:https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/trainer/ray/vllm_engine.py

```python
def create_vllm_engines(
    num_engines: int, # æ¨ç†å¼•æ“æ•°é‡
    tensor_parallel_size: int, # å¼ é‡å¹¶è¡Œå¤§å°
    pretrain: str,
    seed: int,
    full_determinism: bool,
    enable_prefix_caching: bool,
    enforce_eager: bool,
    max_model_len: int,
    num_total_actors: int,
    shared_pg=None,
    gpu_memory_utilization=None,
    vllm_enable_sleep=False,):
    ...
    # 1ã€èµ„æºè°ƒåº¦é…ç½®ã€‚é…ç½®å‚æ•°è®¾ç½®ä¸ºï¼šnum_engines= tensor_parallel_size= 2
    use_hybrid_engine = shared_pg is not None
    num_gpus = int(tensor_parallel_size == 1)
    if use_hybrid_engine and tensor_parallel_size == 1:
        num_gpus = 0.2

    if not use_hybrid_engine:
        bundles = [{"GPU": 1, "CPU": 1} for _ in range(num_engines * tensor_parallel_size)]
        shared_pg = placement_group(bundles, strategy="PACK")
        ray.get(shared_pg.ready())
    ...
    # 2ã€æ„å»ºæ¯ä¸€ä¸ªvLLMï¼ˆ=2ï¼‰
    for i in range(num_engins):
        ...
        scheduling_strategy = PlacementGroupSchedulingStrategy(...) # è°ƒåº¦ç­–ç•¥
        ...
        vllm_engines.append(
            LLMRayActor.options(
            num_cpus=num_gpus,
            num_gpus=num_gpus,
            scheduling_strategy=scheduling_strategy,
        ).remote(...)
        )
```

1ã€**èµ„æºè°ƒåº¦é…ç½®**ï¼šç¬¬ä¸€ç§Hybridæ¨¡å¼ï¼ˆå¤šä¸ªå¼•æ“å…±åŒå ç”¨GPUï¼‰ï¼›ç¬¬äºŒç§æ ‡å‡†æ¨¡å¼ï¼ˆæ¯ä¸ªå¼•æ“éƒ½å•ç‹¬å ç”¨ä¸€ä¸ªGPUå’ŒCPUï¼‰ï¼›
2ã€**æ„å»ºvLLM**ï¼šé¦–å…ˆæ˜¯å»ºç«‹èµ„æºè°ƒåº¦ç­–ç•¥ï¼Œä»¥åŠä½¿ç”¨vLLMã€‚æœ‰å¿…è¦äº†è§£ä¸€ä¸‹å°±æ˜¯åœ¨OpenRLHFä¸­ä½¿ç”¨çš„æ˜¯ [**ray**](https://www.ray.io/) åˆ†å¸ƒå¼æ¶æ„è¿›è¡Œè®­ç»ƒã€‚ç®€å•äº†è§£ä¸€ä¸‹åœ¨è¿™ä¸ªé‡Œé¢ä»–æ˜¯æ€ä¹ˆåšçš„ã€‚é€šè¿‡ rayå°è£…äº†ä¸€ä¸ªvLLMæ¨ç†æ¶æ„ï¼ˆ `LLMRayActor` ï¼‰

> **è¡¥å……-1**ï¼šrayç®€å•ä½¿ç”¨ï¼Œä»£ç ï¼š[ğŸ”—](../code/ray_test.py.txt)
> Rayæ ¸å¿ƒæ¦‚å¿µï¼š1ã€ä»»åŠ¡ï¼ˆTaskï¼‰ï¼šæ— çŠ¶æ€çš„å¹¶è¡Œå‡½æ•°è°ƒç”¨ã€‚
2ã€Actorï¼šæœ‰çŠ¶æ€çš„è®¡ç®—å•å…ƒï¼Œé€‚åˆéœ€è¦æŒä¹…çŠ¶æ€çš„åœºæ™¯ï¼ˆå¦‚æ¨¡å‹æ¨ç†ï¼‰ã€‚3ã€è¿œç¨‹è°ƒç”¨ï¼ˆremoteï¼‰ï¼šé€šè¿‡ .remote() å¼‚æ­¥è°ƒåº¦ä»»åŠ¡æˆ– Actor æ–¹æ³•ã€‚æ¯”å¦‚è¯´ä¸Šé¢ä»£ç ï¼Œåˆå§‹åŒ–æˆ‘éœ€è¦çš„èŠ‚ç‚¹ 
```python
@ray.remote
class PrintActor:
```
> åœ¨vLLMä¸­å¯èƒ½å°±éœ€è¦å¯¹èµ„æºè¿›è¡Œåˆ†é…ï¼š`PrintActor.options(...).remote(...)`å…¶ä¸­ `remote`å°±æ˜¯æ¯ä¸ªâ€œè¿›ç¨‹â€éœ€è¦è¾“å‡ºçš„ä»»åŠ¡å‚æ•°ï¼Œè€Œ `options` åˆ™æ˜¯èµ„æºåˆ†é…ç­–ç•¥ï¼Œæ¯”å¦‚GPUï¼ˆ`num_gpus`ï¼‰/CPUï¼ˆ`num_cpus`ï¼‰æ•°é‡ã€‚åœ¨åé¢è·å–è¿›ç¨‹ç»“æœå¯ä»¥ç›´æ¥é€šè¿‡ï¼š
```python
ray.init()
print_engines = create_print_engines(4)
results = [engine.execture_print.remote(i) for i, engine in enumerate(print_engines)]
print(ray.get(results))
```

## PPOè®­ç»ƒèŒƒå¼
TODO: æ­¤éƒ¨åˆ†æ²¡æœ‰å†™å®Œ
å®Œå…¨äº†è§£PPOè®­ç»ƒèŒƒå¼ä¹‹å‰éœ€è¦äº†è§£ä¸€ä¸‹åœ¨OpenRLHFä¸­å¦‚ä½•å®šä¹‰ PPOè®­ç»ƒå™¨çš„ã€‚

### `experience_maker.py`
> https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/trainer/ppo_utils/experience_maker.py

æ”¹ä»£ç ä¸­é¦–å…ˆå®šä¹‰ä¸€ä¸ªåŸºç¡€ç±» `BaseExperienceMaker` ç”¨äºåˆå§‹åŒ–ï¼Œè¿™ä¸ªç±»ä¸­ä¸»è¦ä½œç”¨ä¸ºï¼š1ã€å®šä¹‰ä¸€ä¸ª `tokenizer`ï¼›2ã€ä¼šå°†æ‰€æœ‰ç”Ÿæˆçš„å†…å®¹éƒ½å­˜å‚¨åœ¨ `Samples` ä¸­ã€‚

```python3
@dataclass
class Samples:
    sequences: torch.Tensor
    attention_mask: Optional[torch.LongTensor]
    action_mask: Optional[torch.BoolTensor]
    num_actions: Union[int, torch.Tensor]
    packed_seq_lens: Optional[torch.Tensor]
    response_length: torch.Tensor
    total_length: torch.Tensor
    prompts: list[str]
    labels: list[str]
    pad_len: Optional[int]
```

å…³é”®æ˜¯ `RemoteExperienceMaker`å®šä¹‰ï¼š

```python
class RemoteExperienceMaker(BaseExperienceMaker):
    def __init__(...):
        ...
    #ï¼ˆ1ï¼‰å®šä¹‰ç”Ÿæˆå™¨ï¼Œé€šè¿‡LLMç”Ÿæˆæ–°çš„æ–‡æœ¬å†…å®¹
    @torch.no_grad()
    def generate_samples(self, all_prompts: List[str], all_labels, **generate_kwargs):
        #ï¼ˆ1.1ï¼‰ä½¿ç”¨hugginfaceç”Ÿæˆ  _generate_with_hf(...)
        #ï¼ˆ1.2ï¼‰ä½¿ç”¨vLLMè¿›è¡Œç”Ÿæˆ     _generate_vllm(...)
    @torch.no_grad()
    def _generate_with_hf(...):
        ...
    def _generate_vllm(...):
        ....
    #ï¼ˆ2ï¼‰è®¡ç®—
    @torch.no_grad()
    def make_experience_list(self, all_prompts: Union[str, List[str]], all_labels, **generate_kwargs):
        ...
        #ï¼ˆ2.1ï¼‰ç”Ÿæˆå†…å®¹
        if self.strategy.ring_attn_group is not None:
            # Only rank 0 in the ring attention group executes the generation function, and then broadcasts it to all other ranks.
            if self.strategy.ring_attn_rank == 0:
                samples_list = self.generate_samples(all_prompts, all_labels, **generate_kwargs)

                dist.broadcast_object_list(samples_list, src=dist.get_rank(), group=self.strategy.ring_attn_group)
            else:
                world_size = torch.distributed.get_world_size() // args.ring_attn_size
                samples_list = [None] * (
                    args.rollout_batch_size * args.n_samples_per_prompt // world_size // args.micro_rollout_batch_size
                )
                dist.broadcast_object_list(
                    samples_list, src=self.strategy.ring_attn_ranks[0], group=self.strategy.ring_attn_group
                )
        else:
            samples_list = self.generate_samples(all_prompts, all_labels, **generate_kwargs)
        ...
        #TODO:ï¼ˆ2.2ï¼‰æ ¸å¿ƒ
        experiences = self.make_experience(samples_list)
        # Process experiences (reward shaping, etc.)
        experiences = self.compute_advantages_and_returns(experiences, **generate_kwargs)
        # send experience to critic
        if self.critic is not None:
            for experience in experiences:
                experience_cpu = deepcopy(experience)
                experience_cpu.to_device("cpu")
                self._ref = self.critic.append.remote(experience_cpu)
        return experiences

    @torch.no_grad()
    def make_experience(self, samples_list: List[Samples]) -> List[Experience]:
       pass
    
```
#### **ä¸€ã€å®šä¹‰ç”Ÿæˆå™¨**
> é€šè¿‡ä¸¤ç§æ–¹å¼ï¼ˆhuggingfaceä»¥åŠvLLMï¼‰

* ç¬¬ä¸€ç§ç›´æ¥ä½¿ç”¨huggingfaceæ“ä½œè¿›è¡Œç”Ÿæˆï¼š

```python
@torch.no_grad()
def _generate_with_hf(self, all_prompts: List[str], all_labels, **generate_kwargs) -> List[Samples]:
    ...    
    # train_ppo_ray.py n_samples_per_prompt=1 å¯¹æ¯ä¸€æ¡ prompt ç”Ÿæˆå†…å®¹æ•°é‡
    all_prompts = sum([[prompt] * args.n_samples_per_prompt for prompt in all_prompts], [])
    all_labels = sum([[label] * args.n_samples_per_prompt for label in all_labels], [])
    samples_list = []
    for i in range(0, len(all_prompts), args.micro_rollout_batch_size): 
        # train_ppo_ray.py micro_rollout_batch_size=8
        prompts = all_prompts[i : i + args.micro_rollout_batch_size]
        labels = all_labels[i : i + args.micro_rollout_batch_size]
        inputs = self.tokenize_fn(prompts, self.prompt_max_len, device="cuda")
        sequences, attention_mask, action_mask = self.actor.generate(**inputs, **generate_kwargs)
        samples = Samples(...) # å°†ç”Ÿæˆå†…å®¹æ‰€æœ‰ä¿¡æ¯å­˜å‚¨ï¼Œæ¯”å¦‚è¯´sequencesè¿™äº›å…³é”®ä¿¡æ¯
        samples_list.append(samples)
    return samples_list
```

* ç¬¬äºŒç§é€šè¿‡vLLMæ–¹å¼è¿›è¡Œç”Ÿæˆï¼š

ä½¿ç”¨vLLMæ–¹å¼è¿›è¡Œç”Ÿæˆéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œæ­£å¦‚æœ€å¼€å§‹ä¸Šé¢ä»‹ç»çš„ï¼Œæˆ‘çš„æ‰€æœ‰çš„vLLMéƒ½æ˜¯é€šè¿‡rayè¿›è¡Œå¤„ç†äº†ï¼Œå› æ­¤è¿™ä¸ªå°±ä¼šæ¯”è¾ƒå¤æ‚
```python
def _generate_vllm(self, all_prompts: List[str], all_labels, **kwargs):
    ...
    #ï¼ˆ1ï¼‰é¦–å…ˆè·å–æ‰€æœ‰çš„è®¾å¤‡æ•°é‡ï¼Œå¹¶ä¸”ç¡®å®š vllm å¼•æ“æ•°é‡
    rank = torch.distributed.get_rank() // self.strategy.ring_attn_size
    world_size = torch.distributed.get_world_size() // self.strategy.ring_attn_size
    if len(self.vllm_engines) <= world_size:
        llms = [self.vllm_engines[rank % len(self.vllm_engines)]]
    else:
        llms = self.vllm_engines[rank::world_size]
    ...
    #ï¼ˆ2ï¼‰ç›´æ¥é€šè¿‡ from vllm import SamplingParams æ¥è®¾ç½®ç”Ÿæˆå™¨ç­–ç•¥æ¯”å¦‚è¯´ top_pç­‰
    sampling_params = SamplingParams(...)
    #ï¼ˆ3ï¼‰å°†promptã€labelsè¿›è¡Œé‡å¤é‡‡ç”¨ï¼Œè¿™é‡Œå’Œhfç”Ÿæˆå¤„ç†ç›¸åŒï¼Œå¹¶ä¸”å°†prompté€šè¿‡tokenizerè¿›è¡Œå¤„ç†
    all_prompts = sum([[prompt] * args.n_samples_per_prompt for prompt in all_prompts], [])
    all_labels = sum(...)
    all_prompt_token_ids = self.tokenize_fn(all_prompts, self.prompt_max_len, padding=False)["input_ids"]
    #ï¼ˆ4ï¼‰ç›´æ¥å°†æ‰€æœ‰çš„promptè¾“å…¥åˆ° vllmä¸­è¿›è¡Œç”Ÿæˆ
    refs = []
    batch_size = (len(all_prompt_token_ids) + len(llms) - 1) // len(llms)
    for i, llm in enumerate(llms):
        prompt_token_ids = all_prompt_token_ids[i * batch_size : (i + 1) * batch_size]
        refs.append(
            llm.add_requests.remote(rank, sampling_params=sampling_params, prompt_token_ids=prompt_token_ids)
        )
    ray.get(refs)
    #ï¼ˆ5ï¼‰æ‰€æœ‰è®¾å¤‡ä¹‹é—´è¿›è¡ŒåŒæ­¥
    ...
    #ï¼ˆ6ï¼‰è·å–vLLMç”Ÿæˆå†…å®¹
    all_output_refs = []
    for i, llm in enumerate(llms):
        all_output_refs.append(llm.get_responses.remote(rank))
    all_outputs = sum(ray.get(all_output_refs), [])
    #ï¼ˆ7ï¼‰å¤„ç†vLLMè¾“å‡º
    samples_list = []
    for i in range(0, len(all_outputs), args.micro_rollout_batch_size):
        outputs = all_outputs[i : i + self.strategy.args.micro_rollout_batch_size]
        prompts = all_prompts[i : i + self.strategy.args.micro_rollout_batch_size]
        labels = all_labels[i : i + self.strategy.args.micro_rollout_batch_size]
        #ï¼ˆ8ï¼‰å¤„ç†æ•°æ®
        if not self.packing_samples:
            # ä¸å¯¹æ•°æ®è¿›è¡Œæ‰“åŒ…è¾“å‡º
            # | [PAD] [PAD] token token token | token token [EOS] [PAD] |
            # | token token token token token | token token [EOS] [PAD] |
            # | [PAD] [PAD] [PAD] token token | token token token [EOS] |
            # |<---------- prompt ----------->|<-------- answer ------->|
            ...
            samples_list.append(...)
        else:
            # å¯¹æ•°æ®è¿›è¡Œæ‰“åŒ…è¾“å‡º
            # | token token token | token token [EOS] | token token token token token | token token [EOS] | token token | token token token [EOS] |
            # |<---  prompt ----->|<---- answer ----->|<---------- prompt ----------->|<----- answer ---->|<- prompt -->|<-------- answer ------->|
            ...
            samples_list.append(...)
    return samples_list
```

ï¼ˆ4ï¼‰`llm.add_requests.remote(...)`ã€`llm.get_responses.remote(...)` å…¶ä¸­ `add_requests` æ‰§è¡Œæ“ä½œï¼šç›´æ¥é€šè¿‡vLLMç”Ÿæˆæ–‡æœ¬ï¼š`self.llm.generate(prompts=requests, sampling_params=sampling_params)` è€Œåœ¨ `get_responses`åˆ™æ˜¯ç›´æ¥è·å–vLLMæ‰€ç”Ÿæˆçš„å†…å®¹ã€‚
ï¼ˆ7-8ï¼‰åœ¨å¤„ç†è¾“å‡ºè¿‡ç¨‹ä¸­ç”±äºæ•°æ®å¯èƒ½ä¼šå°†çŸ­æ–‡æœ¬è¿›è¡Œæ‹¼æ¥ï¼ˆ`packing_samples`ï¼‰ä½†æ˜¯ä¸åŒçŸ­æ–‡æœ¬ä¹‹é—´å¯¹åº”é—®é¢˜æ˜¯ä¸åŒçš„ï¼Œå› æ­¤å°±éœ€è¦å°†è¾“å‡ºè¿›è¡Œæ•´ç†ã€‚å¤„ç†è¿‡ç¨‹æ¯”è¾ƒç®€å•ï¼š
**ä¸å¯¹æ•°æ®è¿›è¡Œæ‰“åŒ…è¾“å‡º**ï¼šåªéœ€è¦å°±ç®— `outputs`ï¼ˆæ˜¯è¢«åˆ‡åˆ†äº†æ¯æ¬¡å¤„ç†çš„ä¸€å—ï¼‰ ä¸­æœ€å¤§é•¿åº¦ç„¶åæŒ‰ç…§è¿™ä¸ªæœ€å¤§é•¿åº¦è¿›è¡Œå¡«è¡¥å³å¯ï¼ˆå·¦å¡«å……æ–¹å¼ï¼šåœ¨æœ€å·¦ä¾§æ·»åŠ  [PAD]æ ‡è®°ï¼‰
**å¦‚æœå°†æ•°æ®è¿›è¡Œæ‰“åŒ…è¾“å‡º**ï¼šåªéœ€è¦å°†promptå’Œè¾“å‡ºæ‹¼æ¥èµ·æ¥å³å¯

#### **äºŒã€è®¡ç®—**
å‰é¢å·²ç»ä»‹ç»äº†å¦‚ä½•é€šè¿‡vllm/hfé€šè¿‡promptç”Ÿæˆå†…å®¹ï¼ˆ`generate_samples`ï¼‰å› æ­¤åœ¨ `make_experience_list`é¦–å…ˆä¹Ÿå°±æ˜¯ç›´æ¥æ ¹æ®promptæ¥ç”Ÿæˆå†…å®¹ï¼Œè€Œå `make_experience`ã€`compute_advantages_and_returns`ã€`critic`ã€‚åˆ†åˆ«ä»‹ç»è¿™ä¸‰éƒ¨åˆ†ä»£ç å†…å®¹ï¼š
* 1ã€`make_experience`


### `ppo_actor.py`
> https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/trainer/ray/ppo_actor.py

è¿™é‡Œä¸»è¦æ˜¯å®šä¹‰äº†ä¸€ä¸ªtrainerç±»ï¼Œç”¨æ¥æ‰§è¡Œæ•°æ®åŠ è½½ã€æ¨¡å‹å¤„ç†ã€è®°å½•lossç­‰æ“ä½œ

```python
class ActorPPOTrainer(BasePPOTrainer):
    def __init__(...):
        ...
        # ï¼ˆ1ï¼‰åˆå§‹åŒ–è®°å½•å™¨ï¼Œæ¯”å¦‚è¯´wandbã€tensorboardï¼Œè¿™éƒ¨åˆ†æ¯”è¾ƒç®€å•ä¸åšä»‹ç»
        ...
        self.experience_maker = RemoteExperienceMaker(...) #TODO: è¿™éƒ¨åˆ†éœ€è¦ç‰¹æ®Šå»çœ‹ä¸€ä¸‹
        # ï¼ˆ2ï¼‰å› ä¸ºè¦ä½¿ç”¨rayå°±éœ€è¦è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®
        backend = getattr(self.strategy.args, "vllm_sync_backend", "nccl") # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨ nccl è¿›è¡Œé€šä¿¡
        ...
        # ï¼ˆ3ï¼‰ray åˆ†å¸ƒå¼èŠ‚ç‚¹è®¾ç½®
        if self.vllm_engines is not None and not self.use_cuda_ipc and torch.distributed.get_rank() == 0: # å¦‚æœä¸ä½¿ç”¨ nccl è¿›è¡Œé€šä¿¡
            # åˆå§‹åŒ–è®¾ç½®ï¼šè·å–ä¸»èŠ‚ç‚¹ï¼Œå¹¶ä¸”éšä¾¿ç»‘å®šä¸€ä¸ªç«¯å£
            master_address = ray._private.services.get_node_ip_address()
            with socket.socket() as sock:
                sock.bind(("", 0))
                master_port = sock.getsockname()[1]

            vllm_num_engines, vllm_tensor_parallel_size = (
                self.strategy.args.vllm_num_engines,
                self.strategy.args.vllm_tensor_parallel_size,
            )
            # è·å–è§„æ¨¡ï¼šè®¡ç®—vllmå¼•æ“ä»¥åŠå‡ å¼ å¡è¿›è¡Œå¼ é‡å¹¶è¡Œ
            world_size = vllm_num_engines * vllm_tensor_parallel_size + 1

            use_ray = getattr(self.strategy.args, "vllm_sync_with_ray", False)
            group_name = "openrlhf"
            refs = [
                engine.init_process_group.remote(
                    master_address,
                    master_port,
                    i * vllm_tensor_parallel_size + 1,
                    world_size,
                    group_name,
                    backend=backend,
                    use_ray=use_ray,
                )
                for i, engine in enumerate(self.vllm_engines)
            ]
            # (4) é€šä¿¡ä¹‹é—´è¿›è¡ŒåŒæ­¥
            ...
            ray.get(refs)
        torch.distributed.barrier()
    def fit(
        ...
        prompts_dataloader,
        pretrain_dataloader,
        consumed_samples=0,
        num_update_steps_per_episodes=1,):
        # ä¸»è¦è¿›è¡Œæ•°æ®åŠ è½½/æ¨¡å‹è®¡ç®—/æ¨¡å‹å­˜å‚¨ç­‰æ“ä½œ
        ...
        for episode in range(start_episode, args.num_episodes):
            ...
            for rand_prompts, labels in self.prompts_dataloader:
                for i, experience in enumerate(
                    self.experience_maker.make_experience_list(rand_prompts, labels, **self.generate_kwargs)
                ):


```

ï¼ˆ3ï¼‰ä½¿ç”¨ ray åˆ†å¸ƒå¼èŠ‚ç‚¹è®¾ç½®ï¼Œåˆå§‹åŒ–è®¾ç½®ä¸»èŠ‚ç‚¹/ç«¯å£å·/åˆ†ç»„ä¿¡æ¯ç­‰ï¼Œé€šè¿‡ `refs` æ¥å­˜å‚¨åˆå§‹åŒ–çš„ vLLMï¼Œä»£ç ä¸­åˆå§‹åŒ–æ“ä½œä¸ºï¼š
```python
def init_process_group(self, master_address, master_port, rank_offset, world_size, group_name, backend, use_ray):
    return self.llm.collective_rpc(
        "init_process_group",
        args=(master_address, master_port, rank_offset, world_size, group_name, backend, use_ray),
    )
```
å…¶ä¸­ `self.llm=vllm.LLM(*args, **kwargs)`

### launcher.py
> https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/trainer/ray/launcher.py

åœ¨launcherä¸­ä¸»è¦ä¹Ÿæ˜¯å®šä¹‰å„ç±»åˆ†å¸ƒå¼ç³»ç»Ÿæ–¹å¼ï¼ˆå¦‚ä½•å°†å„ç±»æ¨¡å‹åˆ†ä¸åˆ°è®¾åˆ«ä¸Šï¼‰ï¼Œé¦–å…ˆæ¥è¯´å®šä¹‰ä¸¤ä¸ªåŸºç±»ï¼š
`DistributedTorchRayActor`:è¿™ä¸ªç±»æ˜¯ä¸€ä¸ªåŸºäº Ray çš„åˆ†å¸ƒå¼ Actorï¼Œè´Ÿè´£è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒæˆ–æ¨ç†ç¯å¢ƒï¼Œ**å®ƒåˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒçš„é…ç½®ï¼ŒåŒ…æ‹¬èŠ‚ç‚¹åœ°å€ã€ç«¯å£ã€è¿›ç¨‹æ’åç­‰ã€‚**
`BasePPORole(DistributedTorchRayActor)`:æ‰©å±•ä¸Šé¢æ‰€å®šä¹‰çš„åˆ†å¸ƒå¼ç¯å¢ƒé…ç½®ï¼Œå¹¶æä¾›äº†æ‰¹é‡æ¨ç†çš„åŠŸèƒ½ã€‚
ä¸‹é¢å°±æ˜¯å¯¹å„ç±»æ¨¡å‹ï¼šrewardmodelç­‰è¿›è¡Œåˆ†å¸ƒå¼è¿›è¡Œå¤„ç†

* 1ã€`ReferenceModelRayActor` å’Œ `RewardModelRayActor`

```python
@ray.remote(num_gpus=1)
class ReferenceModelRayActor(BasePPORole):
    def init_model_from_pretrained(self, strategy: DeepspeedStrategy, pretrain):
        self._setup_distributed(strategy)
        model = Actor(...)
        # 1ï¼‰è¿™é‡Œä¹Ÿå°±æ˜¯å¯¹äºä¸€äº›æ¨¡å‹è¿›è¡Œåˆå§‹åŒ–æ“ä½œ
        ...
    def forward(...):
        ...
        # 2ï¼‰è¿™é‡Œä¹Ÿå°±æ˜¯ç›´æ¥å°†æ–‡æœ¬è¾“å…¥åˆ°æ¨¡å‹ä¸­è¿›è¡Œè®¡ç®—

@ray.remote(num_gpus=1)
class RewardModelRayActor(BasePPORole):
    def init_model_from_pretrained(self, strategy: DeepspeedStrategy, pretrain):
        self._setup_distributed(strategy)
        model = get_llm_for_sequence_regression(...)
        # 3) åˆå§‹åŒ–æ¨¡å‹æ“ä½œ
        ...
    def forward(...):
        ...
        # 4ï¼‰è¿™é‡Œä¹Ÿå°±æ˜¯ç›´æ¥å°†æ–‡æœ¬è¾“å…¥åˆ°æ¨¡å‹ä¸­è¿›è¡Œè®¡ç®—
```

ï¼ˆ1ã€2ã€3ã€4ï¼‰ã€æ¨¡å‹åˆå§‹åŒ–æ“ä½œï¼Œè¿™é‡Œæ²¡ä»€ä¹ˆå¥½è§£é‡Šçš„ç›´æ¥é€šè¿‡ç»§æ‰¿çš„ç±»ï¼ˆ`BasePPORole`ï¼‰ä¸­å¯¹ `Deepspeed` è¿›è¡Œåˆå§‹åŒ–ã€‚æ¨¡å‹è®¡ç®—ä¹Ÿæ˜¯æ¯”è¾ƒå¸¸è§„çš„ `forward` è®¡ç®—èŒƒå¼ã€‚å¯¹äº `Actor` æ“ä½œï¼š[ğŸ“ƒ](https://www.big-yellow-j.top/posts/2025/04/22/OpenRLHF-1.html#:~:text=1%E3%80%81-,actor.py,-https%3A//github.com)ã€‚

* 2ã€`PPORayActorGroup`ï¼šåˆ›å»ºã€åˆå§‹åŒ–å’Œç®¡ç†å¤šä¸ªåˆ†å¸ƒå¼ Actorï¼Œåè°ƒå®ƒä»¬çš„æ¨¡å‹åŠ è½½ã€è®­ç»ƒã€æ¨ç†å’Œä¿å­˜æ“ä½œ

```python
class PPORayActorGroup:
    def __init(
        ...,
        pg: PlacementGroup = None,
        ...):
        ...
        self._initiate_actors(pg, num_gpus_per_actor)
    def _initiate_actors(
        self, 
        pg, # é€šè¿‡rayåˆ›å»ºèµ„æºç»„
        num_gpus_per_actor):
        ...
        self._actor_handlers = [master_actor]

    def async_fit_actor_model(
        self,
        critic_model_group: "PPORayActorGroup",
        initial_model_group: "PPORayActorGroup",
        reward_model_groups: List["PPORayActorGroup"],
        remote_rm_urls: List[str] = None,
        reward_fn: Callable[[List[torch.Tensor]], torch.Tensor] = None,
        vllm_engines: List = None,):
        ...
        critic_actors = critic_model_group._actor_handlers if critic_model_group else None
        initial_actors = initial_model_group._actor_handlers if initial_model_group else None



    def async_init_model_from_pretrained(...):
        # åˆå§‹åŒ–æ¨¡å‹
        return [actor.init_model_from_pretrained.remote(*args, **kwargs) for actor in self._actor_handlers]

    def async_save_model(self):
        # ä¿å­˜æ¨¡å‹
        return [actor.save_model.remote() for actor in self._actor_handlers]

    def async_run_method(self, method_name, *args, **kwargs):
        refs = []
        for actor in self._actor_handlers:
            method = getattr(actor, method_name)
            refs.append(method.remote(*args, **kwargs))
        return refs

```

ï¼ˆ1ï¼‰ã€åœ¨åˆå§‹åŒ–ï¼ˆ `async_init_model_from_pretrained` ï¼‰ä»¥åŠä¿å­˜ï¼ˆ `async_save_model` ï¼‰ä¸­é‡Œé¢æ‰€ä½¿ç”¨çš„ `actor` å°±æ˜¯ä¸Šé¢æåˆ°çš„ `ReferenceModelRayActor` 


## è®°å½•
https://github.com/Dao-AILab/flash-attention/issues/432#issuecomment-1698610752