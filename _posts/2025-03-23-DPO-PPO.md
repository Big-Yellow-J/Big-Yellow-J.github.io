---
layout: mypost
title: LLMä¸­çš„RLHFä¼˜åŒ–æ–¹æ³•ï¼šGRPOã€DPOä¸PPOè§£æ
categories: æ·±åº¦å­¦ä¹ åŸºç¡€ç†è®º
extMath: true
images: true
address: changsha
show_footer_image: true
description: åœ¨ä¹‹å‰blogä¸­ç®€å•ä»‹ç»äº†ä¸€ä¸‹DeepSeek-R1å¯ä»¥ä¸ä½¿ç”¨ä»»ä½•ç›‘ç£æ•°æ®è®©æ¨¡å‹æ‹¥æœ‰æ¨ç†èƒ½åŠ›ï¼Œå…¶ä½¿ç”¨çš„GRPOæŠ€æœ¯è¿™é‡Œå†æ¬¡å…·ä½“ç†è§£ä¸€ä¸‹ä»–çš„å…·ä½“åŸç†ï¼Œä»¥åŠä»£ç å®è·µæ“ä½œ
tags: [GRPO,DPO,PPO,LLM,å¼ºåŒ–å­¦ä¹ ]
---

åœ¨ä¹‹å‰blogä¸­ç®€å•ä»‹ç»äº†ä¸€ä¸‹[DeepSeek-R1](https://arxiv.org/pdf/2501.12948)å¯ä»¥ä¸ä½¿ç”¨ä»»ä½•ç›‘ç£æ•°æ®è®©æ¨¡å‹æ‹¥æœ‰æ¨ç†èƒ½åŠ›ï¼Œå…¶ä½¿ç”¨çš„[GRPO](https://www.big-yellow-j.top/posts/2025/02/15/LLM.html#:~:text=%E6%8C%87%E6%A0%87%EF%BC%89%E6%83%A9%E7%BD%9A%E5%81%8F%E5%B7%AE-,PPO%E5%92%8C%20GRPO,-%E4%B8%8A%E9%9D%A2%E6%8F%90%E5%88%B0%E7%9A%84)æŠ€æœ¯è¿™é‡Œå†æ¬¡å…·ä½“ç†è§£ä¸€ä¸‹ä»–çš„å…·ä½“åŸç†ï¼Œä»¥åŠä»£ç å®è·µæ“ä½œã€‚å¯¹äºGRPOå‚è€ƒè®ºæ–‡ï¼ˆDeepSeekï¼‰ï¼šhttps://arxiv.org/pdf/2402.03300ï¼› å¯¹äºPPOå‚è€ƒè®ºæ–‡ï¼ˆOpenAIï¼‰ï¼šhttps://arxiv.org/pdf/1707.06347ã€‚ å¯¹äºæœ¬æ–‡å»ºè®®ç›´æ¥ç›´æ¥çœ‹ï¼š[PPOï¼ˆProximal Policy Optimizationï¼‰æ¨¡å‹](#ppoproximal-policy-optimizationæ¨¡å‹) [DPOï¼ˆDirect Preference Optimizationï¼‰æ¨¡å‹](#dpodirect-preference-optimizationæ¨¡å‹) [GRPOï¼ˆGroup Relative Preference Optimizationï¼‰æ¨¡å‹](#grpogroup-relative-preference-optimizationæ¨¡å‹)

## ç®€å•å›é¡¾LLMè®­ç»ƒè¿‡ç¨‹

åœ¨[Blog](https://www.big-yellow-j.top/posts/2025/02/15/LLM.html)é‡Œé¢è®¨è®ºè¿‡LLMæ¡†æ¶è¿™é‡Œç®€å•è®¨è®ºä¸€ä¸‹LLMè®­ç»ƒè¿‡ç¨‹ï¼Œä¸€èˆ¬è€Œè¨€åœ¨LLMä¸­è®­ç»ƒä¸»è¦åˆ†ä¸ºå¦‚ä¸‹å‡ ä¸ªé˜¶æ®µï¼š

* 1ã€é¢„è®­ç»ƒï¼ˆ**Pre-Training**ï¼‰

è¿™éƒ¨åˆ†ç®€å•ç†è§£å°±æ˜¯è®©LLMèƒ½å¤Ÿè¯´â€œäººè¯â€ï¼Œè‡ªå›å½’æ¨¡å‹é€šè¿‡å‰ä¸€æ®µæ–‡æœ¬ç„¶åé¢„æµ‹ä¸‹ä¸€ä¸ªæ–‡æœ¬ï¼Œå¹¶ä¸”è®©æ¨¡å‹èƒ½å¤Ÿè¾ƒå¥½çš„â€œè¯´è¯â€ï¼ˆæ¯”å¦‚è¯´ï¼šå¤§è¯­è¨€æ¨¡ï¼Œä¸‹ä¸€ä¸ªå­—å¯ä»¥æ­£ç¡®è¾“å‡º â€œå‹â€ï¼‰

* 2ã€åè®­ç»ƒï¼ˆ**Post-Training**ï¼‰

åœ¨å¾—åˆ°ä¸€ä¸ªèƒ½å¤Ÿè¯´äººè¯çš„æ¨¡å‹ä¹‹åï¼Œå°±éœ€è¦è®©æ¨¡å‹èƒ½å¤Ÿâ€œæ€è€ƒâ€ï¼Œè¿™éƒ¨åˆ†ä¸»è¦åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š1ã€ç›‘ç£å¾®è°ƒï¼ˆ**SFT** Supervised Trainingï¼‰ï¼›2ã€äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼ˆ**RLHF** Reinforcement Learning from Human Feedbackï¼‰ã€‚å‰è€…ï¼šé¡¾åæ€ä¹‰ï¼Œæˆ‘ä»¬é¦–å…ˆä½¿ç”¨ç›‘ç£å­¦ä¹ æ–¹æ³•ï¼Œåœ¨å°‘é‡é«˜è´¨é‡çš„ä¸“å®¶æ¨ç†æ•°æ®ä¸Šå¯¹ LLM è¿›è¡Œå¾®è°ƒï¼Œä¾‹å¦‚æŒ‡ä»¤è·Ÿè¸ªã€é—®é¢˜è§£ç­”å’Œ/æˆ–æ€ç»´é“¾ã€‚å¸Œæœ›åœ¨è®­ç»ƒé˜¶æ®µç»“æŸæ—¶ï¼Œæ¨¡å‹å·²ç»å­¦ä¼šå¦‚ä½•æ¨¡ä»¿ä¸“å®¶æ¼”ç¤ºã€‚åè€…ï¼šRLHF åˆ©ç”¨äººç±»åé¦ˆæ¥è®­ç»ƒå¥–åŠ±æ¨¡å‹ï¼Œç„¶åé€šè¿‡ RL å¼•å¯¼ LLM å­¦ä¹ ã€‚è¿™å°±ä½¿æ¨¡å‹ä¸äººç±»çš„ç»†å¾®åå¥½ä¿æŒä¸€è‡´

## RLå‡ ä¸ªåŸºæœ¬æ¦‚å¿µä»¥åŠå‘å±•å†ç¨‹

ç®€å•ç†è§£RLå°±æ˜¯ï¼šä¸€ä¸ªæ™ºèƒ½ä½“å¦‚ä½•åœ¨ç¯å¢ƒä¸­åšå‡ºæœ€ä½³æ“ä½œã€‚

![](https://s2.loli.net/2025/03/31/IQNVWPXLUgRMlZm.png)

ä¸€äº›åœ¨RLå¸¸ç”¨çš„å‡ ä¸ªå…³é”®è¯ï¼š
1ã€Agentï¼šè¿™é‡Œå¯ä»¥ç›´æ¥ç†è§£ä¸ºæˆ‘ä»¬çš„LLM
2ã€Stateï¼šæ¨¡å‹å½“å‰çš„çŠ¶æ€ï¼Œåæ˜ äº†LLMåœ¨ç‰¹å®šæ—¶åˆ»æ‰€å¤„çš„æƒ…å¢ƒï¼ˆå¯ä»¥æè¿°ä¸ºï¼šLLMçš„å‰n-1ä¸ªè¯ï¼‰
3ã€Actionï¼šLLMæ‰€è¾“å‡ºçš„å†…å®¹ï¼Œä¹Ÿå°±æ˜¯æ™ºèƒ½ä½“æ ¹æ®å½“å‰çŠ¶æ€æ‰€é‡‡å–çš„è¡ŒåŠ¨ï¼ˆLLMè¾“å‡ºçš„ç¬¬nä¸ªè¯ï¼‰
4ã€Reward Modelï¼šå¥–åŠ±æ¨¡å‹ï¼Œå¯¹äºLLMè¾“å‡ºçš„å†…å®¹è¿›è¡Œâ€œæ‰“åˆ†â€
5ã€Policyï¼šå†³å®šLLMå¦‚ä½•è¾“å‡ºå†…å®¹çš„ç­–ç•¥æˆ–è§„åˆ™ï¼ŒæŒ‡å¯¼æ™ºèƒ½ä½“åœ¨ä¸åŒçŠ¶æ€ä¸‹å¦‚ä½•è¡ŒåŠ¨

> æ¯”å¦‚è¯´ï¼šå¦‚ä½•è®©ç”µè„‘è‡ªå·±æ§åˆ¶é©¬é‡Œå¥¥é€šå…³

ä¸ºäº†å…·ä½“çš„äº†è§£åˆ°GRPOåŸç†ï¼Œæœ‰å¿…è¦å¿«é€Ÿäº†è§£ä¸€ä¸‹å¼ºåŒ–å­¦ä¹ å‘å±•

> https://youtu.be/JZZgBu8MV4Q?si=Tr7QC6srxkZJPdsI

é¦–å…ˆäº†è§£ä¸¤ä¸ªæ¨¡å‹æ¦‚å¿µï¼š1ã€**Policy Model**ï¼›2ã€**Value Model**ã€‚å‰è€…å¯ä»¥ç†è§£ä¸ºLLMè¾“å‡ºä¸‹ä¸€ä¸ªè¯ï¼ˆtokenï¼‰çš„è¯„åˆ†ï¼Œåè€…è¿™æ˜¯è¯„ä»·æ•´ä¸ªè¿‡ç¨‹çš„è¯„åˆ†ã€‚
1ã€**Policy Modelä¼˜åŒ–**ã€‚éƒ½çŸ¥é“åœ¨LLMè®­ç»ƒè¿‡ç¨‹ä¸­ï¼ˆæ— è®ºæ˜¯pre-trainingè¿˜æ˜¯SFTï¼‰éƒ½æ˜¯ä¼šé€šè¿‡GTï¼ˆGround Truthï¼‰æ¥è®¡ç®—æ¢¯åº¦ï¼Œç„¶åæ ¹æ®è¿™ä¸ªæ¢¯åº¦æ¥ä¼˜åŒ–æˆ‘çš„å‚æ•°ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼š$\theta = \theta- \text{lr}* \text{gradient}$ ã€‚åˆ†åˆ«è¡¨ç¤ºï¼šå‚æ•°ã€å­¦ä¹ ç‡ã€æ¢¯åº¦ã€‚åœ¨ **Policy Model**ä¼˜åŒ–è¿‡ç¨‹æ¢¯åº¦æ›´æ–°ä¸ºï¼š$\theta= \theta+ \text{lr}*\text{gradient}*\text{reward}$ ï¼ˆä¹‹æ‰€ä»¥ç”¨åŠ å·æ˜¯å› ä¸ºåœ¨RLä¼˜åŒ–è¿‡ç¨‹ä¸­æ˜¯ä¸ºäº†æœ€å¤§åŒ–å¥–åŠ±æ¨¡å‹ï¼‰æ›´åŠ ç›´è§‚çš„äº†è§£RLå‚æ•°ä¼˜åŒ–çš„ $\text{gradient}$ å’Œ $\text{reward}$ å‰è€…è¡¨ç¤ºçš„æ˜¯ä¼˜åŒ–çš„æ–¹å‘ï¼Œåè€…è¡¨ç¤ºçš„ä¼˜åŒ–çš„è¡Œä¸ºï¼ˆæ¯”å¦‚è¯´å°è¯•å„ç±»æŠ•ç¯®åŠ¨ä½œï¼Œå…¶ä¸­æŠ•ç¯®åŠ¨ä½œå°±æ˜¯æˆ‘ä»¬çš„ actorçƒè¿›äº†å°±æ˜¯éœ€è¦çš„ rewardï¼‰ã€‚
2ã€**baselineã€actor-criticã€advanadge actor critic**ã€‚ä½†æ˜¯è¿™æ ·éšä¹‹è€Œæ¥ä¼šå¸¦æ¥ä¸€ä¸ªæ–°çš„é—®é¢˜ï¼Œå‚æ•°ä¼˜åŒ–æ˜¯high varianceçš„ï¼ˆè¿‡åº¦ä¾èµ–æˆ‘ä»¬çš„rewardï¼Œæ¯”å¦‚reward=0å°±ä¼šå¯¼è‡´ä¸€ç›´ä¸æ›´æ–°ï¼‰å› æ­¤å°±æœ‰ä¸€ä¸ª **baseline**çš„æ–¹æ³•ï¼ˆè®¾å®šä¸€ä¸ª**å¹³å‡çŠ¶æ€**æ¥ä¿è¯ä¸è¢«rewardè¿‡åº¦ä¿®æ”¹ï¼‰ï¼ŒåŸºäº **baseline**åˆæå‡º **actor-critic**æ–¹æ³•ï¼Œ**actor-critic**æ–¹æ³•é€šè¿‡å°†value modelåŠ å…¥è¿›æ¥æ‰®æ¼” **critic**è§’è‰²æ¥è¯„ä¼°å½“å‰ç­–ç•¥çš„å¥½åï¼Œè€Œ **policy model**æ‰®æ¼”actorè§’è‰²ï¼Œé€šè¿‡value modelæ¥æŒ‡å¯¼actorã€‚
> æ¯”å¦‚è¯´ç¯®çƒæ¯”èµ›ï¼Œactorï¼ˆpolicy modelï¼‰ï¼šçƒå‘˜ï¼Œcriticï¼ˆvalue modelï¼‰ï¼šæ•™ç»ƒã€‚ä¸¤ä¸ªæ¨¡å‹ä¸€èµ·è¿›è¡Œè®­ç»ƒå‰è€…è¿›è¡ŒåŠ¨ä½œï¼ˆæ‰“çƒï¼‰åè€…æ¥åˆ¤æ–­è¿™ä¸€ç³»åˆ—åŠ¨ä½œå¯¹æœªæ¥å±€é¢å½±å“

ä¸è¿‡ä½¿ç”¨**actor-critic**è¿™ç§ç­–ç•¥å¯¹äºèµ„æºæ¶ˆè€—æ˜¯æ¯”è¾ƒå¤§çš„å› æ­¤å°±æå‡º ***advanadge* actor critic**ä¼˜åŒ–ç­–ç•¥ï¼š$A(s,a)=Q(s,a)-V(s)$ä¹Ÿå°±æ˜¯è¡¨ç¤º**å½“å‰åŠ¨ä½œğ‘ç›¸æ¯”å¹³å‡æ°´å¹³ï¼ˆVå€¼ï¼‰çš„å¥½å**ã€‚
> ä¹Ÿå°±æ˜¯è¯´æ—¢ç„¶è¦æ›´æ–°â€œè¡Œä¸ºâ€ç›´æ¥è®¡ç®—ï¼Œå¦‚æœä»–æ¯”å¹³å‡æ°´å¹³è¦å¥½ï¼Œé‚£ä¹ˆæˆ‘åç»­å°±è¦é‡‡ç”¨è¿™ä¸ªâ€œè¡Œä¸ºâ€

3ã€**TRPOï¼ŒPPOï¼ŒGRPO**ï¼šåœ¨ **TRPO**ä¸­æå‡ºï¼š$\text{ratio}\times \text{advantage}$å…¶ä¸­ $\text{ratio}=\frac{\pi_{\text{new}}}{\pi_{\text{old}}}$ä¹Ÿå°±æ˜¯å‚æ•°æ›´æ–°çš„æ¨¡å‹ä¸æ—§çš„æ²¡æœ‰æ›´æ–°çš„æ¨¡å‹å‚æ•°æ¯”å€¼ï¼Œäºæ­¤åŒæ—¶ä¸ºäº†ä¿è¯å‚æ•°çš„æ›´æ–°ä¸ä¼šå¤ªå¤§ï¼Œè¿˜ä¼šè®¡ç®—ä¸€ä¸ªKLæ•£åº¦å€¼ï¼š$\text{KL}(\pi_{\text{old}}, \pi_{\text{new}})â‰¤\delta$
> 1ã€å¯¹äºKLæ•£åº¦ç†è§£è€Œè¨€ï¼Œé¿å…å‚æ•°çš„æ›´æ–°è¿‡å¤§ï¼›2ã€ratioè€Œè¨€è¡¨ç¤ºæ–°æ—§ç­–ç•¥å¯¹åŠ¨ä½œçš„ç›¸å¯¹å€¾å‘

è®¡ç®—KLæ•£åº¦å¤æ‚åº¦æ¯”è¾ƒé«˜ï¼Œå› æ­¤ **PPO**ä¸­å°±æå‡ºä½¿ç”¨è£å‰ªç­–ç•¥å°†ratioé™å®šåœ¨ä¸€ä¸ªèŒƒå›´ä¸­ï¼ˆ$[1-\epsilon, 1+\epsilon]$ï¼‰ã€‚è€Œåœ¨ **GRPO**ä¸­é€‰æ‹©ç›´æ¥ä¸¢å» **value model**å¦‚æœä¸¢å¼ƒé€‰æ‹©çš„ **value model**å°±ä¼šå¯¼è‡´ä¸Šé¢ç¬¬2ç‚¹ä¸­æ‰€æåˆ°çš„ **baseline**å¦‚ä½•è®¡ç®—ï¼Œ**GRPO**æ¯”è¾ƒç›´æ¥ï¼Œç›´æ¥è®©æ¨¡å‹è¾“å‡ºå¤šä¸ªå›ç­”ï¼Œç„¶åå¯¹å›ç­”è¿›è¡Œè¯„åˆ†ï¼Œå¯¹äºæœ€åçš„ **advantage**è®¡ç®—ï¼š$\frac{r_i- mean(r)}{std(r)}$

![](https://s2.loli.net/2025/03/25/FkLG7VDmEpg4hKS.png)


å¯¹äº**å¥–åŠ±æ¨¡å‹çš„ä¼˜åŒ–**ï¼šä»¥DS-R1çš„ä¸ºä¾‹ï¼Œå¦‚ä½•è®©ä»–äº§å‡ºé«˜è´¨é‡/æ­£ç¡®çš„æ€ç»´é“¾ï¼Œä¸€ä¸ªå¾ˆç®€å•ï¼ˆåºŸäººï¼‰è¿‡ç¨‹å°±æ˜¯ç›´æ¥è®©äººç”Ÿæˆå¾ˆå¤šæ€ç»´è¿‡ç¨‹ï¼ˆæ¯”å¦‚æ¯”è¾ƒ 7.1å’Œ7.11å¤§å°ï¼Œåº”è¯¥å…ˆæ€ä¹ˆæ ·ï¼Œå†æ€ä¹ˆæ ·ï¼‰ä½†æ˜¯è¿™æ ·å°±ä¼šæœ‰é—®é¢˜ï¼š**äººä¸å¯èƒ½å°†æ‰€æœ‰é—®é¢˜éƒ½å†™å‡ºä¸€ä¸ªæ€ç»´é“¾**é‚£å°±æœ‰ä¸€ä¸ªæ›´åŠ ç›´æ¥åŠæ³•ï¼š**ç›´æ¥è®©è®­ç»ƒå¥½çš„LLMè‡ªå·±ç”Ÿæˆæ€ç»´é“¾**ã€‚å‚è€ƒ[Blog](https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/)çš„è§£é‡Šå¯¹äºä¸€ä¸ªå¥–åŠ±æ¨¡å‹$R_{\phi}$å®šä¹‰ä¸€ä¸ªè¿™æ ·çš„ä¼˜åŒ–è¿‡ç¨‹ï¼š

$$
\mathrm{L}(\phi)=-log \sigma(R_{\phi}(p, r_i)- R_{\phi}(p, r_j))
$$

å…¶ä¸­$p$ä»£è¡¨è¾“å…¥é—®é¢˜ï¼Œ$r$ä»£è¡¨LLMè¾“å‡ºçš„çš„ç»“æœã€‚å…¶ä¸­å‡è®¾$r_i$çš„ç»“æœä¼˜äº$r_j$ï¼Œé‚£ä¹ˆä¼˜åŒ–è¿‡ç¨‹å°±æ˜¯è®©æ¨¡å‹è¾“å‡ºå†…å®¹æ›´åŠ çš„â€œè´´åˆâ€$r_i$

> ä¸Šè¿°å…¬å¼æ¨ç†æ¯”è¾ƒç®€å•ï¼Œé€šè¿‡[bradley-terry æ¨¡å‹](https://baike.baidu.com/item/Bradley-Terry%20%E6%A8%A1%E5%9E%8B/24209136)å¯¹äºæ¨¡å‹çš„è¾“å‡º$r_j$è€Œæœ€ä¼˜çš„è¾“å‡º$r_i$è®¡ç®—æ¦‚ç‡ï¼š$P(r_j > r_i)=\frac{exp(R_{\phi}(p, r_i))}{exp(R_{\phi}(p, r_i))+ exp(R_{\phi}(p, r_j))}$åŒ–ç®€å°±å¯ä»¥å¾—åˆ°ï¼š$\sigma(R_{\phi}(p, r_i)- R_{\phi}(p, r_j))$

## DPOï¼ˆDirect Preference Optimizationï¼‰æ¨¡å‹

[DPO](https://github.com/hkproj/dpo-notes/blob/main/DPO_Final.pdf)ç›´æ¥æ ¹æ®äººç±»åå¥½æ•°æ®å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶ç”Ÿæˆæ›´ç¬¦åˆäººç±»æœŸæœ›çš„è¾“å‡ºï¼ŒæŸå¤±å‡½æ•°ä¸ºï¼š

$$\mathcal{L}_{\text{DPO}}(\pi_{\theta};\pi_{\text{ref}})=-\mathbb{E}_{(x,y_c,y_r)\sim\mathcal{D}}\left[\log\sigma\left(\beta\log\frac{\pi_{\theta}(y_c|x)}{\pi_{\text{ref}}(y_c|x)}-\beta\log\frac{\pi_{\theta}(y_r|x)}{\pi_{\text{ref}}(y_r|x)}\right)\right]$$

å…¶ä¸­ï¼š

$\pi_{\theta}$ï¼šå½“å‰ä¼˜åŒ–çš„è¯­è¨€æ¨¡å‹ï¼ˆç­–ç•¥æ¨¡å‹ï¼‰ã€‚
$\pi_{\text{ref}}$ï¼šå‚è€ƒæ¨¡å‹ï¼Œé€šå¸¸æ˜¯ç›‘ç£å¾®è°ƒåçš„æ¨¡å‹ï¼ˆSFT æ¨¡å‹ï¼‰ï¼Œç”¨äºç¨³å®šè®­ç»ƒã€‚
$\sigma$ï¼šSigmoid å‡½æ•°ï¼Œå°†åå¥½åˆ†æ•°æ˜ å°„åˆ° (0, 1)ã€‚
$\beta$ï¼šä¸€ä¸ªè¶…å‚æ•°ï¼Œæ§åˆ¶åå¥½å¼ºåº¦çš„ç¼©æ”¾ï¼ˆé€šå¸¸å–å€¼åœ¨ 0.1 åˆ° 1 ä¹‹é—´ï¼‰ã€‚
$y_c,y_r$ï¼šä¼˜é€‰å’ŒåŠ£é€‰å›ç­”ã€‚
$\mathcal{D}$ï¼šåå¥½æ•°æ®é›†ã€‚

ç›´è§‚ç†è§£ä¸Šé¢è¿‡ç¨‹ï¼ŒDPOæ¨¡å‹æ˜¯ä¸€ç§çº¯ç²¹æ•°æ®é©±åŠ¨çš„è®­ç»ƒèŒƒå¼ï¼Œæ¯”å¦‚è¯´ä¸‹é¢ä¾‹å­ï¼š

![](https://s2.loli.net/2025/04/24/OFJI5L3rhSjAsPY.png)

æ¨¡å‹çš„ä¼˜åŒ–ç›®æ ‡å°±æ˜¯éœ€è¦è®© **Bad answer**ç»å¯èƒ½çš„è´´è¿‘æˆ‘ä»¬çš„ **Good answer**å…¶å®ä»ä¸Šé¢æŸå¤±å‡½æ•°ä¹Ÿå¯ä»¥å‘ç°ã€‚å‡è®¾å­˜åœ¨æ•°æ®é›†$\mathcal{D}=(\text{Prompt}, \text{GoodAnswer}, \text{BadAnswer})$ï¼ˆç®€åŒ–ä¸ºï¼š$D=(x, y_c, y_r)$ï¼‰ï¼›æ¨¡å‹ï¼š$\pi_{\theta}$ã€$\pi_{\text{ref}}$ã€‚é‚£ä¹ˆDPOå¤„ç†è¿‡ç¨‹ä¸ºï¼š
**ç¬¬ä¸€æ­¥**ã€å°†è¾“å…¥æ•°æ®é€šè¿‡tokenizerå¤„ç†ç„¶åè¿›è¡Œæ‹¼æ¥å¾—åˆ°ä¸¤ä¸ªè¾“å…¥é˜Ÿåˆ—ï¼š $[x,y_c]$ ä»¥åŠ $[x,y_r]$
**ç¬¬äºŒæ­¥**ã€è®¡ç®—å¯¹æ•°æ¦‚ç‡ï¼Œç›´æ¥å°†ç»„åˆçš„æ•°æ®ä¼ åˆ°ä¸Šé¢å®šä¹‰çš„ä¸¤ä¸ªæ¨¡å‹ä¸­ä¹Ÿå°±æ˜¯è¯´ï¼š$out_c=\pi_{\theta}(x,y_c)$ ä¾æ¬¡ç±»æ¨ç„¶åç›´æ¥è·å–è¾“å‡ºçš„æ¦‚ç‡ï¼š$out_c.logits$è€Œåè®¡ç®—ä»–ä»¬çš„ $softmax$ä¸è¿‡åœ¨è¿™é‡Œéœ€è¦æ³¨æ„ä¸€ç‚¹ï¼Œè¾“å…¥æ˜¯ï¼š$[x,y_c]$ æ‹¼æ¥èµ·æ¥çš„åºåˆ—ï¼Œä½†æ˜¯æˆ‘ä»¬éœ€è¦çš„æ˜¯ $y_c$ éƒ¨åˆ†è¯çš„æ¦‚ç‡ï¼ï¼ï¼Œè¿™æ ·ä¸€æ¥ä¸Šé¢æŸå¤±å‡½æ•°ä¸­æ‰€æœ‰éœ€è¦è®¡ç®—çš„å€¼éƒ½è·å–äº†ã€‚æ›´åŠ æ•°å­¦çš„è¡¨ç¤ºï¼šè¾“å…¥ï¼š$[x,y_c]$è€Œåé€šè¿‡ $\pi_\theta$è¿›è¡Œå¤„ç†å¾—åˆ°ï¼š$logits= R^{1\times (n+m)\times V}$ï¼ˆéƒ½æ˜¯ä»£è¡¨é•¿åº¦ä»¥åŠè¯æ±‡è¡¨å¤§å°ï¼‰ä½†æ˜¯æˆ‘ä»¬éœ€è¦ç¬¦åˆ$y_c$ éƒ¨åˆ†å†…å®¹ï¼Œå› æ­¤éœ€è¦ç­›é€‰ï¼š$logits[0,n+t-1]$ï¼ˆè¦ç”¨n+t-1æ˜¯å› ä¸ºllmå¤§å¤šä¸ºè‡ªå›å½’$y_c$çš„ç¬¬ä¸€ä¸ªè¯ç”±$x$ æœ€åä¸€ä¸ªè¯å¾—åˆ°ï¼‰
**ç¬¬ä¸‰æ­¥**ã€åå‘ä¼ æ’­ã€‚

## PPOï¼ˆProximal Policy Optimizationï¼‰æ¨¡å‹

![](https://s2.loli.net/2025/03/25/FkLG7VDmEpg4hKS.png)

PPOæ˜¯ä¸€ç§åŸºäºç­–ç•¥æ¢¯åº¦çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡**é™åˆ¶ç­–ç•¥æ›´æ–°çš„å¹…åº¦**æ¥ä¿æŒè®­ç»ƒçš„ç¨³å®šæ€§ã€‚å…¶ç›®æ ‡å‡½æ•°ï¼ˆé€šè¿‡KLæ•£åº¦å¤„ç†ï¼‰ä¸ºï¼š

$$
L^{CLIP}(\theta)=\hat{\mathbb{E}}_{t}\left[\min(r_{t}(\theta)\hat{A}_{t},\operatorname{clip}(r_{t}(\theta),1-\epsilon,1+\epsilon)\hat{A}_{t})\right]
$$

$$
r_t(\theta)=\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}
$$


$\pi_\theta$: å½“å‰ç­–ç•¥å‚æ•°åŒ–çš„ç­–ç•¥å‡½æ•°
$A_t$: ä¼˜åŠ¿å‡½æ•°ï¼Œè¡¡é‡åŠ¨ä½œ$a_t$ç›¸å¯¹äºå¹³å‡æ°´å¹³çš„ä¼˜åŠ¿
$\epsilon$: è¶…å‚æ•°ï¼ˆé€šå¸¸0.1-0.2ï¼‰ï¼Œé™åˆ¶ç­–ç•¥æ›´æ–°çš„æœ€å¤§å¹…åº¦
â€‹**Clippingæœºåˆ¶**ï¼šé€šè¿‡æˆªæ–­é‡è¦æ€§é‡‡æ ·æ¯”ç‡ï¼Œé˜²æ­¢ç­–ç•¥æ›´æ–°è¿‡å¤§å¯¼è‡´è®­ç»ƒä¸ç¨³å®š

å¯¹äºä¸Šè¿°å…¬å¼é‡Œé¢ä¼˜åŠ¿å‡½æ•°$A_t$ï¼ˆç”¨æ¥è¡¡é‡çš„æ˜¯æŸä¸ªåŠ¨ä½œç›¸å¯¹äºå¹³å‡æ°´å¹³çš„ä¼˜åŠ¿ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™ä¸ªåŠ¨ä½œæ¯”å¹³å‡æƒ…å†µå¥½å¤šå°‘ï¼‰å…·ä½“è®¡ç®—å…¬å¼ä¸ºï¼š$A_t=Q(s_t, a_t)-V(s_t)$ï¼Œåˆ†åˆ«è¡¨ç¤ºï¼š1ã€$Q(s_t, a_t)$ï¼šåœ¨çŠ¶æ€$s_t$ä¸‹æ‰§è¡ŒåŠ¨ä½œ$a_t$å¾—åˆ°çš„æœŸæœ›æ±‡æŠ¥ï¼›2ã€$V(s_t)$ï¼šçŠ¶æ€$s_t$çš„å¹³å‡ç´¯è®¡æœŸæœ›ã€‚å¯¹äºå…¶è®¡ç®—å¯ä»¥é€šè¿‡GAEï¼ˆå¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼‰æ¥è¿›è¡Œè®¡ç®—ã€‚

é€šè¿‡ä¸Šé¢å›¾åƒæ¥ç†è§£ **PPO** è®­ç»ƒè¿‡ç¨‹ï¼Œé¦–å…ˆå¯¹äºæˆ‘ä»¬çš„è¾“å…¥$q=[x_1,...,x_n]$ å¯¹äº policy modelï¼ˆä¹Ÿå°±æ˜¯æˆ‘ä»¬å¼ºåŒ–å­¦ä¹ éœ€è¦ä¼˜åŒ–çš„æ¨¡å‹ æˆ–è€…ç§°ä¹‹ä¸ºï¼šactor modelï¼‰ä¼šå°†æ‰€æœ‰å†…å®¹è¾“å‡ºæˆ‘ä»¬çš„ç»“æœï¼š$o=[y_1,...,y_n]$ æ¥ä¸‹æ¥å°±æ˜¯å¦å¤–å‡ ä¸ªæ¨¡å‹è¦èµ·ä½œç”¨äº†ï¼šé¦–å…ˆæ˜¯ï¼šreward modelï¼Œå®ƒéœ€è¦åšçš„å°±æ˜¯å¯¹äºæˆ‘ä»¬å…¨éƒ¨çš„è¾“å‡ºè¿›è¡Œâ€œè¯„åˆ†â€ï¼Œå†…éƒ¨é€»è¾‘å°±æ˜¯ï¼ˆRewardModel(o)è€Œåé€šè¿‡ä¸€å±‚çº¿æ€§æ˜ å°„å‡ºæœ€åè¯„åˆ†è€Œåç”¨ **æœ€åä¸€ä¸ªè¯ä»£è¡¨è¾“å‡ºçš„æ•ˆæœå¦‚ä½•** ï¼‰


## GRPOï¼ˆGroup Relative Preference Optimizationï¼‰æ¨¡å‹

GRPOæ˜¯DPOçš„æ‰©å±•å½¢å¼ï¼Œå¤„ç†**ç»„çº§åˆ«**çš„åå¥½ä¼˜åŒ–é—®é¢˜ï¼Œå…¶æ ¸å¿ƒå…¬å¼ï¼š

![](https://s2.loli.net/2025/02/19/k6qf7PoUvxQJbnR.png)

$r^*$: ç»„å†…æœ€ä¼˜å“åº”
$\mathcal{R}$: åŒ…å«kä¸ªå“åº”çš„å€™é€‰é›†
KLé¡¹ï¼šé˜²æ­¢æ¨¡å‹è¿‡åº¦åç¦»åˆå§‹ç­–ç•¥ï¼Œç¼“è§£æ¨¡å¼åå¡Œ

é™¤å»ä¸Šè¿°å¤æ‚å…¬å¼ç›´æ¥é€šè¿‡ä¸‹é¢PPO/GRPOè¿‡ç¨‹è¿›è¡Œç†è§£

![](https://s2.loli.net/2025/03/25/FkLG7VDmEpg4hKS.png)

> å¼•ç”¨è®ºæ–‡ï¼ˆhttps://arxiv.org/pdf/2402.03300ï¼‰ ä¸­å¯¹äºPPOå’ŒGRPOçš„å¯¹æ¯”åˆ†æ

ä¸Šå›¾ä¸­å‡ ä¸ªæ¯”è¾ƒå…³é”®è¯ï¼š1ã€**Policy Model**ï¼šå³æˆ‘ä»¬éœ€è¦é€šè¿‡å¼ºåŒ–å­¦ä¹ ä¼˜åŒ–çš„æ¨¡å‹ï¼›2ã€**Reward Model**ï¼šå¥–åŠ±æ¨¡å‹ï¼Œå³å¯¹æ¨¡å‹åšå‡ºçš„å†³ç­–æ‰€ç»™å‡ºçš„åé¦ˆï¼ˆåˆ†ç±»æ‰“åˆ†çš„ï¼‰ï¼›3ã€**Value Model**ï¼šä¼°è®¡çŠ¶æ€çš„ä»·å€¼ï¼Œå¸®åŠ©æŒ‡å¯¼ç­–ç•¥ä¼˜åŒ–ï¼ˆåˆ†ç±»æ‰“åˆ†çš„ï¼‰ï¼›4ã€**Reference Model**ï¼šæä¾›å†å²ç­–ç•¥çš„å‚è€ƒï¼Œç¡®ä¿ä¼˜åŒ–è¿‡ç¨‹ä¸­ç­–ç•¥å˜åŒ–ä¸è¿‡åº¦ã€‚

> å‡è®¾ä¸ºä¼˜åŒ–Llamaï¼ˆå‡è®¾å‚æ•°ä¸º1Bï¼‰æ¨¡å‹ï¼Œé‚£ä¹ˆä¸Šè¿°4ä¸ªæ¨¡å‹åˆ†åˆ«ä»£è¡¨ï¼ˆç»“åˆhugging faceè®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨è§£é‡Š,å‚è€ƒå¦‚ä¸‹ä»£ç ï¼‰ï¼š
> 1ã€**Policy Model**ï¼šLlamaæ¨¡å‹æœ¬èº«ï¼›2ã€**Reward Model**ï¼šé€šå¸¸æ˜¯ä¸€ä¸ªæ›´åŠ å¼ºçš„æ¨¡å‹ï¼ˆæ¯”å¦‚è¯´Qwen2.5-13Bï¼‰;3ã€**Value Model**ä»¥åŠ **Reference Model**ï¼šå¯ä»¥ç›´æ¥ä½¿ç”¨ Llamaæœ¬èº«ã€‚

```python
value_model = AutoModelForSequenceClassification.from_pretrained(
    training_args.reward_model_path, trust_remote_code=model_args.trust_remote_code, num_labels=1
)
reward_model = AutoModelForSequenceClassification.from_pretrained(
    training_args.reward_model_path, trust_remote_code=model_args.trust_remote_code, num_labels=1
)
policy = AutoModelForCausalLM.from_pretrained(
    training_args.sft_model_path, trust_remote_code=model_args.trust_remote_code
)

peft_config = get_peft_config(model_args)
if peft_config is None:
    ref_policy = AutoModelForCausalLM.from_pretrained(
        training_args.sft_model_path, trust_remote_code=model_args.trust_remote_code
    )
else:
    ref_policy = None
trainer = PPOTrainer(
        args=training_args,
        processing_class=tokenizer,
        model=policy,
        ref_model=ref_policy,
        reward_model=reward_model,
        value_model=value_model,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        peft_config=peft_config,
    )
Code From:Code:https://github.com/shibing624/MedicalGPT/blob/main/ppo_training.py
```

é‚£ä¹ˆåœ¨PPO/GRPOæ•´ä½“ä¼˜åŒ–è¿‡ç¨‹ä¸­çš„å¤„ç†æ€è·¯ä¸ºï¼š
**é¦–å…ˆ**ï¼Œé€šè¿‡æœ€å¼€å§‹çš„æ¨¡å‹è¾“å‡ºæˆ‘ä»¬éœ€è¦çš„â€œç­”æ¡ˆâ€ä¹Ÿå°±æ˜¯å…¬å¼ä¸­çš„$\pi_{\theta_{old}}(o_i|q)$ï¼Œè€ŒPPOå’ŒGRPOåŒºåˆ«å°±åœ¨äºè¾“å‡ºçš„ç»“æœæ•°é‡ä¸Šçš„å·®å¼‚ï¼Œè€Œä¸Šè¿°å…¬å¼ä¸­æ‰€æåˆ°çš„ **ä¼˜åŠ¿å‡½æ•°**ï¼ˆ$A_t$ï¼‰åˆ™æ˜¯å¯¹äºæ¨¡å‹è¾“å‡ºç»“æœçš„è¯„åˆ†ï¼Œåœ¨PPOä¸­çŸ¥é“è®¡ç®—è¿‡ç¨‹ä¸ºï¼š$A_t=Q(s_t, a_t)-V(s_t)$è€Œåœ¨GRPOä¸­å¤„ç†çš„æ–¹å¼ä¸ºï¼š$A_i=\frac{r_i- \text{mean}(r_1,...,r_n)}{\text{std}(r_1,...,r_n)}$ï¼ˆå…¶ä¸­çš„$r$ä»£è¡¨å¯¹æ¯ä¸€ä¸ªå›ç­”ç»™å‡ºçš„è¯„åˆ†ï¼‰ï¼ŒGRPOçš„ä¼˜åŠ¿å‡½æ•°å¾ˆå¥½ç†è§£ï¼Œå¯¹äºPPOè®¡ç®—å…¬å¼ç†è§£ï¼š$Q(s_t, a_t)$å°±æ˜¯LLMåœ¨å½“å‰çŠ¶æ€ä¸‹å¯¹è¾“å‡ºæ‰€ç»™å‡ºçš„è¯„åˆ†å’ŒGRPOä¸­ç›¸ä¼¼ï¼Œ$V(s_t)$åˆ™æ˜¯è¡¨ç¤ºæœªæ¥æ‰€æœ‰å¯èƒ½ç”Ÿæˆçš„ token åºåˆ—çš„åŠ æƒå›æŠ¥æœŸæœ›å€¼ã€‚

> å¦‚ä½•ç†è§£$V(s_t)$å‘¢ï¼Ÿ
> é€šè¿‡PPOè®­ç»ƒLLMæ—¶å€™ï¼Œä¼šé€šè¿‡ **Pilicy Head**ä»¥åŠ **Value Head**æ¥è¿›è¡Œè°ƒèŠ‚ï¼Œç¬¬ä¸€ä¸ªå¾ˆå¥½ç†è§£å°±æ˜¯ç”¨æ¥è¾“å‡ºä¸‹ä¸€ä¸ªè¯ï¼Œç¬¬äºŒä¸ªå°±æ˜¯ç”¨æ¥è´Ÿè´£è¯„ä¼°å½“å‰ç”ŸæˆçŠ¶æ€çš„ä»·å€¼ $V(s_t)$

**è€Œå**ï¼Œé€šè¿‡è®¡ç®—KLæ•£åº¦æ¥ä¿è¯æ›´æ–°æ¨¡å‹â€œåç¦»â€ä¸ä¼šå¤ªè¿œï¼Œé€šè¿‡è®¡ç®—ä¼˜åŒ–æ¨¡å‹ä¸Reference Modelä¹‹é—´KLæ•£åº¦æ¥è¿›è¡Œé™åˆ¶ã€‚

## å‚è€ƒ
1ã€https://ghost.oxen.ai/why-grpo-is-important-and-how-it-works/
2ã€https://yugeten.github.io/posts/2025/01/ppogrpo/
3ã€https://arxiv.org/pdf/2501.12948
4ã€https://arxiv.org/pdf/2402.03300
5ã€https://arxiv.org/pdf/2305.18290
6ã€https://github.com/hkproj/dpo-notes/blob/main/DPO_Final.pdf
7ã€[æå®æ¯…-ä»‹ç»RLè¯¾ç¨‹](https://www.youtube.com/watch?v=W8XF3ME8G2I)
8ã€[æå®æ¯…-RLç³»åˆ—è¯¾ç¨‹](https://www.youtube.com/playlist?list=PLJV_el3uVTsODxQFgzMzPLa16h6B8kWM_)
9ã€https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow
10ã€https://youtu.be/JZZgBu8MV4Q?si=Tr7QC6srxkZJPdsI
11ã€TRPOï¼šhttps://arxiv.org/pdf/1502.05477
12ã€PPOï¼šhttps://arxiv.org/pdf/1707.06347