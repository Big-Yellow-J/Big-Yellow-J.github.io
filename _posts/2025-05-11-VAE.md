---
layout: mypost
title: 深入浅出了解生成模型-2：VAE模型原理以及代码实战
categories: 生成模型
extMath: true
images: true
address: changsha
show_footer_image: true
tags: [生成模型, VAE]
description: 日常使用比较多的生成模型比如GPT/Qwen等这些大多都是“文生文”模型（当然GPT有自己的大一统模型可以“文生图”）但是网上流行很多AI生成图像，而这些生成图像模型大多都离不开下面三种模型：1、GAN；2、VAE；3、Diffusion Model。因此本文通过介绍这三个模型作为生成模型的入门。本文主要介绍GAN模型
---

前文已经介绍了GAN的基本原理以及代码操作，本文主要介绍VAE其基本原理以及代码实战

## VAE or AE
介绍VAE之前了解两个概念：AE（AutoEncoder，自编码器）和VAE（Variational Autoencoder，变自编码器）。**AE**：自编码器是一种无监督学习神经网络，旨在通过将输入数据压缩到一个低维表示（编码），然后从该表示重建输入数据（解码），来学习数据的特征表示。**VAE**：变分自编码器是自编码器的扩展，结合了概率模型和深度学习，通过引入变分推理使潜在空间具有概率分布特性，适合生成任务。
**AE**的数学描述对于输入 $x$通过编码器将输入映射到 **低纬空间** $z=f(x)$而后通过解码器得到输出：$\hat{x}=g(x)$
**VAE**的数学描述对于输入 $x$通过编码器将输入映射成 **概率分布** $q(z\vert x)$，假设为高斯分布，输出 𝜇和 𝜎，从 $q(z\vert x)$采样 $z$而后通过 $z=\mu+ \sigma+ \epsilon$ 其中 $\epsilon \in N(0,1)$，而后通过采样得到的$z$重新构建输入，生成$p(x\vert z)$
前者不适合对于图片进行生成而后者则是更加适合图像生成，这是因为AE将输入映射到一个低纬空间z这个低纬空间并没有明确的结构，进而就可能不适合去生成新的数据，而VAE之所以可以用于生成新的数据是，比如说对于图像数据（比如说：猫）如果知道其分布特征，就可以直接通过分布特征去构建一个新的图像

![](https://s2.loli.net/2025/05/13/Ji5sAMmGehLjdf9.png)

## VAE（Variational Autoencoder）
上面简单介绍了VAE数学描述这里重新再描述一下其数学描述（涉及到比较多贝叶斯统计相关内容）：
### 1.基本框架
VAE 是一种生成模型，**目标是学习数据的概率分布 p(x)，让模型能生成类似真实数据的新样本**，想象我们要制作各种蛋糕（数据 $x$），但不知道蛋糕的“秘方”（潜在变量 $z$）。假设所有蛋糕组成数据集 $X = {x_1, \dots, x_n}$，每种蛋糕（如巧克力蛋糕或水果蛋糕）背后有独特的秘方。VAE 通过学习秘方的分布和生成过程，制造出逼真的蛋糕。
**秘方**：VAE 假设秘方 $z$ 服从标准正态分布，即先验分布 $p(z) = \mathcal{N}(0, I)$。这意味着大多数秘方是“普通”的，围绕平均值分布。
**生成蛋糕（解码器）**：给定秘方 $z$，VAE 使用一个“蛋糕机”（解码器，参数 $\theta$）生成蛋糕 $x$。解码器建模条件分布 $p_\theta(x\vert z) = \mathcal{N}(x; \mu_\theta(z), \sigma_\theta^2(z))$，表示从 $z$ 生成 $x$ 的概率。
**猜测秘方（编码器）**：直接从蛋糕 $x$ 反推秘方（后验分布 $p_\theta(z\vert x)=\frac{p_\theta(x,z)}{p_\theta(x)}=\frac{p_\theta(x\vert )p(z)}{p_\theta(x)}$）很困难（因为我的变量是一个高维的，换句说法就是我的蛋糕他有千奇百怪种组合）。既然如此就只需要将制造蛋糕的组合分解，分解成低维的变量 $z$（也就是上面提到的 **秘方**）然后我去计算下面一个联合分布（$p(x,z)$）：

$$
p_\theta(x) =\int p_\theta(x\vert z)p(z)dz
$$

不过就算上面积分会存在困难即使你将蛋糕分解成不同的 *潜在变量* 但是这些潜在变量种类也是很多的（蛋糕奶油、加不加巧克力等等）那么上面的联合分布就会变成：

$$
\int p_\theta(x\vert z)p(z)dz = \int_{z_1} ... \int_{z_d}p_\theta(x\vert z)p(z)d_{z_1}...d_{z_d}
$$

这种高维积分没有解析解，数值积分计算复杂度随维度指数增长，因此在VAE 引入一个“猜测机”（编码器，也就是一个神经网络，参数 $\varphi$），用变分分布：

$$
q_\varphi(z\vert x) = \mathcal{N}(z; \mu_\varphi(x), \text{diag}(\sigma_\varphi^2(x)))
$$ 

近似后验分布，估计可能的秘方也就是去估算我们的：$p_\theta(z\vert x)$

> 再去引入新的参数 $\varphi$ 不还是很难计算吗？似乎是这么一回事，但是回顾我们需要解决的问题：$p_\theta(z\vert x)=\frac{p_\theta(x,z)}{p_\theta(x)}=\frac{p_\theta(x\vert z)p(z)}{p_\theta(x)}$ 分子分母三项都是很难计算的那么也就意味着如果要通过 $x$ “推算” $z$ 基本就是很难解决的问题，那么“干脆”不去计算用神经网络进行“模拟”也就是说 $p_\theta(z\vert x) ≈ p_\varphi(z\vert x)$

大致总结一下就是：VAE的主要的任务就是，最开始的数据集里面，我希望通过对这个数据的潜在分布进行学习，如果我模型学会了各类数据的分布，那么就可以通过这些分布去进一步生成新的数据。

### 2.损失函数构建
了解模型基本框架之后就需要对整个模型的参数进行求解，正如上面所述对于数据集分布 $p_\theta(x)$ 的计算我们通过构建一个和 $z$的联合分布，因此整个过程计算如下：

$$
\begin{align}
\log p_{\theta}(x) &= \log p_{\theta}\left(x\right) \tag{1} \\
&= \log p_{\theta}\left(x\right)\int q_{\varphi}(z\vert x)dz \tag{2} \\
&= \int\log p_{\theta}\left(x\right)q_{\varphi}(z\vert x)dz \tag{3} \\
&= \mathbb{E}_{q_{\varphi}(\mathbf{Z}\vert\mathbf{X})}[\log p_{\theta}(\mathbf{x})] \tag{4} \\
&= \mathbb{E}_{q_{\varphi}(\mathbf{Z}\vert\mathbf{X})}\left[\log\frac{p_{\theta}(\mathbf{x},\mathbf{z})}{p_{\theta}(\mathbf{z}\vert\mathbf{x})}\right] \tag{5} \\
&= \mathbb{E}_{q_{\varphi}(\mathbf{Z}\vert\mathbf{X})}\left[\log\frac{p_{\theta}(\mathbf{x},\mathbf{z})q_{\varphi}(\mathbf{z}\vert\mathbf{x})}{p_{\theta}(\mathbf{z}\vert\mathbf{x})q_{\varphi}(\mathbf{z}\vert\mathbf{x})}\right] \tag{6} \\
&= \mathbb{E}_{q_{\varphi}(\mathbf{z}\vert\mathbf{x})}\left[\log\frac{p_{\theta}(\mathbf{x},\mathbf{z})}{q_{\varphi}(\mathbf{z}\vert\mathbf{x})}\right] + \mathbb{E}_{q_{\varphi}(\mathbf{z}\vert\mathbf{x})}\left[\log\frac{q_{\varphi}(\mathbf{z}\vert\mathbf{x})}{p_{\theta}(\mathbf{z}\vert\mathbf{x})}\right] \tag{7} \\
&= \mathbb{E}_{q_{\varphi}(\mathbf{z}\vert\mathbf{x})}\left[\log\frac{p_{\theta}(\mathbf{x},\mathbf{z})}{q_{\varphi}(\mathbf{z}\vert\mathbf{x})}\right] + \underbrace{D_{\mathrm{KL}}\left(q_{\varphi}(\mathbf{z}\vert\mathbf{x}) \parallel p_{\theta}(\mathbf{z}\vert\mathbf{x})\right)}_{\geq 0} \tag{8} \\
&\geq \mathbb{E}_{q_{\varphi}(\mathbf{z}\vert\mathbf{x})}\left[\log\frac{p_{\theta}(\mathbf{x},\mathbf{z})}{q_{\varphi}(\mathbf{z}\vert\mathbf{x})}\right] \quad (\text{ELBO}) \tag{9}
\end{align}
$$

> 其中 $\text{ELBO}$也就是所谓的变分下界

（1-3）上面介绍对于联合分布 $p_\theta(x) =\int p_\theta(x\vert z)p(z)dz$ 计算存在困难，因此替换为 $p_\theta(x) =\int p_\varphi(x\vert z)p(z)dz$；（4-7）直接就是贝叶斯公式和一些基本变形；（8）最后一项就是 $KL$散度。最后上面公式就可以写成：

$$
\begin{aligned}
\log p_{\theta}(x)\geq E_{q_{\varphi}(\mathbf{z}|\mathbf{x})}\left[\log\frac{p_{\theta}(\mathbf{x},\mathbf{z})}{q_{\varphi}(\mathbf{z}|\mathbf{x})}\right] & =E_{q_{\varphi}(\mathbf{Z}|\mathbf{X})}\left[\log\frac{p_{\theta}(\mathbf{x}|\mathbf{z})p(\mathbf{z})}{q_{\varphi}(\mathbf{z}|\mathbf{x})}\right] \\
 & =E_{q_{\varphi}(\mathbf{Z}|\mathbf{X})}[\log p_{\theta}(\mathbf{x}|\mathbf{z})]+E_{q_{\varphi}(\mathbf{Z}|\mathbf{X})}\left[\log\frac{p(\mathbf{z})}{q_{\varphi}(\mathbf{z}|\mathbf{x})}\right] \\
 & =E_{q_{\varphi}(\mathbf{Z}|\mathbf{X})}[\log p_{\theta}(\mathbf{x}|\mathbf{z})]-D_{KL}\left(q_{\varphi}(\mathbf{z}|\mathbf{x})\|p_{\theta}(\mathbf{z})\right)
\end{aligned}
$$

那么我们的损失函数就是（最大化下面计算）：

$$
L(\theta, \varphi,x)= E_{q_{\varphi}(\mathbf{z}|\mathbf{x})}\left[\log\frac{p_{\theta}(\mathbf{x},\mathbf{z})}{q_{\varphi}(\mathbf{z}|\mathbf{x})}\right]=E_{q_{\varphi}(\mathbf{Z}|\mathbf{X})}[\log p_{\theta}(\mathbf{x}|\mathbf{z})]-D_{KL}(q_{\varphi}(\mathbf{z}|\mathbf{x})\|p_{\theta}(\mathbf{z}))
$$

在深度学习中自然就会直接用 *梯度下降*的方法去优化参数，下面推荐借鉴：[https://arxiv.org/pdf/1906.02691](https://arxiv.org/pdf/1906.02691) 中的描述
#### 2.1 参数 $\theta$ 计算

对于 $\theta$参数可以得到：

$$
\begin{align}
\nabla_{\theta,\varphi} \mathcal{L}(x,\theta,\varphi) &= \nabla_{\theta} \mathbb{E}_{q_\varphi(z|x)} \left[ \log p_\theta(x,z) - \log q_\varphi(z|x) \right] \\
&= \mathbb{E}_{q_\varphi(z|x)} \left[ \nabla_{\theta} (\log p_\theta(x,z) - \log q_\varphi(z|x)) \right] \\
&\approx \nabla_{\theta} (\log p_\theta(x,z) - \log q_\varphi(z|x)) \\
&= \nabla_{\theta} (\log p_\theta(x,z))
\end{align}
$$

（1-2）也需要注意之所以可以将梯度拿到期望里面（直接借鉴 grok里面解释）

![](https://s2.loli.net/2025/05/16/DrLX36eKG5whNPk.png)

（3）这是因为在后面一项中参数是 $\varphi$ 因此计算梯度直接为0因此就可以拿掉，对于下面公式：

$$
\mathbb{E}_{q_\varphi(z|x)} [\nabla_{\theta} (\log p_\theta(x,z)] \rightarrow \nabla_{\theta} (\log p_\theta(x,z))
$$

一个容易接受的说法：可以直接通过蒙特卡洛方法，通过从分布中抽取少量样本（甚至单样本）来近似期望值。因此对于期望 

$$\mathbb{E}_{q_\varphi(z\vert x)}[ \nabla_\theta \log p_\theta(x, z)] 
\approx \frac{1}{S} \sum_{s=1}^S \nabla_\theta \log p_\theta(x, z)
$$ 

计算，可以去通过从 $q_\varphi(z\vert x)$ 进行抽样，当 $S=1$ 时候就直接变成单个样本去近似整个期望。（单样本估计的方差可以通过SGD的多次迭代和批量数据的处理来缓解，模型会在优化过程中逐渐收敛到一个较好的解（通常是局部最优或接近全局最优））

#### 2.2 参数 $\varphi$ 计算
对于参数 $\varphi$不能直接像上面计算一样直接将 $\nabla$ 拿到期望里面（期望和分布都依赖参数 $\varphi$）
> 下面是成立的并且可以直接求导变为0
> $$
> \mathbb{E}_{q_\varphi(z\vert x)}[\nabla_\theta \log p_\theta(x, z)]
> $$ 

## 3.代码操作
所有的代码：1、[VAE](./code/../../code/VAE.py.txt)；2、[PixelCNN](../code/PixcelCNN.py.txt)

### 实际生成效果
> VAE测试的主要是生成效果（MNIST数据集），而VA-VAE则是测试重构效果

**VAE**在MNIST数据集上表现

| 固定输入 | 重构图像 | 随机生成 |
|:--------:|:---------:|:--------:|
| ![](https://s2.loli.net/2025/05/14/ABJDCTjW46Odx3g.gif) |![](https://s2.loli.net/2025/05/14/fQmMFOXRNovIubA.gif) | ![](https://s2.loli.net/2025/05/14/28Tln5qcap6HvPf.gif) |

不过值得注意的是 MNIST数据集很简单所以VAE可以很容易就生成需要的图片
**VQ-VAE**在CIFAR10数据集上重构图像的表现（**生成图像只是测试代码运行效果**，CIFAR10数据集自身也比较复杂！，在CIFAR10上都没能生成较好的图片）

|重构图像 | PixelCNN生成图像 | PixelCNNPlusPlus 生成图像 | GatedPixelCNN 生成图像 |
|:------:|:----------------:|:-----------------------:|:--------------------:|
|![](https://s2.loli.net/2025/05/14/38zIQHaL9Vukret.gif)| ![](https://s2.loli.net/2025/05/15/DHvAXfZIRimwOen.gif) | ![](https://s2.loli.net/2025/05/15/mQqgLNF83uzoZiJ.gif) | ![](https://s2.loli.net/2025/05/15/gDcBlhsyS1or8Iu.gif)|

## 总结

## 参考
1、https://github.com/hkproj/vae-from-scratch-notes/blob/main/VAE.pdf
2、https://mbernste.github.io/posts/vae/
3、https://arxiv.org/pdf/1906.02691
