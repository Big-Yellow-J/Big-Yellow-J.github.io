---
layout: mypost
title: 深入浅出了解生成模型-2：VAE模型原理以及代码实战
categories: 生成模型
extMath: true
images: true
address: changsha
show_footer_image: true
tags: [生成模型, VAE]
description: 日常使用比较多的生成模型比如GPT/Qwen等这些大多都是“文生文”模型（当然GPT有自己的大一统模型可以“文生图”）但是网上流行很多AI生成图像，而这些生成图像模型大多都离不开下面三种模型：1、GAN；2、VAE；3、Diffusion Model。因此本文通过介绍这三个模型作为生成模型的入门。本文主要介绍GAN模型
---

前文已经介绍了GAN的基本原理以及代码操作，本文主要介绍VAE其基本原理以及代码实战

## VAE or AE
介绍VAE之前了解两个概念：AE（AutoEncoder，自编码器）和VAE（Variational Autoencoder，变自编码器）。**AE**：自编码器是一种无监督学习神经网络，旨在通过将输入数据压缩到一个低维表示（编码），然后从该表示重建输入数据（解码），来学习数据的特征表示。**VAE**：变分自编码器是自编码器的扩展，结合了概率模型和深度学习，通过引入变分推理使潜在空间具有概率分布特性，适合生成任务。
**AE**的数学描述对于输入 $x$通过编码器将输入映射到 **低纬空间** $z=f(x)$而后通过解码器得到输出：$\hat{x}=g(x)$
**VAE**的数学描述对于输入 $x$通过编码器将输入映射成 **概率分布** $q(z|x)$，假设为高斯分布，输出 𝜇和 𝜎，从 $q(z|x)$采样 $z$而后通过 $z=\mu+ \sigma+ \epsilon$ 其中 $\epsilon \in N(0,1)$，而后通过采样得到的$z$重新构建输入，生成$p(x|z)$
前者不适合对于图片进行生成而后者则是更加适合图像生成，这是因为AE将输入映射到一个低纬空间z这个低纬空间并没有明确的结构，进而就可能不适合去生成新的数据，而VAE之所以可以用于生成新的数据是，比如说对于图像数据（比如说：猫）如果知道其分布特征，就可以直接通过分布特征去构建一个新的图像

![](https://s2.loli.net/2025/05/11/8kYHhJuSrbZvpWD.png)

## VAE（Variational Autoencoder）
上面简单介绍了VAE数学描述这里重新再描述一下其数学描述：
### 1.基本框架
VAE 是一种生成模型，**目标是学习数据的概率分布 p(x)，让模型能生成类似真实数据的新样本**，假设观测数据 ($\mathbf{x}$) 由潜在变量 ( $\mathbf{z}$ ) 生成，目标是学习数据的概率分布 ( $p(\mathbf{x})$ )。VAE通过引入变分推断来近似后验分布 ( $p(\mathbf{z}|\mathbf{x})$ )，并优化证据下界（ELBO）。
* **概率模型**

**先验分布**：潜在变量 ( $\mathbf{z}$ ) 通常假设服从标准正态分布： $p(\mathbf{z}) = \mathcal{N}(\mathbf{z}; \mathbf{0}, \mathbf{I}) $
**生成模型**：给定潜在变量 ( $\mathbf{z}$ )，生成数据的条件分布为： $p_\theta(\mathbf{x}|\mathbf{z})$ 其中 ( $\theta$ ) 是生成网络（解码器）的参数，通常建模为高斯分布： 

$$ 
p_\theta(\mathbf{x}|\mathbf{z}) = \mathcal{N}(\mathbf{x}; \mu_\theta(\mathbf{z}), \sigma_\theta^2(\mathbf{z})\mathbf{I}) 
$$

**联合分布**（直接通过贝叶斯计算）： $p_\theta(\mathbf{x}, \mathbf{z}) = p_\theta(\mathbf{x}|\mathbf{z}) p(\mathbf{z})$

* **变分推断**

由于后验分布（$p(\mathbf{z}|\mathbf{x})$）难以直接计算，VAE引入变分分布（$q_\phi(\mathbf{z}|\mathbf{x})$）来近似，其中（$\phi$）是推断网络（编码器）的参数。通常假设： 

$$
q_{\phi}(\mathbf{z}|\mathbf{x}) = \mathcal{N}(\mathbf{z}; \mu_{\phi}(\mathbf{x}), \text{diag}(\sigma_{\phi}^2(\mathbf{x})))
$$

目标是通过最小化 ( $q_\phi(\mathbf{z}|\mathbf{x})$ ) 和真实后验 ( $p(\mathbf{z}|\mathbf{x})$ ) 之间的KL散度来优化 ( $\phi$ ) 和 ( $\theta$ )。
对于复杂的数学公式，我们可以用一个简单例子解释：想象要制作各种蛋糕（数据 $\mathbf{x}$），但不知道蛋糕的“秘方”（潜在变量 $\mathbf{z}$）。VAE 的任务是：1、猜出每种蛋糕的秘方（推断 $\mathbf{z}$）。2、用秘方制作蛋糕（生成 $\mathbf{x}$）。

**秘方（潜在变量 $\mathbf{z}$）**：VAE 假设秘方服从标准正态分布（先验分布 $ p(\mathbf{z}) = \mathcal{N}(\mathbf{0}, \mathbf{I}) $），就像“大多数秘方都比较普通”。
**生成蛋糕（解码器）**：给定秘方 $\mathbf{z}$，通过一个“蛋糕机”（解码器，参数 $\theta$）生成蛋糕 $\mathbf{x}$，即计算条件分布 $ p_{\theta}(\mathbf{x}|\mathbf{z}) = \mathcal{N}(\mathbf{x}; \mu_{\theta}(\mathbf{z}), \sigma_{\theta}^2(\mathbf{z})\mathbf{I}) $。
**猜测秘方（编码器）**：直接从蛋糕反推秘方（后验分布 $ p(\mathbf{z}|\mathbf{x}) $）很困难。VAE 用一个“猜测机”（编码器，参数 $\phi$）来近似推断秘方，建模为 $ q_{\phi}(\mathbf{z}|\mathbf{x}) = \mathcal{N}(\mathbf{z}; \mu_{\phi}(\mathbf{x}), \text{diag}(\sigma_{\phi}^2(\mathbf{x}))) $。
**优化目标**：通过最小化 $ q_{\phi}(\mathbf{z}|\mathbf{x}) $ 与真实后验 $ p(\mathbf{z}|\mathbf{x}) $ 的 KL 散度，优化编码器和解码器。实际优化证据下界（ELBO），平衡重构蛋糕（生成逼真 $\mathbf{x}$）和秘方简单性（接近先验）。
VAE 像一个智能厨师：用编码器猜秘方，用解码器做蛋糕，训练后能从普通秘方生成新蛋糕！

### 2.**目标函数：证据下界（ELBO）**
VAE的优化目标是最大化数据的边际似然 ( $\log p(\mathbf{x})$ )。通过变分推断，边际似然可以分解为： 

$$ 
\log p(\mathbf{x}) = \mathbb{E}{q{\phi}(\mathbf{z}|\mathbf{x})}[\log p_{\theta}(\mathbf{x}, \mathbf{z}) - \log q_{\phi}(\mathbf{z}|\mathbf{x})] + D_{KL}(q_{\phi}(\mathbf{z}|\mathbf{x}) || p(\mathbf{z}|\mathbf{x})) 
$$

由于KL散度非负，证据下界（ELBO）定义为： 

$$ 
\mathcal{L}(\theta, \phi; \mathbf{x}) = \mathbb{E}{q{\phi}(\mathbf{z}|\mathbf{x})}[\log p_{\theta}(\mathbf{x}, \mathbf{z}) - \log q_{\phi}(\mathbf{z}|\mathbf{x})] 
$$

展开后，ELBO可以写为： 

$$ 
\mathcal{L}(\theta, \phi; \mathbf{x}) = \mathbb{E}{q{\phi}(\mathbf{z}|\mathbf{x})}[\log p_{\theta}(\mathbf{x}|\mathbf{z})] - D_{KL}(q_{\phi}(\mathbf{z}|\mathbf{x}) || p(\mathbf{z})) 
$$

第一项是重构误差，表示从潜在变量生成数据的期望似然。第二项是KL散度，鼓励变分分布接近先验分布。

### 3.**重参数化技巧**

为了使ELBO可微并能够通过梯度下降优化，VAE使用重参数化技巧。假设： 

$$ 
\mathbf{z} = \mu_{\phi}(\mathbf{x}) + \sigma_{\phi}(\mathbf{x}) \odot \epsilon, \quad \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) 
$$

这样，期望可以通过蒙特卡洛估计计算，梯度可以直接传播到 ( $\mu_\phi$ ) 和 ( $\sigma_\phi$ )。

### 4.KL散度的解析形式

当（$q_\phi(\mathbf{z}|\mathbf{x})$）和（$p(\mathbf{z})$）均为高斯分布时，KL散度有解析解： 

$$ 
D_{KL}(q_{\phi}(\mathbf{z}|\mathbf{x}) || p(\mathbf{z})) = \frac{1}{2} \sum_{j=1}^J \left( \mu_{\phi,j}^2(\mathbf{x}) + \sigma_{\phi,j}^2(\mathbf{x}) - 1 - \log \sigma_{\phi,j}^2(\mathbf{x}) \right) 
$$ 

其中 ( J ) 是潜在变量的维度。

### 5.优化

VAE的训练目标是最小化负ELBO： 

$$ 
\min_{\theta, \phi} -\mathcal{L}(\theta, \phi; \mathbf{x}) 
$$

通常使用随机梯度下降（SGD）或其变种（如Adam）优化参数 ( $\theta$ ) 和 ( $\phi$ )，通过小批量数据估计梯度
## 总结

## 参考
1、https://github.com/hkproj/vae-from-scratch-notes/blob/main/VAE.pdf