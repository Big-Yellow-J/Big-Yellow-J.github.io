---
layout: mypost
title: æ·±å…¥æµ…å‡ºäº†è§£ç”Ÿæˆæ¨¡å‹-5ï¼šdiffuser/accelerateåº“å­¦ä¹ åŠå…¶æ•°æ®åˆæˆ
categories: ç”Ÿæˆæ¨¡å‹
extMath: true
images: true
address: æ­¦æ±‰ğŸ¯
show_footer_image: true
tags:
- ç”Ÿæˆæ¨¡å‹
- diffusion model
- python
show: true
stickie: true
description: æœ¬æ–‡ä»‹ç»ç”Ÿæˆæ¨¡å‹å¼€å‘å¸¸ç”¨Pythonåº“ï¼Œé‡ç‚¹è®²è§£Diffuserså’ŒAccelerateçš„åŸºæœ¬ä½¿ç”¨ã€‚Accelerateæ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒã€æ··åˆç²¾åº¦è®­ç»ƒã€æ¢¯åº¦ç´¯è®¡ç­‰åŠ é€Ÿæ–¹æ³•ï¼Œç®€åŒ–å¤šæ˜¾å¡è®­ç»ƒæµç¨‹ï¼›DiffusersåŒ…å«Schedulerï¼ˆåŠ å™ªå¤„ç†ã€é€æ­¥è§£å™ªï¼‰ã€Stable
  Diffusion Pipelineç­‰ï¼Œè¾…åŠ©å®ç°ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒä¸æ¨ç†ï¼Œä¸ºç®—æ³•å·¥ç¨‹å¸ˆæä¾›é«˜æ•ˆå·¥å…·æ”¯æŒã€‚
---

å·¥æ¬²å–„å…¶äº‹ï¼Œå¿…å…ˆåˆ©å…¶å™¨ã€‚å³ä¾¿ä»‹ç»äº†å†å¤šç”Ÿæˆæ¨¡å‹ï¼Œæ²¡æœ‰è¶æ‰‹çš„å·¥å…·ä¹Ÿéš¾ä»¥æ–½å±•æ‰åã€‚å› æ­¤ï¼Œæœ¬æ–‡å°†é‡ç‚¹ä»‹ç»å‡ ä¸ªåœ¨ç”Ÿæˆæ¨¡å‹å¼€å‘ä¸­å¸¸ç”¨çš„ Python åº“ï¼Œç€é‡è®²è§£ **Diffusers** å’Œ **Accelerate** çš„åŸºæœ¬ä½¿ç”¨ã€‚æ„Ÿè°¢ Hugging Face ä¸ºæ— æ•°ç®—æ³•å·¥ç¨‹å¸ˆæä¾›äº†å¼ºå¤§çš„å¼€æºæ”¯æŒï¼éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå®˜æ–¹æ–‡æ¡£å¯¹è¿™ä¸¤ä¸ªåº“å·²æœ‰è¯¦å°½çš„è¯´æ˜ï¼Œæœ¬æ–‡ä»…ä½œä¸ºä¸€ç¯‡ç®€æ˜çš„ä½¿ç”¨ç¬”è®°ï¼ŒæŠ›ç –å¼•ç‰ï¼Œä¾›å‚è€ƒå’Œäº¤æµã€‚

## accelerate
> æ¨èç›´æ¥é˜…è¯»å®˜æ–¹æ–‡æ¡£ï¼š[https://huggingface.co/docs/accelerate/index](https://huggingface.co/docs/accelerate/index)
> [`pip install accelerate`](https://huggingface.co/docs/accelerate/basic_tutorials/install)

ä»‹ç»ä¹‹å‰äº†è§£ä¸€ä¸‹è¿™ä¸ªåº“æ˜¯å¹²ä»€ä¹ˆçš„ï¼šè¿™ä¸ªåº“ä¸»è¦æä¾›ä¸€ä¸ªå¿«é€Ÿçš„åˆ†å¸ƒå¼è®­ç»ƒï¼ˆé¿å…äº†ç›´æ¥ç”¨torchè¿›è¡Œæ‰‹æ“ï¼‰å¹¶ä¸”æ”¯æŒå„ç±»åŠ é€Ÿæ–¹æ³•ï¼š[æ··åˆç²¾åº¦è®­ç»ƒ](https://www.big-yellow-j.top/posts/2025/01/01/mixed-precision.html)ã€[Deepspeed](https://www.big-yellow-j.top/posts/2025/02/24/deepspeed.html)ã€æ¢¯åº¦ç´¯è®¡ç­‰

### ä¸€ä¸ªåŸºæœ¬ä½¿ç”¨åœºæ™¯
ä¸€èˆ¬ä»»åŠ¡ä¸­ä¸€ä¸ªå¸¸è§çš„åº”ç”¨åœºæ™¯æ˜¯ï¼šéœ€è¦å®ç°ä¸€ä¸ªå¤šæ˜¾å¡ï¼ˆè¿™é‡Œå‡è®¾ä¸ºåŒæ˜¾å¡ï¼‰åˆ†å¸ƒå¼è®­ç»ƒï¼Œå¹¶ä¸”ä½¿ç”¨æ¢¯åº¦ç´¯è®¡ã€æ··åˆç²¾åº¦è®­ç»ƒï¼Œå¹¶ä¸”è®­ç»ƒå¾—åˆ°çš„ç»“æœé€šè¿‡tensorboard/wandbè¿›è¡Œè®°å½•ï¼Œé™¤æ­¤ä¹‹å¤–è¿˜éœ€è¦ä½¿ç”¨warm-upå­¦ä¹ ç‡è°ƒæ•´ç­–ç•¥ï¼Œå¹¶ä¸”æˆ‘çš„æ¨¡å‹ä¸åŒæ¨¡å—ä½¿ç”¨çš„å­¦ä¹ ç‡ä¸åŒï¼Œè®­ç»ƒå®Œæˆä¹‹åæ‰€æœ‰çš„æ¨¡å‹æƒé‡è¦è¿›è¡Œä¿å­˜/è¯»å–æƒé‡è¿›è¡Œæµ‹è¯•ã€‚é‚£ä¹ˆå¯ä»¥ç›´æ¥é€šè¿‡ä¸‹é¢ä»£ç è¿›è¡Œå®ç°ï¼ˆéƒ¨åˆ†åº“çš„å¯¼å…¥ä»¥åŠä¸€äº›å‚æ•°æ¯”å¦‚è¯´configç›´æ¥å¿½ç•¥ï¼‰

```python
from accelerate import Accelerator
kwargs_handlers=[DistributedDataParallelKwargs(find_unused_parameters=True)] # ä¸æ˜¯å¿…é¡»çš„
# Step-1 é¦–å…ˆåˆå§‹åŒ– accelerate
accelerator = Accelerator(mixed_precision= 'fp16', 
                            gradient_accumulation_steps= 2,
                            log_with= ['tensorboard', 'wandb'], # ä¸€èˆ¬æ¥è¯´ç”¨ä¸€ä¸ªå³å¯
                            project_dir=os.path.join(config.output_dir, "logs"),
                            kwargs_handlers= kwargs_handlers
                            )
# ä»…åœ¨ä¸»çº¿ç¨‹ä¸Šåˆ›å»ºæ–‡ä»¶å¤¹
if accelerator.is_main_process: 
    os.makedirs(config.output_dir, exist_ok=True)
    # åˆå§‹åŒ–ä¸€ä¸ªå®éªŒè®°å½•å™¨ï¼ˆæ­¤å¤„å†…å®¹éœ€è¦æ³¨æ„â­ï¼‰
    # accelerator.init_trackers(f"Train-{config.training}")
    log_name = 'Model-Test'
    accelerator.init_trackers(
        project_name= f"Page-Layout-Analysis-{config.pred_heads}",
        init_kwargs={
            "wandb": {
                "name": log_name,
                "dir": os.path.join(config.output_dir, "logs"),
                "config": vars(config)
            }
        }
        )
 
# Step-2 åˆå§‹åŒ–å®Œæˆä¹‹åå¯ä»¥ç›´æ¥å°†æˆ‘ä»¬éœ€è¦çš„å†…å®¹é€šè¿‡ accelerator.prepare è¿›è¡Œå¤„ç†
optimizer = torch.optim.AdamW([
        {'params': model.image_model.parameters(), 'lr': 2e-5, 'weight_decay': 1e-4},
        {'params': model.text_model.parameters(), 'lr': 4e-5},
        {'params': [p for n, p in model.named_parameters() 
                    if 'image_model' not in n and 'text_model' not in n], 
        'lr': config.learning_rate, 'weight_decay': 1e-4}, 
    ])
total_steps = config.epochs * len(train_dataloader)
warmup_steps = int(0.15 * total_steps)

# Warmup è°ƒåº¦å™¨ï¼šä» 0.1*lr çº¿æ€§å¢åŠ åˆ° lr
warmup_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, 
                                                        start_factor=0.1, 
                                                        total_iters=warmup_steps
)

# ä½™å¼¦é€€ç«è°ƒåº¦å™¨ï¼šæ·»åŠ  eta_min é˜²æ­¢å­¦ä¹ ç‡è¿‡ä½
cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 
                                                                T_max=total_steps - warmup_steps, 
                                                                eta_min=1e-6
)

cosine_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 
                                                                T_max=total_steps - warmup_steps)
lr_scheduler = torch.optim.lr_scheduler.SequentialLR(
    optimizer,
    schedulers=[warmup_scheduler, cosine_scheduler],
    milestones=[warmup_steps] 
)
dataloader, model, optimizer, scheduler = accelerator.prepare(dataloader, model, optimizer, scheduler)

# Step-3 æ¨¡å‹è®­ç»ƒä»¥åŠæ¨¡å‹ä¼˜åŒ–
total_data = len(dataloader)
for i, batch in enumerate(dataloader):
    with accelerator.accumulate(model): # æ¢¯åº¦ç´¯è®¡
        inputs, targets = batch

        # ä¸‹é¢ä¸¤å¥å¯ä»¥ä¸ç”¨ï¼Œä½†æ˜¯ä¹ æƒ¯è¿˜æ˜¯ç›´æ¥ä½¿ç”¨
        inputs = inputs.to(accelerator.device)
        targets = targets.to(accelerator.device)

        outputs = model(inputs)
        loss = loss_function(outputs, targets)
        accelerator.backward(loss)
        if accelerator.sync_gradients: # è¿›è¡Œæ¢¯åº¦è£å‰ª
            accelerator.clip_grad_norm_(model.parameters(), 1.0)
        optimizer.step()
        scheduler.step()
        optimizer.zero_grad()

        # è®°å½•ä¸€ä¸‹å®éªŒç»“æœ
        logs = {
                "Train/loss": loss.item(),
                "Train/lr": optimizer.param_groups[0]['lr'], # è¿™é‡Œæ˜¯å‡è®¾æ¨¡å‹ä½¿ç”¨çš„ä¼˜åŒ–å­¦ä¹ ç‡ä¸åŒ æˆ–è€…ç›´æ¥ä½¿ç”¨ scheduler.get_last_lr()[0]
                "Train/ACC": acc,
            }
            progress_bar.set_postfix(
                loss=loss.item(),
                acc=acc, f1=f1)
            accelerator.log(logs, step= epoch* total_data+ i)

# Step-3 åŒæ­¥ä¸åŒè¿›ç¨‹
accelerator.wait_for_everyone()
if accelerator.is_main_process:
    model = accelerator.unwrap_model(model)
    model.save_pretrained(os.path.join(args.output_dir, "model"))
accelerator.end_training()
```

ä¸è¿‡å¯¹äºä¸Šé¢çš„ä»£ç éœ€è¦æ³¨æ„å¦‚ä¸‹å‡ ä¸ªå†…å®¹
1ã€è¿½è¸ªå™¨ä½¿ç”¨ï¼šä¸€èˆ¬å¤šæ˜¾å¡ä½¿ç”¨è¿‡ç¨‹ä¸­é€šè¿‡ä½¿ç”¨ `accelerator.end_training()` å»ç»“æŸ `tracker`
2ã€tqdmä½¿ç”¨ï¼šä¸€èˆ¬åªéœ€è¦ä¸»è¿›ç¨‹è¿›è¡Œæ˜¾ç¤ºè¿›åº¦æ¡ï¼Œå› æ­¤ä¸€èˆ¬ç›´æ¥ï¼š`tqdm(..., disable=not accelerator.is_local_main_process)`

## diffuser
> æ¨èç›´æ¥é˜…è¯»å®˜æ–¹æ–‡æ¡£ï¼š[https://huggingface.co/docs/diffusers/main/en/index](https://huggingface.co/docs/diffusers/main/en/index)
> [`pip install git+https://github.com/huggingface/diffusers`](https://huggingface.co/docs/diffusers/main/en/installation?install=Python)

### åŸºæœ¬ä½¿ç”¨
å¯¹äº[Diffusion ModelåŸç†](https://www.big-yellow-j.top/posts/2025/05/19/DiffusionModel.html)ç†è§£å¯ä»¥å‚è€ƒï¼Œä»¥åŠç›´æ¥é€šè¿‡ä¸‹é¢[è®­ç»ƒä¸€ä¸ªDiffusion Modelä»£ç ](https://github.com/shangxiaaabb/ProjectCode/blob/main/code/Python/DFModelTraining/df_training.py)ï¼ˆä»£ç ä¸ä¸€å®šå¾ˆè§„èŒƒï¼‰è¿›è¡Œè§£é‡Šã€‚

```python
from diffusers import DDPMScheduler

noise_scheduler = DDPMScheduler(num_train_timesteps= config.num_train_timesteps,
                            beta_start= config.beta_start, # ä¸¤ä¸ªbetaä»£è¡¨åŠ å™ªæƒé‡
                            beta_end= config.beta_end,
                            beta_schedule= 'scaled_linear')
...
# training
for epoch in range(config.epochs):
    for i, batch in enumerate(train_dataloader):
        image = batch["images"]
        ...
        timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, 
                                    (image.shape[0],), 
                                    device=image.device, 
                                    dtype=torch.int64)
            
        noise = torch.randn(image.shape, device= accelerator.device)
        noise_image = noise_scheduler.add_noise(image, noise, timesteps)
        ...
        noise_pred = model(noise_image, timesteps)
        loss = F.mse_loss(noise_pred, noise)
        ...
# eva
def evaluate(..., noise_scheduler, ):
    ...
    noise = torch.randn((config.eval_batch_size, config.channel, config.image_size, config.image_size)) # å¯ä»¥é€‰æ‹©å›ºå®šéšæœºæ•°ç§å­
    for t in noise_scheduler.timesteps:
        t_tensor = torch.full((noise.shape[0],), 
                                t, 
                                dtype=torch.long, 
                                device= device)
        predicted_noise = model(noise, t_tensor, text_label)
        noise = noise_scheduler.step(predicted_noise, t, noise).prev_sample
    images = (noise.clamp(-1, 1) + 1) / 2
    ...
```

è®­ç»ƒè¿‡ç¨‹
**1ã€åŠ å™ªå¤„ç†**ï¼šé€šè¿‡é€‰æ‹©ä½¿ç”¨DDPM/DDIMè€Œåå°†ç”Ÿæˆçš„"ç¡®å®šçš„å™ªå£°"æ·»åŠ åˆ°å›¾ç‰‡ä¸Š `noise_scheduler.add_noise(image, noise, timesteps)`
![image.png](https://s2.loli.net/2025/06/27/yLPrx7tkdOh3AiD.webp)

**2ã€æ¨¡å‹é¢„æµ‹**ï¼šé€šè¿‡æ¨¡å‹å»é¢„æµ‹æ‰€æ·»åŠ çš„å™ªå£°å¹¶ä¸”è®¡ç®—loss
ç”Ÿæˆè¿‡ç¨‹
**3ã€é€æ­¥è§£å™ª**ï¼šè®­ç»ƒå¥½çš„æ¨¡å‹é€æ­¥é¢„æµ‹å™ªå£°ä¹‹åå°†å…¶ä»å™ªå£°å›¾ç‰‡ä¸­å°†å™ªå£°å‰¥ç¦»å‡ºæ¥

### 1ã€Scheduler
> https://huggingface.co/docs/diffusers/api/schedulers/overview
> æ›´åŠ è¯¦ç»†çš„æè¿°ï¼š[https://www.big-yellow-j.top/posts/2025/07/06/DFscheduler.html](https://www.big-yellow-j.top/posts/2025/07/06/DFscheduler.html)

ä»¥[DDPMScheduler](https://github.com/huggingface/diffusers/blob/d7dd924ece56cddf261cd8b9dd901cbfa594c62c/src/diffusers/schedulers/scheduling_ddpm.py#L129)ä¸ºä¾‹ä¸»è¦ä½¿ç”¨ä¸¤ä¸ªåŠŸèƒ½ï¼š
**1ã€add_noise**ï¼ˆ[è¾“å…¥](https://github.com/huggingface/diffusers/blob/d7dd924ece56cddf261cd8b9dd901cbfa594c62c/src/diffusers/schedulers/scheduling_ddpm.py#L501)ï¼š`sampleã€noiseã€timesteps`ï¼‰ï¼šè¿™ä¸ªæ¯”è¾ƒç®€å•å°±æ˜¯ç›´æ¥ï¼š$x=\sqrt{\alpha}x+ \sqrt{1-\alpha}\epsilon$
**2ã€step**ï¼ˆ[è¾“å…¥](https://github.com/huggingface/diffusers/blob/d7dd924ece56cddf261cd8b9dd901cbfa594c62c/src/diffusers/schedulers/scheduling_ddpm.py#L398)ï¼š`model_outputã€timestepã€sample`ï¼‰ï¼šstepåšçš„å°±æ˜¯å°†ä¸Šé¢çš„add_noiseè¿›è¡Œé€†æ“ä½œã€‚å…·ä½“ä»£ç å¤„ç†
* [**Step-1**](https://github.com/huggingface/diffusers/blob/d7dd924ece56cddf261cd8b9dd901cbfa594c62c/src/diffusers/schedulers/scheduling_ddpm.py#L437) é¦–å…ˆè®¡ç®—å‡ ä¸ªå‚æ•°ï¼š$\alphaã€\beta$

```python
alpha_prod_t = self.alphas_cumprod[t]
alpha_prod_t_prev = self.alphas_cumprod[prev_t] if prev_t >= 0 else self.one
beta_prod_t = 1 - alpha_prod_t
beta_prod_t_prev = 1 - alpha_prod_t_prev
current_alpha_t = alpha_prod_t / alpha_prod_t_prev
current_beta_t = 1 - current_alpha_t
```

* [**Step-2**](https://github.com/huggingface/diffusers/blob/d7dd924ece56cddf261cd8b9dd901cbfa594c62c/src/diffusers/schedulers/scheduling_ddpm.py#L445) æ ¹æ®è®¡ç®—å¾—åˆ°å‚æ•°åæ¨$t-1$çš„è®¡ç®—ç»“æœï¼ˆæä¾›3ç§ç±»ï¼Œä»‹ç»â€œepsilonâ€ï¼‰$x_0=\frac{x_T- \sqrt{1- \alpha_t}\epsilon}{\alpha_t}$

```pred_original_sample = (sample - beta_prod_t ** (0.5) * model_output) / alpha_prod_t ** (0.5)```

* [**Step-3**](https://github.com/huggingface/diffusers/blob/d7dd924ece56cddf261cd8b9dd901cbfa594c62c/src/diffusers/schedulers/scheduling_ddpm.py#L469C9-L474C109)ï¼šä»æ•°å­¦å…¬å¼ä¸Šåœ¨ä¸Šä¸€æ­¥å°±å¯ä»¥è®¡ç®—å¾—åˆ°ï¼Œä½†æ˜¯åœ¨[è®ºæ–‡](https://arxiv.org/pdf/2006.11239)ä¸­ä¸ºäº†æ›´åŠ è¿‘ä¼¼é¢„æµ‹ç»“æœè¿˜ä¼šè®¡ç®—ï¼š

$$
\frac{\sqrt{\bar{\alpha}_{t-1}}\beta_{t}}{1-\bar{\alpha}_{t}}\mathbf{x}_{0}+\frac{\sqrt{\alpha_{t}}(1-\bar{\alpha}_{t-1})}{1-\bar{\alpha}_{t}}\mathbf{x}_{t}
$$

```python
pred_original_sample_coeff = (alpha_prod_t_prev ** (0.5) * current_beta_t) / beta_prod_t
current_sample_coeff = current_alpha_t ** (0.5) * beta_prod_t_prev / beta_prod_t
pred_prev_sample = pred_original_sample_coeff * pred_original_sample + current_sample_coeff * sample
```

åŒºåˆ«DDIMçš„å¤„ç†è¿‡ç¨‹å°†DDPMçš„é©¬å°”ç§‘å¤«é“¾æ›¿æ¢ä¸ºéé©¬å°”ç§‘å¤«é“¾è¿‡ç¨‹è€Œåè¿›è¡Œé‡‡æ ·ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥æ¯æ¬¡è¿­ä»£ä¸­è·¨å¤šä¸ªstepï¼Œä»è€Œå‡å°‘æ¨ç†è¿­ä»£æ¬¡æ•°å’Œæ—¶é—´ï¼š

$$
x_{t-1}=\sqrt{\alpha_{t-1}}\left(\frac{x_t-\sqrt{1-\alpha_t}\epsilon_\theta(x_t,t)}{\sqrt{\alpha_t}}\right)+\sqrt{1-\alpha_{t-1}-\sigma_t^2}\epsilon_\theta(x_t,t)+\sigma_tz
$$

```python
std_dev_t = eta * variance ** (0.5)
pred_sample_direction = (1 - alpha_prod_t_prev - std_dev_t**2) ** (0.5) * pred_epsilon
prev_sample = alpha_prod_t_prev ** (0.5) * pred_original_sample + pred_sample_direction
 
prev_sample = prev_sample + variance
```

### 2ã€pipeline
> æ‰€æœ‰æ”¯æŒçš„pipelineï¼š[Diffusers Pipelines](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/README.md)

ä¸€èˆ¬æ¥è¯´å¾ˆå¤šè®ºæ–‡é‡Œé¢æå‡ºçš„æ¨¡å‹ï¼ŒåŸºæœ¬éƒ½æ˜¯åŸºäºSDï¼ˆStableDiffusionï¼‰ç­‰æ¨¡å‹è¿›è¡Œâ€œå¾®è°ƒâ€çš„ï¼Œå› æ­¤å¾ˆå¤šæ”¹è¿›ä¹Ÿéƒ½æ˜¯å»äº‰å¯¹è¾“å…¥æ¨¡å‹çš„å‚æ•°è¿›è¡Œè°ƒæ•´ï¼ˆæ¢è¨€ä¹‹å°±æ˜¯æ­ç§¯æœ¨è®²æ•…äº‹ï¼‰ï¼Œæ¯”å¦‚è¯´æ”¹å˜è¾“å…¥å›¾ç‰‡å†…å®¹ã€æ”¹å˜SDä¸­æ¡ä»¶ç­‰ã€‚é™¤æ­¤ä¹‹å¤–åˆ†æä¸€ä¸ª`pipeline`ç›´æ¥é€šè¿‡åˆ†æé‡Œé¢çš„`__call__`å³å¯ï¼ŒåŸºæœ¬ä½¿ç”¨ï¼š
```python
from diffusers import StableDiffusionPipeline
import torch

model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)

if torch.cuda.is_available():
    pipe = pipe.to("cuda")

prompt = "A futuristic city at sunset, cyberpunk style, highly detailed, cinematic lighting"
image = pipe(prompt, num_inference_steps=50, guidance_scale=7.5).images[0]
image.save("output.png")
```

#### 2.1 StableDiffusionPipeline
> https://huggingface.co/docs/diffusers/v0.34.0/en/api/pipelines/overview#diffusers.DiffusionPipeline

å¾ˆå¤šè®ºæ–‡é‡Œé¢åŸºæœ¬éƒ½æ˜¯ç›´æ¥å»å¾®è°ƒè®­ç»ƒå¥½çš„æ¨¡å‹æ¯”å¦‚è¯´StableDiffusionç­‰ï¼Œä½¿ç”¨åˆ«äººè®­ç»ƒåçš„å°±å°‘ä¸äº†çœ‹åˆ° `pipeline`çš„å½±å­ï¼Œç›´æ¥ä»‹ç»[`StableDiffusionPipeline`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py)çš„æ„å»ºï¼ˆ**æ–‡ç”Ÿå›¾pipeline**ï¼‰ã€‚åœ¨[ä»£ç ](https://github.com/huggingface/diffusers/blob/v0.34.0/src/diffusers/pipelines/pipeline_utils.py#L180)ä¸­ä¸»è¦ä½¿ç”¨åˆ°çš„åŸºç¡€æ¨¡å‹å¦‚ä¸‹å‡ ä¸ªï¼š1ã€VAEï¼ˆAutoencoderKLï¼‰ï¼›2ã€CLIPï¼ˆç”¨äºæ–‡æœ¬ç¼–ç ï¼ŒCLIPTextModelã€CLIPTokenizerï¼‰ï¼›3ã€Unetï¼ˆæ¨¡å‹éª¨æ¶ï¼ŒUNet2DConditionModelï¼‰
**Step-1**ï¼šå¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç ï¼ˆæ–‡ç”Ÿå›¾ç›´æ¥è¾“å…¥æ–‡æœ¬ï¼‰é€šè¿‡æ­£ã€è´Ÿç¼–ç å¯¹ç”Ÿæˆå›¾åƒè¿›è¡ŒæŒ‡å¯¼ï¼š
```python
def encode_prompt(..., prompt, do_classifier_free_guidance,...,):
    # 1ã€åˆ¤æ–­æ–‡æœ¬ç¼–ç å™¨æ˜¯å¦loraå¾®è°ƒ
    if lora_scale is not None and isinstance(self, StableDiffusionLoraLoaderMixin):
        self._lora_scale = lora_scale
        if not USE_PEFT_BACKEND:
            adjust_lora_scale_text_encoder(self.text_encoder, lora_scale)
        else:
            scale_lora_layers(self.text_encoder, lora_scale)
    # 2ã€é€šè¿‡promptæ¥ç¡®å®šéœ€è¦ç”Ÿæˆå¤šå°‘å›¾ç‰‡
    ...
    # 3ã€å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç 
    if prompt_embeds is None:
        ...
        text_inputs = self.tokenizer(...)
        text_input_ids = text_inputs.input_ids
            untruncated_ids = self.tokenizer(prompt, padding="longest", return_tensors="pt").input_ids
        ...
        # ä¼šæ˜¾ç¤ºä¸€ä¸ªè¿‡é•¿æˆªæ–­è­¦å‘Š
        ...
        # é€‰æ‹©clipä¸­å€’æ•°ç¬¬å‡ å±‚ä½œä¸ºæ–‡æœ¬ç¼–ç è¾“å‡º
        if clip_skip is None:
            # é»˜è®¤ç›´æ¥æœ€åä¸€å±‚
            prompt_embeds = self.text_encoder(text_input_ids.to(device), attention_mask=attention_mask)
                prompt_embeds = prompt_embeds[0]
        else:
            # å€’æ•°å±‚
            prompt_embeds = self.text_encoder(
                text_input_ids.to(device), attention_mask=attention_mask, output_hidden_states=True
            )
            prompt_embeds = prompt_embeds[-1][-(clip_skip + 1)]
            prompt_embeds = self.text_encoder.text_model.final_layer_norm(prompt_embeds)
        # æ”¹å˜å½¢çŠ¶å¾—åˆ° batch_sizeï¼ˆå¯¹åº”promptæ•°é‡ï¼‰, 77, 748 CLIP: CLIP-ViT-L
        ...
        prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1) 
    if do_classifier_free_guidance and negative_prompt_embeds is None:
        # æ­¤éƒ¨åˆ†å’Œä¸Šé¢æ­£å¸¸çš„ç¼–ç å¤„ç†æ–¹å¼ç›¸ä¼¼ç›´æ¥å¯¹negative_promptè¿›è¡Œç¼–ç 
        ...
    ...
    return prompt_embeds, negative_prompt_embeds
```

**Step-2**ï¼šè·å–æ¨ç†æ—¶é—´æ­¥ä»¥åŠç”Ÿæˆlatentå˜é‡
**Step-3**ï¼šæ¨¡å‹å¤„ç†
```python
# é¦–å…ˆé€šè¿‡uneté€æ­¥è¿›è¡Œè§£ç å›¾åƒ
with self.progress_bar(total=num_inference_steps) as progress_bar:
    for i, t in enumerate(timesteps):
        ...
        noise_pred = self.unet(...)[0]
        ...
        # é€šè¿‡stepæ¥ä»tåæ¨t-1
        latents = self.scheduler.step(noise_pred, t, latents, **extra_step_kwargs, return_dict=False)[0]
        ...
        # classifier_free_guidance
if not output_type == "latent":
    # å›¾ç‰‡è¿”å›
    image = self.vae.decode(latents / self.vae.config.scaling_factor, return_dict=False,generator=generator)[0]
    image, has_nsfw_concept = self.run_safety_checker(image, device, prompt_embeds.dtype)
else:
    # ç›´æ¥è¿”å›æ²¡è¢«vaeå¤„ç†çš„ç»“æœ
    image = latents
    has_nsfw_concept = None

if has_nsfw_concept is None:
    do_denormalize = [True] * image.shape[0]
else:
    do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]
image = self.image_processor.postprocess(image, output_type=output_type, do_denormalize=do_denormalize)
...
if not return_dict:
    return (image, has_nsfw_concept)

return StableDiffusionPipelineOutput(images=image, nsfw_content_detected=has_nsfw_concept)
```

> **è¡¥å……-1**ï¼š`classifier_free_guidance`ï¼ˆCFGï¼‰ ä»¥åŠ `classifier_guidance`ï¼ˆCGï¼‰
> `classifier_guidance`[^1]ï¼šé€šè¿‡ä¸€ä¸ªåˆ†ç±»å™¨æ¥å¼•å¯¼æ¨¡å‹ç”Ÿæˆçš„æ–¹å‘ï¼Œä¹Ÿå°±æ˜¯ä½¿å¾—æ¨¡å‹æŒ‰ç±»è¿›è¡Œç”Ÿæˆã€‚æ•°å­¦ä¸Šæè¿°ä¸º[^2]ï¼š$\nabla p(x_t\vert y)=\nabla \log p(x_t)+ \nabla \log p(y \vert x_t)$ ä¹Ÿå°±æ˜¯è¯´å‰é¢éƒ¨åˆ†ä»£è¡¨unconditional scoreåé¢éƒ¨åˆ†ä»£è¡¨åˆ†ç±»å™¨çš„æ¢¯åº¦ï¼Œä¹Ÿå°±æ˜¯æ·»åŠ ä¸€ä¸ªåˆ†ç±»å™¨æ¢¯åº¦æ¥â€œæŒ‡å¯¼â€æ¨¡å‹ç”Ÿæˆæ–¹å‘ã€‚
> `classifier_free_guidance`[^3]ï¼šå¯¹ä¸Šé¢çš„æ”¹è¿›ç‰ˆæœ¬ï¼Œä¸Šé¢è¿‡ç¨‹ä¸­ä¼šé¢å¤–è®­ç»ƒä¸€ä¸ªåˆ†ç±»å™¨è¿›è€Œå¢åŠ è®­ç»ƒæˆæœ¬ã€‚å› æ­¤å¯¹äºä¸Šé¢è®¡ç®—å…¬å¼ä¸­ï¼š$\nabla \log p(y \vert x_t)= \nabla p(x_t\vert y)- \nabla \log p(x_t)= -\frac{1}{\sqrt{1- \alpha_t}}(\epsilon_\theta(x_t, t, y)- \epsilon_\theta(x_t, t))$ æœ€åå¾—åˆ°æ¢¯åº¦è¿‡ç¨‹ä¸ºï¼š $(w+1)\epsilon_\theta(x_t, t, y)- w\epsilon_\theta(x_t, t)$


å›åˆ°ä»£ç ä¸­ï¼Œä»£ç ä¸­å…·ä½“æ“ä½œè¿‡ç¨‹ä¸ºï¼š**1ã€æ–‡æœ¬ç¼–ç è¿‡ç¨‹ä¸­**ï¼Œè¿™éƒ¨åˆ†æ¯”è¾ƒç®€å•ç›´æ¥æ ¹æ®å¯¹negative_promptè¿›è¡ŒCLIP text encoderå¤„ç†å³å¯ï¼ˆå¦‚æœæ²¡æœ‰è¾“å…¥negative_prompté»˜è®¤å°±æ˜¯ç›´æ¥ç”¨ç©ºå­—ç¬¦è¿›è¡Œæ›¿ä»£ï¼‰å¦‚æœè¿›è¡ŒCFGé‚£ä¹ˆç›´æ¥å°†ä¸¤éƒ¨åˆ†è¿›è¡Œæ‹¼æ¥ï¼ˆ`torch.cat([negative_prompt_embeds, prompt_embeds])`ï¼‰ `prompt_embeds`ï¼›**2ã€æ¨¡å‹è§£ç è¿‡ç¨‹ä¸­**ï¼Œè¿™éƒ¨åˆ†å¤„ç†è¿‡ç¨‹æ¯”è¾ƒç²—æš´ï¼Œå¦‚æœè¦è¿›è¡ŒCFGé‚£ä¹ˆç›´æ¥å°†latentæ‰©å±•ä¸ºä¸¤ä»½ï¼ˆUncond+Condå„ä¸€ä»½ï¼‰å¯¹åº”çš„textè¾“å‡ºä¹Ÿæ˜¯ä¸¤ä»½ï¼Œé€šè¿‡ä¸€ä¸ªæ¨¡å‹å¤„ç†ä¹‹åå†é€šè¿‡`chunk`åˆ†å‡ºæ— æ¡ä»¶è¾“å‡ºã€æœ‰æ¡ä»¶è¾“å‡ºï¼Œæœ€åè®¡ç®—ä¸¤éƒ¨åˆ†ç»„åˆï¼š$\epsilon(x,t)+ w(\epsilon(x,t,y)- \epsilon(x,t))$

```python
if self.do_classifier_free_guidance:
    prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds])
...
latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents
...
noise_pred = self.unet(latent_model_input, t, prompt_embeds, ...)[0]

if self.do_classifier_free_guidance:
    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)

if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:
    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)
```

#### 2.2 StableDiffusionXLInpaintPipeline
> [https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl_inpaint.py)

å¯¹äºå›¾åƒæ¶ˆé™¤ä»»åŠ¡è€Œè¨€ä½¿ç”¨è¾ƒå¤šçš„ä¹Ÿæ˜¯æ­¤ç±»pipelineï¼ˆSDXLå¼€æºå¯ä»¥å•†ç”¨ï¼‰å…·ä½“ä½¿ç”¨ä»£ç å¦‚ä¸‹ï¼š
```python
from diffusers import StableDiffusionXLInpaintPipeline
from diffusers.utils import load_image, make_image_grid
import torch
from PIL import Image

device = "cuda" if torch.cuda.is_available() else "cpu"

# åŠ è½½åŸºç¡€æ¨¡å‹
base = StableDiffusionXLInpaintPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-base-1.0",
    torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦æµ®ç‚¹æ•°ä»¥å‡å°‘æ˜¾å­˜å ç”¨
    variant="fp16",             # ä½¿ç”¨ fp16 å˜ä½“ä»¥ä¼˜åŒ–æ€§èƒ½
    use_safetensors=True        # ä½¿ç”¨ safetensors æ ¼å¼ä»¥æé«˜åŠ è½½é€Ÿåº¦
).to(device)

# åŠ è½½ä¼˜åŒ–æ¨¡å‹ï¼ˆrefiner modelï¼‰
refiner = StableDiffusionXLInpaintPipeline.from_pretrained(
    "stabilityai/stable-diffusion-xl-refiner-1.0",
    text_encoder_2=base.text_encoder_2,  # å…±äº«åŸºç¡€æ¨¡å‹çš„ç¬¬äºŒä¸ªæ–‡æœ¬ç¼–ç å™¨
    vae=base.vae,                        # å…±äº«åŸºç¡€æ¨¡å‹çš„å˜åˆ†è‡ªç¼–ç å™¨
    torch_dtype=torch.float16,
    use_safetensors=True,
    variant="fp16",
).to(device)

img_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo.png"
mask_url = "https://raw.githubusercontent.com/CompVis/latent-diffusion/main/data/inpainting_examples/overture-creations-5sI6fQgYIuo_mask.png"
init_image = load_image(img_url)
mask_image = load_image(mask_url)


prompt = "A majestic tiger sitting on a bench" 
negative_prompt = "distorted, blurry, low quality" 

num_inference_steps = 75 
high_noise_frac = 0.7

# ä½¿ç”¨åŸºç¡€æ¨¡å‹è¿›è¡Œåˆæ­¥å»å™ªï¼ˆè¾“å‡ºæ½œåœ¨è¡¨ç¤ºï¼‰
base_output = base(
    prompt=prompt,
    negative_prompt=negative_prompt,
    image=init_image,
    mask_image=mask_image,
    num_inference_steps=num_inference_steps,
    denoising_end=high_noise_frac,  # åŸºç¡€æ¨¡å‹å¤„ç†é«˜å™ªå£°é˜¶æ®µ
    output_type="latent"           # è¾“å‡ºæ½œåœ¨è¡¨ç¤ºä»¥ä¾›ä¼˜åŒ–æ¨¡å‹ä½¿ç”¨
).images

# ä½¿ç”¨ä¼˜åŒ–æ¨¡å‹è¿›è¡Œç»†èŠ‚å¢å¼º
refined_image = refiner(
    prompt=prompt,
    negative_prompt=negative_prompt,
    image=base_output,
    mask_image=mask_image,
    num_inference_steps=num_inference_steps,
    denoising_start=high_noise_frac,  # ä¼˜åŒ–æ¨¡å‹å¤„ç†ä½å™ªå£°é˜¶æ®µ
).images[0]

# å¯è§†åŒ–ç»“æœ
grid = make_image_grid([init_image, mask_image, refined_image.resize((512, 512))], rows=1, cols=3)
grid.save("inpainting_result.png")
refined_image.save("refined_image.png")
```

é¦–å…ˆæ¨¡å‹è¾“å…¥ä¸»è¦ä¸ºå¦‚ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š1ã€æ–‡æœ¬è¾“å…¥ï¼›2ã€å›¾ç‰‡è¾“å…¥ï¼ˆæ­£å¸¸å›¾ç‰‡ä»¥åŠmaskå›¾ç‰‡ï¼‰ã€‚**é¦–å…ˆå¯¹äºæ–‡æœ¬ç¼–ç **ã€‚å¯¹äºSDXLæ¨¡å‹è€Œè¨€æ–‡æœ¬ä¼šé€šè¿‡ä¸¤ä¸ªclipçš„æ–‡æœ¬ç¼–ç å™¨è¿›è¡Œç¼–ç ï¼ˆ**OpenCLIP-ViT/G**ï¼š1280ã€**CLIP-ViT/L**ï¼š768ï¼‰å¯¹äºä¸¤ä¸ªç¼–ç å™¨ä»£ç å¤„ç†æ€è·¯ä¸ºï¼š
```python
...
tokenizers = [self.tokenizer, self.tokenizer_2] if self.tokenizer is not None else [self.tokenizer_2]
text_encoders = (
    [self.text_encoder, self.text_encoder_2] if self.text_encoder is not None else [self.text_encoder_2]
)

if prompt_embeds is None:
    prompt_2 = prompt_2 or prompt
    prompt_2 = [prompt_2] if isinstance(prompt_2, str) else prompt_2

    prompt_embeds_list = []
    prompts = [prompt, prompt_2]
    for prompt, tokenizer, text_encoder in zip(prompts, tokenizers, text_encoders):
        ...
        text_inputs = tokenizer(prompt,...)
        text_input_ids = text_inputs.input_ids
        untruncated_ids = tokenizer(prompt, ...).input_ids
        ...
        prompt_embeds = text_encoder(text_input_ids.to(device), output_hidden_states=True)
        ...
        prompt_embeds_list.append(prompt_embeds)

    prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)
...
if self.do_classifier_free_guidance:
    prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)
    ...
  prompt_embeds = prompt_embeds.to(device)
```
æœ€åå¾—åˆ°çš„`prompt_embeds`ä¸ºï¼š`[1, 77, 2048]`ï¼ˆç”±`[1, 77, 768]` å’Œ `[1, 77, 1280]`ï¼‰æ‹¼æ¥å¾—åˆ°ï¼Œå¦‚æœè¦ä½¿ç”¨CFGçš„è¯å°±éœ€è¦è¾“å…¥`negative_prompt`ä»¥åŠå‚æ•°`guidance_scale`ï¼Œå¯¹äº`negative_prompt`çš„å¤„ç†æ–¹å¼å’Œä¸Šé¢ç›¸åŒã€‚é™¤æ­¤ä¹‹å¤–å†ä»£ç ä¸­ä¼šæœ‰`added_cond_kwargs = {"text_embeds": add_text_embeds, "time_ids": add_time_ids}`è¿™ä¸ªå‚æ•°ï¼Œä¸€èˆ¬ä½œç”¨æ˜¯ï¼šä½œä¸ºä¸€ä¸ªâ€œé¢å¤–â€çš„æ¡ä»¶æ·»åŠ åˆ°æ—¶é—´ç¼–ç ä¸­ï¼ˆ`emb = emb + aug_emb if aug_emb is not None else emb`ï¼‰ã€‚ä¸è¿‡å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¾ˆå¤šè®ºæ–‡é‡Œé¢éƒ½ä½¿ç”¨ï¼š**å°†å›¾åƒå’Œæ–‡æœ¬ç¼–ç ç»„åˆä½œä¸ºâ€œæ–‡æœ¬ç¼–ç â€è¾“å…¥**ï¼ˆ[objectclear](https://www.big-yellow-j.top/posts/2025/07/25/ImageEraser2.html#:~:text=4s%E5%88%B00.5s%EF%BC%89%E3%80%82-,ObjectClear,-https%3A//arxiv.org)ï¼‰å¦‚æœè¦å®ç°è¿™ä¸ªï¼ˆobjectclearï¼‰åŠŸèƒ½ä¼ªä»£ç å¦‚ä¸‹ï¼š
```python
...
    masked_image = init_image
    # masked_image = init_image * (mask < 0.5)
    obj_only = init_image * (mask > 0.5)
    obj_only = obj_only.to(device=device)
    object_embeds = self.image_prompt_encoder(obj_only)
prompt_embeds = self.postfuse_module(prompt_embeds, object_embeds, 5)
```
å…¶ä¸­`prompt_embeds`å°±æ˜¯æ­£å¸¸çš„æ–‡æœ¬ç¼–ç ï¼Œ`self.image_prompt_encoder`ä¸€èˆ¬å°±æ˜¯ä½¿ç”¨clip imageçš„æ–‡æœ¬ç¼–ç å™¨è¿™æ ·ä¸€æ¥å°±ä¼šå°†æ–‡æœ¬å’Œå›¾ç‰‡ç¼–ç æˆå‘é‡ï¼Œ`self.postfuse_module`ä¸€èˆ¬å°±æ˜¯å°†ä¸¤ä¸ªå‘é‡è¿›è¡Œèåˆï¼ˆè¿™ä¸ªä¸€èˆ¬å°±æ˜¯é€šè¿‡mlpå¯¹é½ç»´åº¦ä¹‹åç›´æ¥æ‹¼æ¥å³å¯ï¼‰
**è€Œåå†å›¾åƒç¼–ç **ã€‚è¿™éƒ¨åˆ†æ¯”è¾ƒå®¹æ˜“ç›´æ¥é€šè¿‡vaeå»ç¼–ç å³å¯
```python
...
masked_image = init_image * (mask < 0.5)
...
mask = torch.cat([mask] * 2) if do_classifier_free_guidance else mask
masked_image_latents = self._encode_vae_image(masked_image, generator=generator)
```
å¯¹äºå›¾ç‰‡ä¸€èˆ¬åšæ³•æ˜¯ç›´æ¥`masked_image = init_image * (mask < 0.5)`ä½†æ˜¯è®ºæ–‡é‡Œé¢æœ‰äº›ç›´æ¥ä½¿ç”¨`masked_image = init_image`ã€‚åœ¨æ–‡æœ¬ä»¥åŠå›¾åƒéƒ½ç¼–ç ä¹‹åå°±æ˜¯æ¨¡å‹å¤„ç†ï¼Œåªä¸è¿‡å¦‚æœä½¿ç”¨CFGï¼š
```python
if self.do_classifier_free_guidance:
    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)
    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)

if self.do_classifier_free_guidance and self.guidance_rescale > 0.0:
    # Based on 3.4. in https://arxiv.org/pdf/2305.08891.pdf
    noise_pred = rescale_noise_cfg(noise_pred, noise_pred_text, guidance_rescale=self.guidance_rescale)
```

> **è¡¥å……ä¸€ç‚¹**ï¼šå¦‚æœè¦åšCFGä¸€èˆ¬ä¼šå°†æ–‡æœ¬çš„promptï¼šnegative_prompt_embedsï¼ˆå¦‚æœæ²¡æœ‰è¾“å…¥negative_promptä¼šç›´æ¥ç”¨0ä»£æ›¿ï¼‰, prompt_embedsç›´æ¥æ‹¼æ¥èµ·æ¥ï¼Œè€Œåå…¶ä»–çš„å°±ç›´æ¥â€œæ‹¼æ¥æœ¬ä½“â€


### 3ã€Loraå¾®è°ƒ
å’Œå¤§è¯­è¨€æ¨¡å‹çš„å¤„ç†æ–¹å¼ç›¸ä¼¼ï¼Œé€šè¿‡`peft`å»å¾®è°ƒæ¨¡å‹ï¼Œç®€å•äº†è§£ä¸€ä¸‹`peft`é‡Œé¢å¾®è°ƒçš„å¤„ç†æ€è·¯ï¼ˆå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œä½¿ç”¨`peft`æ¥å¾®è°ƒåªé€‚ç”¨äºåŸºäº`transformer`åº“æ¥æ­å»ºçš„æ¨¡å‹å¯¹äºè‡ªå·±çš„æ¨¡å‹å¯èƒ½æ²¡é‚£ä¹ˆå¥½çš„é€‚åº”æ€§ï¼‰ï¼š
```python
unet = UNet2DConditionModel.from_pretrained(
        "stable-diffusion-v1-5/stable-diffusion-inpainting", 
        subfolder="unet",
        cache_dir= '/data/huangjie',
    )
unet.requires_grad_(False)
print(unet.down_blocks[0])

unet_lora_config = LoraConfig(
        r=2,
        lora_alpha=2,
        lora_dropout=0.2,
        init_lora_weights="gaussian",
        target_modules=["to_k", "to_q", "to_v", "to_out.0", "add_k_proj", "add_v_proj"],
    )
unet.add_adapter(unet_lora_config)
print("after Lora Model:", unet.down_blocks[0])
```

ä¸Šé¢ä¸¤ä¸ªè¿‡ç¨‹æ¨¡å‹å˜åŒ–ä¸ºï¼š
![image.png](https://s2.loli.net/2025/07/02/7KOzpIxEN3bdZQ9.webp)

ä»”ç»†åˆ†æä¸€ä¸‹`LoraConfig`é‡Œé¢çš„å…·ä½“åŸç†ï¼Œå› ä¸ºå¾ˆå¤šæ¨¡å‹ï¼ˆåŸºäºattentionï¼‰åŸºæœ¬å°±æ˜¯qã€kã€vä¸‰ä¸ªï¼Œå› æ­¤é€šè¿‡`target_modules`æŒ‡å®šå“ªäº›æ¨¡å—çš„å‚æ•°æ˜¯éœ€è¦é€šè¿‡loraè¿›è¡Œè°ƒæ•´çš„æ¨¡å—ã€‚`init_lora_weights`ä»£è¡¨loraåˆå§‹åŒ–å‚æ•°åˆ†å¸ƒç­–ç•¥ï¼Œå‚æ•°`r`ä»¥åŠ `lora_alpha`ä»£è¡¨çš„å«ä¹‰æ˜¯ï¼š
$$
y=Wx+ \text{Dropout}(\text{B}_{out \times r} \text{A}_{r \times in}x)  \times \frac{\text{lora\_alpha}}{r}
$$

**ç»å…¸é—®é¢˜**ï¼š1ã€loraé‡Œé¢å‚æ•°é‡Œé¢ä¹‹æ‰€ä»¥åˆå§‹åŒ–ä¸º0æ˜¯å› ä¸ºå¯¹äºæˆ‘ä»¬çš„llm/DFæ¨¡å‹ä¸€èˆ¬éƒ½æ˜¯â€œä¼˜ç§€â€çš„ï¼Œè€Œå¯¹äºâ€œé™Œç”Ÿâ€çš„æ•°æ®é€šè¿‡é›¶åˆå§‹åŒ–ç¡®ä¿ä¸€åˆ‡å¹²å‡€ï¼Œä» 0 å¼€å§‹ç¨³æ­¥é€‚é…ï¼ˆåœ¨è®­ç»ƒåˆæœŸå¼•å…¥å™ªå£°ï¼Œå¯èƒ½å¯¼è‡´ä¸ç¨³å®šï¼Œå°¤å…¶åœ¨å¾®è°ƒå°‘æ­¥æ•°ã€ä½å­¦ä¹ ç‡æ—¶ï¼Œæ”¶æ•›æ›´æ…¢ï¼‰2ã€å¤šä¸ªloraæ¨¡å‹åŒæ—¶ä½œç”¨äºä¸€ä¸ªSDæ¨¡å‹ï¼Œå¹¶é…ç½®ä»–ä»¬çš„å„è‡ªæƒé‡ï¼Œå¹¶ä¸”ä¸åŒloraå‚æ•°å¯¹æ¨¡å‹ç”Ÿæˆçš„å½±å“[^4]:
![image.png](https://s2.loli.net/2025/07/02/oi1umR5jek4LIWp.webp)

### 4ã€Adaptersä½¿ç”¨
loraä¹Ÿæ˜¯Adaptersï¼ˆå¯ä»¥ç®€å•ç†è§£ä¸ºå¯¹è®­ç»ƒå¥½çš„æ¨¡å‹å†å»æ·»åŠ ä¸€ä¸ªæ’ä»¶ï¼Œé€šè¿‡è¿™ä¸ªæ’ä»¶è®©SDå»ç”Ÿæˆå…¶ä»–çš„æ ·å¼çš„å›¾ç‰‡ï¼‰ä¸€ç§ï¼Œå…·ä½“è§ï¼š[æ·±å…¥æµ…å‡ºäº†è§£ç”Ÿæˆæ¨¡å‹-6ï¼šå¸¸ç”¨åŸºç¡€æ¨¡å‹ä¸ Adaptersç­‰è§£æ](https://www.big-yellow-j.top/posts/2025/07/06/DFBaseModel.html)

### 5ã€è‡ªæ³¨æ„åŠ›æŠ€æœ¯ï¼ˆAttnProcessorã€AttnProcessor2_0ï¼‰
> https://huggingface.co/docs/diffusers/v0.30.1/en/api/attnprocessor

* 1ã€AttnProcessor

æ­¤éƒ¨åˆ†å°±æ˜¯éå¸¸å¸¸è§„çš„æ³¨æ„åŠ›è®¡ç®—æ–¹å¼

* 2ã€AttnProcessor2_0

å®ƒè°ƒç”¨äº† PyTorch 2.0 èµ·å¯ç”¨çš„ç®—å­` F.scaled_dot_product_attention` ä»£æ›¿æ‰‹åŠ¨å®ç°çš„æ³¨æ„åŠ›è®¡ç®—ã€‚è¿™ä¸ªç®—å­æ›´åŠ é«˜æ•ˆï¼Œå¦‚æœä½ ç¡®å®š PyTorch ç‰ˆæœ¬è‡³å°‘ä¸º 2.0ï¼Œå°±å¯ä»¥ç”¨ AttnProcessor2_0 ä»£æ›¿
å‚è€ƒçŸ¥ä¹[^5]ä¸­çš„æè¿°ï¼Œå¦‚ä½•å°†è‡ªæ³¨æ„åŠ›è¿›è¡Œä¿®æ”¹ï¼Œæ¯”å¦‚è¯´å¦‚ä¸‹ä»£ç ï¼š
```python
from diffusers.models.attention_processor import (Attention,AttnProcessor,AttnProcessor2_0)
unet = UNet2DConditionModel()
for name, module in unet.named_modules():
   if isinstance(module, Attention) and "attn2" in name:
      print(f'name: {name}')
      print("*"*20)
      break
```
é‚£ä¹ˆå°±ä¼šå¾—åˆ°ä¸€ä¸ªæ¯”å¦‚è¯´ï¼š`down_blocks.0.attentions.0.transformer_blocks.0.attn2`æ¯”å¦‚è¯´å¦‚æœæˆ‘éœ€è¦å°†è¿™ä¸ªæ›¿æ¢é‚£ä¹ˆå¤„ç†æ–¹å¼ä¸ºï¼š
```python3
for name, module in unet.named_modules():
   if isinstance(module, Attention) and "attn2" in name:
      print(f'raw name: {name} \n raw module: {module.processor}')
      print("*"*20)
      if isinstance(module.processor, AttnProcessor2_0):
         module.set_processor(AttnProcessor())
      print(f"change name: {name} \n change module: {module.processor}")
      print("*"*20)
      break
```
è¿™æ ·ä¸€æ¥æœ‰æœ€å¼€å§‹çš„ï¼š`<diffusers.models.attention_processor.AttnProcessor2_0 object at 0x7ff392734eb0>` æ›¿æ¢ä¸º`<diffusers.models.attention_processor.AttnProcessor object at 0x7ff5b776bc40>`ã€‚æˆ–è€…ç›´æ¥æ”¹æˆè‡ªå®šä¹‰çš„å¤„ç†æ–¹å¼ï¼š
```python3
class CustonAttnProcessor(AttnProcessor):
    def __call__(self, attn, hidden_states, encoder_hidden_states=None, attention_mask=None):
        query = attn.to_q(hidden_states)
        encoder_states = hidden_states if encoder_hidden_states is None else encoder_hidden_states
        key = attn.to_k(encoder_states)
        value = attn.to_v(encoder_states)

        attn_scores = torch.baddbmm(
            torch.empty(query.shape[0], query.shape[1], key.shape[1], device=query.device),
            query,
            key.transpose(-1, -2),
            beta=0,
            alpha=attn.scale,
        )

        # æ¯”å¦‚è¯´å¯¹ attn_scores å–log
        attn_probs = torch.log(attn_scores) 
        attn_probs = attn_scores.softmax(dim=-1)

        hidden_states = torch.bmm(attn_probs, value)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        return hidden_states

attn_processor_dict = {}
for k in unet.attn_processors.keys():
    if "attn2" in k:
        attn_processor_dict[k] = CustonAttnProcessor()
    else:
        attn_processor_dict[k] = unet.attn_processors[k]
unet.set_attn_processor(attn_processor_dict)
for name, processor in unet.attn_processors.items():
   print(name, "=>", type(processor))
```
**æ€»çš„æ¥è¯´**å¦‚æœè¦å»ä¿®æ”¹æ³¨æ„åŠ›å¤„ç†æ–¹å¼ï¼Œç›´æ¥å»ä¾¿åˆ©`unet.attn_processors.keys()`ç„¶åå»æ‰¾åˆ°éœ€è¦ä¿®æ”¹çš„å±‚å°†å…¶æ›¿æ¢å³å¯ï¼Œåªä¸è¿‡å…³é”®åœ¨äº`CustonAttnProcessor`çš„å®šä¹‰æ–¹å¼ã€‚

## æ•°æ®åˆæˆ
[æ•°æ®åˆæˆ/æ ‡ç­¾ç®—æ³•æ±‡æ€»](https://github.com/shangxiaaabb/ProjectCode/tree/main/code/Python/DFDataBuild)

## ä»£ç Demo
[å¾®è°ƒDFæ¨¡å‹ä»£ç Demo](https://www.big-yellow-j.top/posts/2025/07/06/DFBaseModel.html#:~:text=%E9%87%8D%E4%B8%8B%E8%BD%BD%EF%BC%89%EF%BC%9A-,%E7%AE%80%E6%98%93Demo%E4%BB%A3%E7%A0%81,-%E9%80%9A%E8%BF%87%E6%80%BB%E7%BB%93%E4%B8%8A%E9%9D%A2)


## å‚è€ƒ
[^1]: https://arxiv.org/abs/2105.05233
[^2]: https://zhuanlan.zhihu.com/p/640631667
[^3]: https://openaccess.thecvf.com/content/WACV2023/papers/Liu_More_Control_for_Free_Image_Synthesis_With_Semantic_Diffusion_Guidance_WACV_2023_paper.pdf
[^4]: https://github.com/cloneofsimo/lora/discussions/37
[^5]: https://zhuanlan.zhihu.com/p/680035048