---
layout: mypost
title: æ·±åº¦å­¦ä¹ å¸¸ç”¨çš„Attentionæ“ä½œï¼ˆMHA/Casual Attentionï¼‰ä»¥åŠå†…å­˜ä¼˜åŒ–ç®¡ç†(Flash Attention/Page Attention)
categories: æ·±åº¦å­¦ä¹ åŸºç¡€ç†è®º
extMath: true
images: true
address: changsha
show_footer_image: true
description: ä¸»è¦ä»‹ç»å„ç±»Attention(Flash Attention/MLA/Page Attention)
---

## Attentionæ“ä½œä»¥åŠå†…å­˜ä¼˜åŒ–ç®¡ç†

### ä¸€ã€Attentionæ“ä½œ

#### 1ã€`Multi Head Attention`

å…³äº **Multi Head Attention**ç½‘ä¸Šæœ‰è¾ƒå¤šçš„è§£é‡Šäº†ï¼Œè¿™é‡Œä¸»è¦è®°å½•å¦‚ä¸‹å‡ ç‚¹

1ã€å¯¹äºæ³¨æ„åŠ›è®¡ç®—å…¬å¼çš„ç†è§£ï¼š   

$$
Attention(Q,K,V)=\text{softmax}(\frac{QK^T}{\sqrt{d_k}})V
$$

**é¦–å…ˆæ˜¯**å¯¹äºQã€Kã€Vå¦‚æ­¤è®¡ç®—ç¼˜ç”±ï¼Œ[è®ºæ–‡](https://arxiv.org/pdf/1706.03762)æœ€å¼€å§‹æ˜¯ç”¨åœ¨NLPä¸­ï¼Œå› æ­¤æˆ‘ä»¬ä»¥ NLP è§’åº¦æ¥è§£é‡Šã€‚å‡è®¾è¾“å…¥çš„ Qã€Kã€V å½¢çŠ¶ä¸º $n \times d_k$ï¼Œå…¶ä¸­ $n$ æ˜¯æ–‡æœ¬ token çš„æ•°é‡ï¼Œ$d_k$ æ˜¯é”®ï¼ˆKeyï¼‰å’ŒæŸ¥è¯¢ï¼ˆQueryï¼‰çš„ç»´åº¦ã€‚é€šè¿‡çº¿æ€§å˜æ¢ï¼Œæˆ‘ä»¬å°† token å¤„ç†ä¸º $d_k$ ç»´çš„è¡¨ç¤ºã€‚è®¡ç®— $QK^T$ åï¼Œå¾—åˆ°ä¸€ä¸ª $n \times n$ çš„çŸ©é˜µï¼Œå¯ä»¥ç†è§£ä¸º token ä¹‹é—´çš„**æ³¨æ„åŠ›æƒé‡**ã€‚éšåï¼Œæˆ‘ä»¬ç”¨è¿™äº›æ³¨æ„åŠ›æƒé‡åŠ æƒæ±‚å’Œ Value çŸ©é˜µ $V$ï¼Œä»è€Œæ›´æ–° token è¡¨ç¤ºã€‚
**å…¶æ¬¡**ä¸ºä»€ä¹ˆåœ¨å…¬å¼é‡Œé¢è¦é™¤$\sqrt{d_k}$å‘¢ï¼Ÿ**1.é˜²æ­¢å†…ç§¯å€¼è¿‡å¤§ï¼Œä¿æŒç¨³å®šçš„æ¢¯åº¦**ã€‚å‡è®¾ $Q$ å’Œ $K$ çš„æ¯ä¸ªå…ƒç´ æœä»å‡å€¼ä¸º 0ï¼Œæ–¹å·®ä¸º $\sigma^2$ çš„åˆ†å¸ƒã€‚$QK^T$ çš„æ¯ä¸ªå…ƒç´ æ˜¯ $d_k$ ä¸ªå…ƒç´ çš„å†…ç§¯ï¼ŒæŒ‰ç…§ç‹¬ç«‹åŒåˆ†å¸ƒå‡è®¾ï¼Œç»“æœçš„æ–¹å·®ä¼šéšç€ $d_k$ å¢å¤§è€Œå¢å¤§ï¼Œå¤§çº¦æ˜¯ $\mathbb{V}[QK^T] \approx d_k \sigma^2$ã€‚è¿™æ ·ï¼Œ$QK^T$ çš„å€¼ä¼šéšç€ $d_k$ çš„å¢å¤§è€Œå˜å¤§ï¼Œ**å¯¼è‡´ softmax å½’ä¸€åŒ–åï¼Œæ¢¯åº¦å˜å¾—å¾ˆå°ï¼Œè®­ç»ƒå˜å¾—ä¸ç¨³å®š**ã€‚é€šè¿‡é™¤ä»¥ $\sqrt{d_k}$ï¼Œå¯ä»¥è®© $QK^T$ çš„æ–¹å·®å¤§è‡´ä¿æŒåœ¨ 1 çš„æ•°é‡çº§ï¼Œä½¿ softmax è¾“å‡ºä¸ä¼šè¿‡äºæç«¯ï¼ˆæ¥è¿‘ 0 æˆ– 1ï¼‰ï¼Œä»è€Œä¿æŒè®­ç»ƒç¨³å®šæ€§ã€‚**2. è®© softmax å…·æœ‰åˆé€‚çš„åˆ†å¸ƒ**ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±softmax è®¡ç®—çš„æ˜¯ $e^{x_i}$ï¼Œå¦‚æœ $x_i$ è¿‡å¤§ï¼Œä¼šå¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼Œæ¨¡å‹éš¾ä»¥å­¦ä¹ ã€‚é€šè¿‡ $\sqrt{d_k}$ å½’ä¸€åŒ–ï¼Œæ§åˆ¶ $QK^T$ çš„èŒƒå›´ï¼Œä½¿ softmax è¾“å‡ºä¸ä¼šè¿‡äºæç«¯ï¼Œä»è€Œæé«˜è®­ç»ƒæ•ˆæœã€‚

2ã€ä¹‹æ‰€ä»¥è¦é‡‡ç”¨å¤šå¤´ï¼Œè¿™ä¸ªç†ç”±ä¹Ÿæ¯”è¾ƒç®€å•ï¼Œåœ¨è®¡ç®— $QK^T$ æ—¶ï¼Œåªèƒ½åŸºäºä¸€ä¸ªç›¸åŒçš„æŸ¥è¯¢-é”®è¡¨ç¤ºæ¥è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œå¯èƒ½ä¼šåå‘æŸä¸€ç§å…³ç³»æ¨¡å¼ï¼Œå¯¼è‡´æ¨¡å‹éš¾ä»¥æ•æ‰æ›´å¤šå±‚æ¬¡çš„è¯­ä¹‰ä¿¡æ¯
3ã€åœ¨æ¨¡å‹ç»“æ„é‡Œé¢çš„æ®‹å·®å¤„ç†æ€è·¯æ˜¯ï¼š$\text{Norm}(x+f(x))$ä¹Ÿå°±æ˜¯è¯´å…ˆé€šè¿‡MHAå¤„ç†è€Œåæ®‹å·®è¿æ¥æ¬¸ï¼Œä½†æ˜¯**æ®‹å·®ä¼šè¿›ä¸€æ­¥æ”¾å¤§æ–¹å·®** å› æ­¤ä¹Ÿæœ‰æå‡ºï¼š$x+\text{Norm}(f(x))$å‰é¢æåˆ°çš„ä¸¤ç§åˆ†åˆ«æ˜¯Post Normä»¥åŠPre Normã€‚å¯¹äºé‚£ç§å¥½é‚£ç§åå¹¶æ²¡æœ‰å¾ˆå¥½çš„è§£é‡Šï¼Œä¸æ­¤åŒæ—¶æœ‰å¦å¤–ä¸€ç§è¿æ¥æ–¹å¼ï¼š$x+ \alpha f(x)$åœ¨åç»­è®­ç»ƒä¸­ä¸æ–­æ›´æ–°$\alpha$ï¼Œ[å‚è€ƒ](https://spaces.ac.cn/archives/8620)$\alpha$ä»¥å›ºå®šçš„ã€å¾ˆå°çš„æ­¥é•¿æ…¢æ…¢é€’å¢ï¼Œç›´åˆ°å¢åŠ åˆ°$\alpha=1$å°±å›ºå®šä¸‹æ¥ã€‚

![](https://s2.loli.net/2025/03/02/2Csoc9fVhWPxHrv.png)

#### 2ã€`Casual Attention`

å› æœæ³¨æ„åŠ›çš„ä¸»è¦ç›®çš„æ˜¯é™åˆ¶æ³¨æ„åŠ›çš„è®¡ç®—ï¼Œä½¿å¾—æ¯ä¸ªä½ç½®çš„æŸ¥è¯¢åªèƒ½ä¸å½“å‰å’Œä¹‹å‰çš„ä½ç½®è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼Œè€Œä¸èƒ½â€œçª¥è§†â€æœªæ¥çš„ä½ç½®ã€‚å…·ä½“æ¥è¯´ï¼šå¯¹äºä½ç½®$ğ‘–$ï¼Œæ¨¡å‹åªèƒ½è€ƒè™‘ä½ç½® $1,2,...,ğ‘–$çš„ä¿¡æ¯ï¼Œè€Œä¸èƒ½è€ƒè™‘ä½ç½®$ğ‘–+1,ğ‘–+2,...,ğ‘›$ã€‚å› æ­¤ï¼Œå½“è®¡ç®—æ¯ä¸ªä½ç½®çš„æ³¨æ„åŠ›æ—¶ï¼Œé”®ï¼ˆkeyï¼‰å’Œå€¼ï¼ˆvalueï¼‰çš„ä½ç½®ä¼šè¢«é™åˆ¶åœ¨å½“å‰çš„ä½ç½®åŠå…¶ä¹‹å‰çš„ä½ç½®ã€‚å®ç°æ–¹å¼ä¹Ÿå¾ˆç®€å•ç›´æ¥æœ€æ³¨æ„åŠ›çŸ©é˜µè¿›è¡Œ**å±è”½**å³å¯ï¼Œæ¯”å¦‚è¯´æ³¨æ„åŠ›çŸ©é˜µä¸ºï¼š

![](https://s2.loli.net/2025/02/07/ovpbyFk3m75laGg.png)

### äºŒã€å†…å­˜ä¼˜åŒ–ç®¡ç†

#### 1ã€`Flash Attention`

[è®ºæ–‡](https://arxiv.org/pdf/2205.14135)æå‡ºï¼Œæ˜¯ä¸€ç§é«˜æ•ˆçš„æ³¨æ„åŠ›è®¡ç®—æ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³ Transformer æ¨¡å‹åœ¨å¤„ç†é•¿åºåˆ—æ—¶çš„è®¡ç®—æ•ˆç‡å’Œå†…å­˜æ¶ˆè€—é—®é¢˜ã€‚**å…¶æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡åœ¨ GPU æ˜¾å­˜ä¸­åˆ†å—æ‰§è¡Œæ³¨æ„åŠ›è®¡ç®—ï¼Œå‡å°‘æ˜¾å­˜è¯»å†™æ“ä½œï¼Œæå‡è®¡ç®—æ•ˆç‡å¹¶é™ä½æ˜¾å­˜å ç”¨**ã€‚

![1](https://s2.loli.net/2025/01/31/Gqe94YpAXKftVJg.png)

`Flash Attention`è®¡ç®—æœºåˆ¶ï¼š
**åˆ†å—è®¡ç®—**ï¼šä¼ ç»Ÿæ³¨æ„åŠ›è®¡ç®—ä¼šå°†æ•´ä¸ªæ³¨æ„åŠ›çŸ©é˜µ (NÃ—N) å­˜å…¥ GPU å†…å­˜ï¼ˆHBMï¼‰ï¼Œè¿™å¯¹é•¿åºåˆ—æ¥è¯´éå¸¸æ¶ˆè€—å†…å­˜ï¼ŒFlashAttention å°†è¾“å…¥åˆ†å—ï¼Œæ¯æ¬¡åªåŠ è½½ä¸€å°å—æ•°æ®åˆ°æ›´å¿«çš„ SRAM ä¸­è¿›è¡Œè®¡ç®—ï¼Œä¼ ç»Ÿ`Attention`è®¡ç®—å’Œ`flash attention`è®¡ç®—ï¼š
![1](https://s2.loli.net/2025/01/31/IbjDs6EKdO9VUJ2.png)

å¯¹æ¯”ä¸Šï¼šä¼ ç»Ÿçš„è®¡ç®—å’Œå­˜å‚¨éƒ½æ˜¯å‘ç”Ÿå†`HBM`ä¸Šï¼Œè€Œå¯¹äº`flash attention`åˆ™æ˜¯**é¦–å…ˆ**ä¼šå°†`Q,K,V`è¿›è¡Œåˆ’åˆ†ï¼ˆç®—æ³•1-4ï¼šæ•´ä½“æµç¨‹ä¸Šé¦–å…ˆæ ¹æ®`SRAM`çš„å¤§å°`M`å»è®¡ç®—åˆ’åˆ†æ¯”ä¾‹ï¼ˆ$\lceil \frac{N}{B_r} \rceil$ï¼‰ç„¶åæ ¹æ®åˆ’åˆ†æ¯”ä¾‹å»å¯¹`QKV`è¿›è¡Œåˆ’åˆ†è¿™æ ·ä¸€æ¥Qï¼ˆ$N\times d$å°±ä¼šè¢«åˆ’åˆ†ä¸ºä¸åŒçš„å°å—ï¼Œç„¶ååªéœ€è¦å»éå†è¿™äº›å°å—ç„¶åè®¡ç®—æ³¨æ„åŠ›å³å¯ï¼‰ï¼‰ï¼Œ**ç„¶åè®¡ç®—**`Attention`ï¼ˆç®—æ³•5-15ï¼‰ï¼Œè®¡ç®—ä¸­ä¹Ÿå®¹æ˜“å‘ç°ï¼šå…ˆå°†åˆ†å—å­˜å‚¨å†`HBM`ä¸Šçš„å€¼è¯»å–åˆ°`SRAM`ä¸Šå†å®ƒä¸Šé¢è¿›è¡Œè®¡ç®—ï¼Œä¸è¿‡å€¼å¾—æ³¨æ„çš„æ˜¯ï¼šåœ¨ä¼ ç»Ÿçš„$QK^T$è®¡ç®—ä¹‹åé€šè¿‡`softmax`è¿›è¡Œå¤„ç†ï¼Œä½†æ˜¯å¦‚æœå°†ä¸Šè¿°å€¼æ‹†åˆ†äº†ï¼Œå†å»ç”¨æ™®é€šçš„`softmax`å°±ä¸åˆé€‚ï¼Œå› æ­¤ä½¿ç”¨`safe softmax`

---
1ã€**HBM**ï¼ˆHigh Bandwidth Memoryï¼Œé«˜å¸¦å®½å†…å­˜ï¼‰:æ˜¯ä¸€ç§ä¸“ä¸ºé«˜æ€§èƒ½è®¡ç®—å’Œå›¾å½¢å¤„ç†è®¾è®¡çš„å†…å­˜ç±»å‹ï¼Œæ—¨åœ¨æä¾›é«˜å¸¦å®½å’Œè¾ƒä½çš„åŠŸè€—ã€‚HBM å¸¸ç”¨äºéœ€è¦å¤§é‡æ•°æ®è®¿é—®çš„ä»»åŠ¡ï¼Œå¦‚å›¾å½¢å¤„ç†ã€å¤§è§„æ¨¡çŸ©é˜µè¿ç®—å’Œ AI æ¨¡å‹è®­ç»ƒã€‚
2ã€ **SRAM**ï¼ˆStatic Random Access Memoryï¼Œé™æ€éšæœºå­˜å–å­˜å‚¨å™¨ï¼‰:æ˜¯ä¸€ç§é€Ÿåº¦æå¿«çš„å­˜å‚¨å™¨ï¼Œç”¨äºå­˜å‚¨å°å—æ•°æ®ã€‚åœ¨ GPU ä¸­ï¼ŒSRAM ä¸»è¦ä½œä¸ºç¼“å­˜ï¼ˆå¦‚å¯„å­˜å™¨æ–‡ä»¶ã€å…±äº«å†…å­˜å’Œç¼“å­˜ï¼‰ï¼Œç”¨äºå¿«é€Ÿè®¿é—®é¢‘ç¹ä½¿ç”¨çš„æ•°æ®ã€‚ä¾‹å¦‚åœ¨å›¾ä¸­ FlashAttention çš„è®¡ç®—ä¸­ï¼Œå°†å…³é”®çš„è®¡ç®—å—ï¼ˆå¦‚å°è§„æ¨¡çŸ©é˜µï¼‰å­˜æ”¾åœ¨ SRAM ä¸­ï¼Œå‡å°‘é¢‘ç¹çš„æ•°æ®ä¼ è¾“ï¼Œæå‡è®¡ç®—é€Ÿåº¦ã€‚
3ã€ä¸åŒ`softmax`è®¡ç®—ï¼š
`softmax`:$x_i=\frac{e^{x_i}}{\sum e^{x_j}}$
`safe softmax`ï¼ˆä¸»è¦é˜²æ­¢è¾“å‡ºè¿‡å¤§æº¢å‡ºï¼Œå°±å‡æœ€å¤§å€¼ï¼‰:$x_i=\frac{e^{x_i-max(x_{:N})}}{\sum e^{x_j-max(x_{:N})}}$

---

ä»£ç æ“ä½œï¼Œé¦–å…ˆå®‰è£…`flash-attn`ï¼š`pip install flash-attn`ã€‚ä»£ç ä½¿ç”¨ï¼š

```python
from flash_attn import flash_attn_func
import torch
import torch.nn as nn

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
q = torch.randn(32, 64, 8, int(1024/8)).to(device, dtype=torch.bfloat16)
out = flash_attn_func(q, q, q, causal= False)
print(out.shape)
```

`flash_attn_func`è¾“å…¥å‚æ•°ï¼š
1ã€`q,k,v`ï¼šå½¢çŠ¶ä¸ºï¼š`(batch_size, seqlen, nheads, headdim)`ä¹Ÿå°±æ˜¯è¯´ä¸€èˆ¬æ–‡æœ¬è¾“å…¥ä¸ºï¼š`(batch_size, seqlen, embed_dim)`è¦æ ¹æ®è®¾è®¡çš„`nheads`æ¥å¤„ç†è¾“å…¥çš„ç»´åº¦ï¼Œå¹¶ä¸”éœ€è¦ä¿è¯ï¼š`headdim`â‰¤256ï¼Œäºæ­¤åŒæ—¶è¦ä¿è¯æ•°æ®ç±»å‹ä¸ºï¼š`float16` æˆ– `bfloat16`
2ã€`causal`ï¼š`bool`åˆ¤æ–­æ˜¯ä¸æ˜¯ä½¿ç”¨`causal attention mask`

#### 2ã€`Multi-head Latent Attention`ï¼ˆ`MLA`ï¼‰

å¯¹äº[`KV-cache`](https://www.big-yellow-j.top/posts/2025/01/27/MoE-KV-cache.html)ä¼šå­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼šåœ¨æ¨ç†é˜¶æ®µè™½ç„¶å¯ä»¥åŠ å¿«æ¨ç†é€Ÿåº¦ï¼Œä½†æ˜¯å¯¹äºæ˜¾å­˜å ç”¨ä¼šæ¯”è¾ƒé«˜ï¼ˆå› ä¸º`KV`éƒ½ä¼šè¢«å­˜å‚¨ä¸‹æ¥ï¼Œå¯¼è‡´æ˜¾å­˜å ç”¨é«˜ï¼‰ï¼Œå¯¹äºæ­¤ç±»é—®é¢˜åç»­æå‡º`Grouped-Query-Attentionï¼ˆGQAï¼‰`ä»¥åŠ`Multi-Query-Attentionï¼ˆMQAï¼‰`å¯ä»¥é™ä½`KV-cache`çš„å®¹é‡é—®é¢˜ï¼Œä½†æ˜¯ä¼šå¯¼è‡´æ¨¡å‹çš„æ•´ä½“æ€§èƒ½ä¼šæœ‰ä¸€å®šçš„ä¸‹é™ã€‚

![1](https://s2.loli.net/2025/02/01/7rLk8NDKXm3aFuI.png)

> `MHA`: å°±æ˜¯æ™®é€šçš„è®¡ç®—æ–¹æ³•
> `GQA`: å°†å¤šä¸ª`Q`åˆ†ç»„ï¼Œå¹¶å…±äº«ç›¸åŒçš„`K`å’Œ`V`
> `MQA`: æ‰€æœ‰Attention Headå…±äº«åŒä¸€ä¸ª`K`ã€`V`
> 
> ![1](https://s2.loli.net/2025/02/01/86puHjycqt43Ow9.png)

å¯¹äº`MLA`ï¼ˆ[DeepSeek-V2](https://arxiv.org/pdf/2405.04434)ä»¥åŠ[DeepSeek-V3](https://arxiv.org/pdf/2412.19437v1)ä¸­éƒ½ç”¨åˆ°ï¼‰ä½œä¸ºä¸€ç§`KV-cache`å‹ç¼©æ–¹æ³•ï¼ŒåŸç†å¦‚ä¸‹ï¼š

$$
\mathbf{c}_{t}^{KV}=W^{DKV}\mathbf{h}_{t} \\
\mathbf{k}_{t}^{C}=W^{UK}\mathbf{c}_{t}^{KV} \\
\mathbf{v}_{t}^{C}=W^{UV}\mathbf{c}_{t}^{KV} \\
$$

![MLAå®Œæ•´è®¡ç®—è¿‡ç¨‹](https://s2.loli.net/2025/02/01/54VOc7slBMiXWTK.png)

> å¯¹äºä¸Šé¢å®Œæ•´çš„è®¡ç®—è¿‡ç¨‹ï¼Œå¯¹äºQä¹‹æ‰€ä»¥è¦è®¡ç®—ä¸¤æ¬¡ï¼ˆçº¿é™ç»´è€Œåå‡ç»´ï¼‰è€Œä¸æ˜¯åªå»è®¡ç®—ä¸€æ¬¡ï¼Œæ€è·¯å’ŒLoRAçš„ç›¸ä¼¼ï¼Œå°†ï¼š$xw$ä¸­çš„$w$åˆ†è§£ä¸ºä¸¤éƒ¨åˆ†æ›´åŠ å°çš„çŸ©é˜µï¼ˆå¯¹åº”ä¸Šè¿°å›¾ä¸­çš„$W^{DQ}\text{å’Œ}W^{UQ}$ï¼‰

ä»ä¸Šè¿°å…¬å¼ä¹Ÿå®¹æ˜“å‘ç°ï¼Œåœ¨`MLA`ä¸­åªæ˜¯å¯¹ç¼“å­˜è¿›è¡Œä¸€ä¸ªâ€œæ›¿æ¢â€æ“ä½œï¼Œç”¨ä¸€ä¸ªä½çº¬åº¦çš„$C_t^{KV}$æ¥ä»£æ›¿ï¼ˆä¹Ÿå°±æ˜¯è¯´ï¼š**åªéœ€è¦å­˜å‚¨$c_t^{KV}$å³å¯**ï¼‰åŸæœ¬çš„`KV`ï¼ˆæˆ–è€…è¯´å°†å®¹é‡å¤šçš„`KV`è¿›è¡ŒæŠ•å½±æ“ä½œï¼Œè¿™ä¸ªè¿‡ç¨‹å’Œ[LoRA](https://arxiv.org/pdf/2106.09685)æœ‰äº›è®¸ç›¸ä¼¼ï¼‰ï¼Œåœ¨è¿›è¡ŒæŠ•å½±æ“ä½œä¹‹åå°±éœ€è¦å¯¹`attention`è¿›è¡Œè®¡ç®—ã€‚å¯¹äºä¸Šè¿°å…¬å¼ç®€å•ç†è§£ï¼š
å‡è®¾è¾“å…¥æ¨¡å‹ï¼ˆè¾“å…¥åˆ°`Attention`ï¼‰æ•°æ®ä¸º$h_t$ï¼ˆå‡è®¾ä¸ºï¼š$n\times d$ï¼‰ï¼Œåœ¨ä¼ ç»Ÿçš„`KV-cache`ä¸­ä¼šå°†è®¡ç®—è¿‡ç¨‹ä¸­çš„`KV`ä¸æ–­ç¼“å­˜ä¸‹æ¥ï¼Œåœ¨åç»­è®¡ç®—è¿‡ç¨‹ä¸­â€œæ‹¿å‡ºæ¥â€ï¼ˆè¿™æ ·å°±ä¼šå¯¼è‡´éšç€è¾“å‡ºæ–‡æœ¬åŠ å¤šï¼Œå¯¼è‡´ç¼“å­˜çš„å ç”¨ä¸æ–­ç´¯è®¡ï¼š$\sum 2n\times d$ï¼‰ï¼Œå› æ­¤åœ¨`MLA`ä¸­çš„æ“ä½œå°±æ˜¯ï¼šå¯¹äº$h_t$è¿›è¡Œå‹ç¼©ï¼š$n \times d \times d \times d_s= n \times d_s$è¿™æ ·ä¸€æ¥æˆ‘å°±åªéœ€è¦ç¼“å­˜ï¼š$n \times d_s$å³å¯ï¼ˆå¦‚æœéœ€è¦å¤åŸå°±åªéœ€è¦å†å»ä¹˜ä¸€ä¸‹æ–°çš„çŸ©é˜µå³å¯ï¼‰

![MLA](https://s2.loli.net/2025/02/01/bLTQeUsHKE5MByc.png)

[éƒ¨åˆ†ä»£ç ](https://github.com/deepseek-ai/DeepSeek-V3/blob/b5d872ead062c94b852d75ce41ae0b10fcfa1c86/inference/model.py#L393)éƒ¨åˆ†å‚æ•°åˆå§‹åŒ–å€¼æŒ‰ç…§[236Bçš„è®¾ç½®ä¸­çš„è®¾å®š](https://github.com/deepseek-ai/DeepSeek-V3/blob/main/inference/configs/config_236B.json)ï¼š

```python
class MLA(nn.Module):
    def __init__(...):
        super().__init__()
        ...
        self.n_local_heads = args.n_heads // world_size # n_heads=128

        self.q_lora_rank = args.q_lora_rank # qè¢«å‹ç¼©çš„ç»´åº¦ || 1536
        self.kv_lora_rank = args.kv_lora_rank # KVè¢«å‹ç¼©çš„ç»´åº¦ || 512

        # QKå¸¦æ—‹è½¬ä½ç½®ç¼–ç ç»´åº¦å’Œä¸å¸¦æ—‹è½¬ä½ç½®ç¼–ç ç»´åº¦
        self.qk_nope_head_dim = args.qk_nope_head_dim # 128
        self.qk_rope_head_dim = args.qk_rope_head_dim # 64

        self.qk_head_dim = args.qk_nope_head_dim + args.qk_rope_head_dim # 192
        self.v_head_dim = args.v_head_dim # 128
        ...
        if self.q_lora_rank == 0:
            self.wq = ColumnParallelLinear(self.dim, self.n_heads * self.qk_head_dim)
        else:
            self.wq_a = Linear(self.dim, self.q_lora_rank)
            self.q_norm = RMSNorm(self.q_lora_rank)
            self.wq_b = ColumnParallelLinear(self.q_lora_rank, self.n_heads * self.qk_head_dim)

        self.wkv_a = Linear(self.dim, self.kv_lora_rank + self.qk_rope_head_dim)
        self.kv_norm = RMSNorm(self.kv_lora_rank)
        self.wkv_b = ColumnParallelLinear(self.kv_lora_rank, self.n_heads * (self.qk_nope_head_dim + self.v_head_dim))
        self.wo = RowParallelLinear(self.n_heads * self.v_head_dim, self.dim)
        self.softmax_scale = self.qk_head_dim ** -0.5
    
    def forward(self, ...):
        bsz, seqlen, _ = x.size() # å‡è®¾ä¸ºï¼š3, 100, 4096
        ...
        if self.q_lora_rank == 0:
            q = self.wq(x)
        else:
            q = self.wq_b(self.q_norm(self.wq_a(x))) # 3, 100, 192*128
        q = q.view(bsz, seqlen, self.n_local_heads, self.qk_head_dim) # 3, 100, 128, 192
        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1) # (3, 100, 128, 128), (3, 100, 128, 64)
        # ä½¿ç”¨RoPE 
        q_pe = apply_rotary_emb(q_pe, freqs_cis)

        kv = self.wkv_a(x) # 3, 100, 576
        kv, k_pe = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1) # (3,100,512) (3,100,64)
        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis)

        if attn_impl == "naive":
            q = torch.cat([q_nope, q_pe], dim=-1) # 3, 100, 128, 192
            kv = self.wkv_b(self.kv_norm(kv)) # 3, 100, 32768
            kv = kv.view(bsz, seqlen, self.n_local_heads, self.qk_nope_head_dim + self.v_head_dim) # 3, 100, 128, 256
            k_nope, v = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)
            k = torch.cat([k_nope, k_pe.expand(-1, -1, self.n_local_heads, -1)], dim=-1)
            # è®¾è®¡åˆ°å¤šå¡é›†ç¾¤start_pos:end_posæ˜¯å¤šå¡é›†ç¾¤ä¸Šçš„æ“ä½œ
            self.k_cache[:bsz, start_pos:end_pos] = k
            self.v_cache[:bsz, start_pos:end_pos] = v
            scores = torch.einsum("bshd,bthd->bsht", q, self.k_cache[:bsz, :end_pos]) * self.softmax_scale
        else:
            wkv_b = self.wkv_b.weight if self.wkv_b.scale is None else weight_dequant(self.wkv_b.weight, self.wkv_b.scale, block_size) 
            wkv_b = wkv_b.view(self.n_local_heads, -1, self.kv_lora_rank)
            q_nope = torch.einsum("bshd,hdc->bshc", q_nope, wkv_b[:, :self.qk_nope_head_dim])
            self.kv_cache[:bsz, start_pos:end_pos] = self.kv_norm(kv)
            self.pe_cache[:bsz, start_pos:end_pos] = k_pe.squeeze(2)
            scores = (torch.einsum("bshc,btc->bsht", q_nope, self.kv_cache[:bsz, :end_pos]) +
                      torch.einsum("bshr,btr->bsht", q_pe, self.pe_cache[:bsz, :end_pos])) * self.softmax_scale
        
        if mask is not None:
            scores += mask.unsqueeze(1)
        scores = scores.softmax(dim=-1, dtype=torch.float32).type_as(x)
        if attn_impl == "naive":
            x = torch.einsum("bsht,bthd->bshd", scores, self.v_cache[:bsz, :end_pos])
        else:
            x = torch.einsum("bsht,btc->bshc", scores, self.kv_cache[:bsz, :end_pos])
            x = torch.einsum("bshc,hdc->bshd", x, wkv_b[:, -self.v_head_dim:])
        x = self.wo(x.flatten(2))
        return x
```

ä¸è¿‡ **MLA**å­˜åœ¨ä¸€ä¸ªé—®é¢˜ï¼Œä¸å…¼å®¹ **RoPE**ï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼Œå› ä¸ºä½ å°†KVè¿›è¡Œå‹ç¼©ï¼‰ä»ä¸Šè¿°ä»£ç çš„è§’åº¦é™¤æ³•ç†è§£å¦‚ä½•ä½¿ç”¨`RoPE`ï¼Œä»ä¸Šé¢ä»£ç ä¸Šï¼Œæ— è®ºæ˜¯Qè¿˜æ˜¯KVéƒ½æ˜¯ä»å‹ç¼©åçš„å†…å®¹ä¸­åˆ†ç¦»é™¤éƒ¨åˆ†å†…å®¹ï¼Œç„¶åè®¡ç®—ç»“æœ

#### 3ã€`Page Attention`ï¼ˆ`vLLM`ï¼‰

ä¸Šè¿°æè¿°ä¸­ï¼š`Flash Attention`ï¼ˆåŠ å¿«é€Ÿåº¦ï¼‰ã€`MLA`ï¼ˆä¼˜åŒ–`KV-cache`å­˜å‚¨ï¼‰ï¼Œè€Œ`Page Attention`ä¹Ÿæ˜¯ä¸€ç§ä¼˜åŒ–æ–¹æ³•ï¼ˆåŒºåˆ«äº`MLA`ï¼Œ`page attention`æ˜¯å¯¹å†…å­˜è¿›è¡Œåˆ†é…ç®¡ç†ï¼‰ã€‚å‚è€ƒ[è®ºæ–‡](https://dl.acm.org/doi/pdf/10.1145/3600006.3613165)ä¸­æè¿°ï¼Œå¯¹äº`KV-cache`å­˜åœ¨3ä¸ªé—®é¢˜ï¼š

![](https://s2.loli.net/2025/02/02/lEp4YocVSIghJ1z.png)

1ã€**é¢„ç•™æµªè´¹ (Reserved)**ï¼šä¸ºå°†æ¥å¯èƒ½çš„ token é¢„ç•™çš„ç©ºé—´ï¼Œè¿™äº›ç©ºé—´è¢«ä¿ç•™ä½†æš‚æœªä½¿ç”¨ï¼Œå…¶ä»–è¯·æ±‚æ— æ³•ä½¿ç”¨è¿™äº›é¢„ç•™ç©ºé—´ï¼›
2ã€**å†…éƒ¨å†…å­˜ç¢ç‰‡åŒ–é—®é¢˜ï¼ˆinternal memory fragmentationï¼‰**ï¼šç³»ç»Ÿä¼šä¸ºæ¯ä¸ªè¯·æ±‚é¢„å…ˆåˆ†é…ä¸€å—è¿ç»­çš„å†…å­˜ç©ºé—´ï¼Œå¤§å°åŸºäºæœ€å¤§å¯èƒ½é•¿åº¦(æ¯”å¦‚2048ä¸ªtoken)ï¼Œä½†å®é™…è¯·æ±‚é•¿åº¦å¾€å¾€è¿œå°äºæœ€å¤§é•¿åº¦ï¼Œè¿™å¯¼è‡´é¢„åˆ†é…çš„å†…å­˜æœ‰å¤§é‡ç©ºé—´è¢«æµªè´¹ã€‚
3ã€**å¤–éƒ¨å†…å­˜ç¢ç‰‡åŒ–é—®é¢˜ï¼ˆexternal memory fragmentationï¼‰**ï¼šä¸åŒå†…å­˜å—ä¹‹é—´çš„é›¶æ•£ç©ºé—²ç©ºé—´ï¼Œè™½ç„¶æ€»ç©ºé—²ç©ºé—´è¶³å¤Ÿï¼Œä½†å› ä¸è¿ç»­è€Œéš¾ä»¥ä½¿ç”¨ã€‚

![](https://s2.loli.net/2025/02/02/M9DibRVTUFXqxjo.png)

åªæœ‰ **20.4%-38.2%** çš„tokenæ˜¯è¢«ä½¿ç”¨çš„ï¼Œå¤§éƒ¨åˆ†éƒ½è¢«æµªè´¹æ‰äº†ã€‚`Page Attention`å…è®¸åœ¨éè¿ç»­çš„å†…å­˜ç©ºé—´ä¸­å­˜å‚¨è¿ç»­çš„ key å’Œ value ã€‚å…·ä½“æ¥è¯´ï¼Œ`Page Attention`å°†æ¯ä¸ªåºåˆ—çš„ `KV-cache` åˆ’åˆ†ä¸ºå—ï¼Œæ¯ä¸ªå—åŒ…å«å›ºå®šæ•°é‡ token çš„é”®å’Œå€¼ã€‚åœ¨æ³¨æ„åŠ›è®¡ç®—æœŸé—´ï¼Œ`Page Attention`å†…æ ¸å¯ä»¥æœ‰æ•ˆåœ°è¯†åˆ«å’Œè·å–è¿™äº›å—ã€‚å¦‚ä½•ç†è§£ä¸Šé¢æè¿°å‘¢ï¼Ÿè¿˜æ˜¯å€Ÿç”¨è®ºæ–‡ä¸­çš„æè¿°ï¼š

![](https://s2.loli.net/2025/02/02/k6a4xh3AvZWmg9j.png)

æ¯”å¦‚è¯´æŒ‰ç…§ä¸Šé¢Promptè¦è¾“å‡ºï¼ˆå‡è®¾åªè¾“å‡ºè¿™äº›å†…å®¹ï¼‰ï¼šâ€œfathers brought a carâ€ï¼Œä¸€èˆ¬çš„å¥—è·¯å¯èƒ½æ˜¯ï¼šæ¯”å¦‚è¯´ï¼šâ€œFour score and seven years ago our xxxxxâ€ï¼ˆxxxä»£è¡¨é¢„ç•™ç©ºé—´ï¼‰å› ä¸ºå®é™…ä¸çŸ¥é“åˆ°åº•è¦è¾“å‡ºå¤šå°‘æ–‡æœ¬ï¼Œå› æ­¤ä¼šæå‰é¢„ç•™å¾ˆé•¿çš„ä¸€éƒ¨åˆ†ç©ºé—´ï¼ˆä½†æ˜¯å¦‚æœåªè¾“å‡º4ä¸ªå­—ç¬¦ï¼Œè¿™é¢„ç•™ç©ºé—´å°±è¢«æµªè´¹äº†ï¼‰ï¼Œå› æ­¤åœ¨`page attention`é‡Œé¢å°±åˆ°ç”¨ä¸€ç§â€œåˆ†å—â€çš„æ€æƒ³å¤„ç†ï¼Œä»¥ä¸Šå›¾ä¸ºä¾‹ï¼Œåˆ†ä¸º8ä¸ªBlockæ¯ä¸ªBlockåªèƒ½å­˜å‚¨4ä¸ªå†…å®¹ï¼Œå› æ­¤å°±å¯ä»¥é€šè¿‡ä¸€ä¸ª`Block Table`æ¥å»ºç«‹ä¸€ä¸ªè¡¨æ ¼å‘Šè¯‰é‚£äº›Blockå­˜å‚¨äº†å¤šå°‘ï¼Œå­˜å‚¨æ»¡äº†å°±å»å…¶ä»–Blobkç»§ç»­å­˜å‚¨ã€‚æ•´ä¸ªè¿‡ç¨‹å¦‚ä¸‹ï¼š

![](https://s2.loli.net/2025/02/02/3lWpNMUQyLojhP9.webp)

è¿™æ ·ä¸€æ¥æµªè´¹å°±åªä¼šå‘ç”Ÿåœ¨æœ€åä¸€ä¸ªBlockä¸­ï¼ˆæ¯”å¦‚è¯´å­˜å‚¨4ä¸ªä½†æ˜¯åªå­˜è¿›å»äº†1ä¸ªå°±ä¼šæµªè´¹3ä¸ªï¼‰
[ä»£ç ](https://docs.vllm.ai/en/latest/index.html)æ“ä½œï¼š

```bash
git lfs clone https://www.modelscope.cn/qwen/Qwen1.5-1.8B-Chat.git
```

```python
from vllm import LLM, SamplingParams
import torch

# Sample prompts.
prompts = [
    "Who're you?",
]
# Create a sampling params object.
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

# Create an LLM.
llm = LLM(model="./Qwen1.5-1.8B-Chat/", dtype= torch.float16, enforce_eager= True)
# Generate texts from the prompts. The output is a list of RequestOutput objects
# that contain the prompt, generated text, and other information.
outputs = llm.generate(prompts, sampling_params)
# Print the outputs.
for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")
```

## å‚è€ƒ

1ã€https://mloasisblog.com/blog/ML/AttentionOptimization
2ã€https://github.com/vllm-project/vllm
3ã€https://arxiv.org/pdf/2205.14135
4ã€https://zhuanlan.zhihu.com/p/676655352
5ã€https://arxiv.org/pdf/2405.04434
6ã€https://spaces.ac.cn/archives/10091
7ã€https://zhuanlan.zhihu.com/p/696380978
8ã€https://dl.acm.org/doi/pdf/10.1145/3600006.3613165
9ã€https://zhuanlan.zhihu.com/p/638468472
10ã€https://mloasisblog.com/blog/ML/AttentionOptimization
11ã€https://github.com/vllm-project/vllm
12ã€https://docs.vllm.ai/en/latest/index.html
13ã€https://arxiv.org/pdf/2103.03493
14ã€https://www.cnblogs.com/gongqk/p/14772297.html
15ã€https://spaces.ac.cn/archives/8620