---
layout: mypost
title: æ·±å…¥æµ…å‡ºäº†è§£ç”Ÿæˆæ¨¡å‹-6ï¼šå¸¸ç”¨åŸºåº§æ¨¡å‹ä¸ Adaptersç­‰è§£æ
categories: ç”Ÿæˆæ¨¡å‹
extMath: true
images: true
address: é•¿æ²™ğŸŒ·
show_footer_image: true
tags:
- ç”Ÿæˆæ¨¡å‹
- diffusion model
- ControlNet
- T2I-Adapter
- SD
- SDVL
show: true
stickie: true
description: æœ¬æ–‡ä¸»è¦ä»‹ç»åŸºäºUnetå’ŒDitæ¡†æ¶çš„åŸºåº§æ‰©æ•£æ¨¡å‹ï¼Œé‡ç‚¹å¯¹æ¯”SD1.5ä¸SDXLçš„æ ¸å¿ƒå·®å¼‚ï¼ŒåŒ…æ‹¬CLIPç¼–ç å™¨ï¼ˆSDXLé‡‡ç”¨åŒç¼–ç å™¨æ‹¼æ¥æå‡æ–‡æœ¬ç†è§£èƒ½åŠ›ï¼‰ã€å›¾åƒè¾“å‡ºç»´åº¦ï¼ˆSDXLé»˜è®¤1024x1024ä¼˜äºSD1.5çš„512x512ï¼‰åŠæŠ€æœ¯ä¼˜åŒ–ç­–ç•¥ã€‚è¿˜æ¶µç›–Imagençš„å¤šé˜¶æ®µç”Ÿæˆä¸åŠ¨æ€è°ƒæ•´æ–¹æ³•ï¼ŒDitæ¨¡å‹çš„patchåˆ‡åˆ†ä¸adaLNæ¨¡å—ï¼ŒHunyuan-DiTçš„åŒæ–‡æœ¬ç¼–ç å™¨ä¸æ—‹è½¬ä½ç½®ç¼–ç ï¼ŒFLUX.1çš„VAEé€šé“ä¼˜åŒ–ä¸æ—‹è½¬ä½ç½®ç¼–ç ï¼Œä»¥åŠSD3çš„ä¸‰æ–‡æœ¬ç¼–ç å™¨ä¸MM-Ditæ¶æ„ã€‚åŒæ—¶æ¶‰åŠVAEæ¨¡å‹é‡æ„è¡¨ç°å¯¹æ¯”ã€guidance_rescaleå‚æ•°å¯¹ç”Ÿæˆæ•ˆæœçš„å½±å“ï¼Œå’ŒAdaptersæŠ€æœ¯å¦‚ControlNetï¼ˆé›¶å·ç§¯å±‚æ¡ä»¶æ§åˆ¶ï¼‰ã€DreamBoothï¼ˆæ ·æœ¬å¾®è°ƒä¸ç±»åˆ«å…ˆéªŒæŸå¤±ï¼‰ç­‰æ’ä»¶å¼æ¨¡å‹è°ƒæ•´æ–¹æ³•ï¼Œæ—¨åœ¨å…¨é¢è§£æä¸åŒæ‰©æ•£æ¨¡å‹çš„ç»“æ„ç‰¹æ€§ä¸åº”ç”¨æŠ€æœ¯ã€‚
---

## åŸºåº§æ‰©æ•£æ¨¡å‹
ä¸»è¦ä»‹ç»åŸºäºUnetä»¥åŠåŸºäºDitæ¡†æ¶çš„åŸºåº§æ‰©æ•£æ¨¡å‹ä»¥åŠéƒ¨åˆ†GANå’ŒVAEæ¨¡å‹ï¼Œå…¶ä¸­SDè¿­ä»£ç‰ˆæœ¬æŒºå¤šçš„ï¼ˆä»1.2åˆ°3.5ï¼‰å› æ­¤æœ¬æ–‡ä¸»è¦é‡ç‚¹ä»‹ç»SD 1.5ä»¥åŠSDXLä¸¤ä¸ªåŸºåº§æ¨¡å‹ï¼Œä»¥åŠä¸¤è€…ä¹‹é—´çš„å¯¹æ¯”å·®å¼‚ï¼Œé™¤æ­¤ä¹‹å¤–è¿˜æœ‰è®¸å¤šé—­æºçš„æ‰©æ•£æ¨¡å‹æ¯”å¦‚è¯´Imagenã€DALEç­‰ã€‚å¯¹äºDitåŸºåº§æ¨¡å‹ä¸»è¦ä»‹ç»ï¼šHunyuan-DiTã€FLUX.1ç­‰ã€‚å¯¹äºå„ç±»æ¨¡å‹è¯„åˆ†ç½‘ç«™ï¼ˆæ¨¡å‹è¯„åˆ†ä»è€…è§ä»æ™ºè€…è§æ™ºï¼Œç‰¹åˆ«æ˜¯æ­¤ç±»ç”Ÿæˆæ¨¡å‹è§†è§‰å›¾åƒç”Ÿæˆæ˜¯ä¸€ä¸ªå¾ˆä¸»è§‚çš„è¿‡ç¨‹ï¼ŒåŒä¸€å¼ å›¾ç‰‡ä¸åŒäººè§†è§‰æ„Ÿå®˜éƒ½æ˜¯ä¸åŒçš„ï¼‰ï¼š[https://lmarena.ai/leaderboard](https://lmarena.ai/leaderboard)

### SDv1.5 vs SDXL[^1]
> **SDv1.5**: https://huggingface.co/stable-diffusion-v1-5/stable-diffusion-v1-5
> **SDXL**:https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0

ä¸¤è€…æ¨¡å‹è¯¦ç»†çš„æ¨¡å‹ç»“æ„ï¼š[SDv1.5--SDXLæ¨¡å‹ç»“æ„å›¾](https://1drv.ms/u/c/667854cf645e8766/ESgZEHNEn3RJsKY0t1KQAgABYKHDhQtutJztw6OhEt9DPg?e=5SqEro)ï¼Œå…¶ä¸­å…·ä½“æ¨¡å‹å‚æ•°çš„å¯¹æ¯”å¦‚ä¸‹ï¼š
**1ã€CLIPç¼–ç å™¨åŒºåˆ«**ï¼š
åœ¨SD1.5ä¸­é€‰æ‹©çš„æ˜¯**CLIP-ViT/L**ï¼ˆå¾—åˆ°çš„ç»´åº¦ä¸ºï¼š768ï¼‰è€Œåœ¨SDXLä¸­é€‰æ‹©çš„æ˜¯ä¸¤ä¸ªCLIPæ–‡æœ¬ç¼–ç å™¨ï¼š**OpenCLIP-ViT/G**ï¼ˆå¾—åˆ°çš„ç»´åº¦ä¸ºï¼š1280ï¼‰ä»¥åŠ**CLIP-ViT/L**ï¼ˆå¾—åˆ°ç»´åº¦ä¸ºï¼š768ï¼‰åœ¨ä»£ç ä¸­å¯¹äºä¸¤ä¸ªæ–‡æœ¬é€šè¿‡ç¼–ç å™¨å¤„ç†ä¹‹åSDXLç›´æ¥é€šè¿‡catæ–¹å¼æ‹¼æ¥ï¼š`prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)` ä¹Ÿå°±æ˜¯è¯´æœ€åå¾—åˆ°çš„ç»´åº¦ä¸ºï¼š[..,..,1280+768]ã€‚æœ€åæ•ˆæœå¾ˆæ˜æ˜¾ï¼š**SDXLå¯¹äºæ–‡æœ¬çš„ç†è§£èƒ½åŠ›å¤§äºSD1.5**
**2ã€å›¾åƒè¾“å‡ºç»´åº¦åŒºåˆ«**ï¼š
å†SD1.5ä¸­çš„é»˜è®¤è¾“å‡ºæ˜¯ï¼š512x512è€Œå†SDXLä¸­çš„é»˜è®¤è¾“å‡ºæ˜¯ï¼š1024x1024ï¼Œå¦‚æœå¸Œæœ›å°†SD1.5ç”Ÿæˆçš„å›¾åƒå¤„ç†ä¸º1024x1024å¯ä»¥ç›´æ¥é€šè¿‡è¶…åˆ†ç®—æ³•æ¥è¿›è¡Œå¤„ç†ï¼Œé™¤æ­¤ä¹‹å¤–åœ¨SDXLä¸­è¿˜ä¼šä½¿ç”¨ä¸€ä¸ªrefineræ¨¡å‹ï¼ˆå’ŒUnetçš„ç»“æ„ç›¸ä¼¼ï¼‰æ¥å¼ºåŒ–baseæ¨¡å‹ï¼ˆUnetï¼‰ç”Ÿæˆçš„æ•ˆæœã€‚
**3ã€SDXLè®ºæ–‡ä¸­çš„æŠ€æœ¯ç»†èŠ‚**ï¼š
* 1ã€**å›¾åƒåˆ†è¾¨ç‡ä¼˜åŒ–ç­–ç•¥**ã€‚

æ•°æ®é›†ä¸­å›¾åƒçš„å°ºå¯¸å›¾åƒåˆ©ç”¨ç‡é—®é¢˜ï¼ˆé€‰æ‹©512x512èˆå¼ƒ256x256å°±ä¼šå¯¼è‡´å›¾åƒå¤§é‡è¢«èˆå¼ƒï¼‰å¦‚æœé€šè¿‡è¶…åˆ†è¾¨ç‡ç®—æ³•å°†å›¾åƒå°±è¡Œæ‰©å±•ä¼šæ”¾å¤§ä¼ªå½±ï¼Œè¿™äº›ä¼ªå½±å¯èƒ½ä¼šæ³„æ¼åˆ°æœ€ç»ˆçš„æ¨¡å‹è¾“å‡ºä¸­ï¼Œä¾‹å¦‚ï¼Œå¯¼è‡´æ ·æœ¬æ¨¡ç³Šã€‚ï¼ˆThe second method, on the other hand, usually introduces upscaling artifacts which may leak into the final model outputs, causing, for example, blurry samples.ï¼‰ä½œè€…åšæ³•æ˜¯ï¼š**è®­ç»ƒé˜¶æ®µ**ç›´æ¥å°†åŸå§‹å›¾åƒçš„åˆ†è¾¨ç‡ $c=(h_{org},w_{org})$ä½œä¸ºä¸€ä¸ªæ¡ä»¶ï¼Œé€šè¿‡å‚…é‡Œå¶ç‰¹å¾ç¼–ç è€ŒååŠ å…¥åˆ°time embeddingä¸­ï¼Œ**æ¨ç†é˜¶æ®µ**ç›´æ¥å°†åˆ†è¾¨ç‡ä½œä¸ºä¸€ä¸ªæ¡ä»¶å°±è¡ŒåµŒå…¥ï¼Œè¿›è€Œå®ç°ï¼š**å½“è¾“å…¥ä½åˆ†è¾¨ç‡æ¡ä»¶æ—¶ï¼Œç”Ÿæˆçš„å›¾åƒè¾ƒæ¨¡ç³Šï¼›åœ¨ä¸æ–­å¢å¤§åˆ†è¾¨ç‡æ¡ä»¶æ—¶ï¼Œç”Ÿæˆçš„å›¾åƒè´¨é‡ä¸æ–­æå‡ã€‚**
![image.png](https://s2.loli.net/2025/07/09/pMcLmdHThu2CnNx.webp)

* 2ã€**å›¾åƒè£å‰ªä¼˜åŒ–ç­–ç•¥**

ç›´æ¥ç»Ÿä¸€é‡‡æ ·è£å‰ªåæ ‡topå’Œcleftï¼ˆåˆ†åˆ«æŒ‡å®šä»å·¦ä¸Šè§’æ²¿é«˜åº¦å’Œå®½åº¦è½´è£å‰ªçš„åƒç´ æ•°é‡çš„æ•´æ•°ï¼‰ï¼Œå¹¶é€šè¿‡å‚…é‡Œå¶ç‰¹å¾åµŒå…¥å°†å®ƒä»¬ä½œä¸ºè°ƒèŠ‚å‚æ•°è¾“å…¥æ¨¡å‹ï¼Œç±»ä¼¼äºä¸Šé¢æè¿°çš„å°ºå¯¸è°ƒèŠ‚ã€‚ç¬¬1ï¼Œ2ç‚¹ä»£ç ä¸­çš„å¤„ç†æ–¹å¼ä¸ºï¼š
```python
def _get_add_time_ids(
        self, original_size, crops_coords_top_left, target_size, dtype, text_encoder_projection_dim=None
    ):
    add_time_ids = list(original_size + crops_coords_top_left + target_size)

    passed_add_embed_dim = (
        self.unet.config.addition_time_embed_dim * len(add_time_ids) + text_encoder_projection_dim
    )
    expected_add_embed_dim = self.unet.add_embedding.linear_1.in_features
    ...
    add_time_ids = torch.tensor([add_time_ids], dtype=dtype)
    return add_time_ids
```

> **æ¨èé˜…è¯»**ï¼š
> 1ã€[SDv1.5-SDXL-SD3ç”Ÿæˆæ•ˆæœå¯¹æ¯”](https://www.magicflow.ai/showcase/sd3-sdxl-sd1.5)

### Imagen
> https://imagen.research.google/
> https://deepmind.google/models/imagen/
> éå®˜æ–¹å®ç°ï¼šhttps://github.com/lucidrains/imagen-pytorch
> ç±»ä¼¼Githubï¼Œé€šè¿‡3é˜¶æ®µç”Ÿæˆï¼šhttps://github.com/deep-floyd/IF

Imagen[^6]è®ºæ–‡ä¸­ä¸»è¦æå‡ºï¼š1ã€çº¯æ–‡æœ¬è¯­æ–™åº“ä¸Šé¢„è®­ç»ƒçš„é€šç”¨å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆä¾‹å¦‚[T5](https://huggingface.co/collections/google/t5-release-65005e7c520f8d7b4d037918)ã€CLIPã€BERTç­‰ï¼‰åœ¨ç¼–ç å›¾åƒåˆæˆçš„æ–‡æœ¬æ–¹é¢éå¸¸æœ‰æ•ˆï¼šåœ¨Imagenä¸­å¢åŠ è¯­è¨€æ¨¡å‹çš„å¤§å°æ¯”å¢åŠ å›¾åƒæ‰©æ•£æ¨¡å‹çš„å¤§å°æ›´èƒ½æé«˜æ ·æœ¬ä¿çœŸåº¦å’ŒImagetextå¯¹é½ã€‚
![](https://s2.loli.net/2025/07/12/lCFNWwDmgGnZueE.webp)

2ã€é€šè¿‡æé«˜classifier-free guidance weightï¼ˆ$\epsilon(z,c)=w\epsilon(z,c)+ (1-w)\epsilon(z)$ ä¹Ÿå°±æ˜¯å…¶ä¸­çš„å‚æ•° $w$ï¼‰å¯ä»¥æé«˜image-textä¹‹é—´çš„å¯¹é½ï¼Œä½†ä¼šæŸå®³å›¾åƒé€¼çœŸåº¦ï¼Œäº§ç”Ÿé«˜åº¦é¥±å’Œä¸è‡ªç„¶çš„å›¾åƒï¼ˆè®ºæ–‡é‡Œé¢ç»™å‡ºçš„åˆ†ææ˜¯ï¼šæ¯ä¸ªæ—¶é—´æ­¥ä¸­é¢„æµ‹å’Œæ­£å¼çš„xéƒ½ä¼šé™å®šåœ¨ $[-1,1]$è¿™ä¸ªèŒƒå›´ä½†æ˜¯è¾ƒå¤§çš„ $w$å¯èƒ½å¯¼è‡´è¶…å‡ºè¿™ä¸ªèŒƒå›´ï¼‰ï¼Œè®ºæ–‡é‡Œé¢åšæ³•å°±æ˜¯æå‡º **åŠ¨æ€è°ƒæ•´æ–¹æ³•**ï¼šåœ¨æ¯ä¸ªé‡‡æ ·æ­¥éª¤ä¸­ï¼Œæˆ‘ä»¬å°†sè®¾ç½®ä¸º $x_0^t$ä¸­çš„æŸä¸ªç™¾åˆ†ä½ç»å¯¹åƒç´ å€¼ï¼Œå¦‚æœs>1ï¼Œåˆ™æˆ‘ä»¬å°† $x_0^t$é˜ˆå€¼è®¾ç½®ä¸ºèŒƒå›´ $[-s,s]$ï¼Œç„¶åé™¤ä»¥sã€‚
![](https://s2.loli.net/2025/07/12/jAEBS7I1Ob6DPal.webp)

3ã€å’Œä¸Šé¢SDæ¨¡å‹å·®å¼‚æ¯”è¾ƒå¤§çš„ä¸€ç‚¹å°±æ˜¯ï¼Œåœ¨imagenä¸­ç›´æ¥ä½¿ç”¨å¤šé˜¶æ®µç”Ÿæˆç­–ç•¥ï¼Œæ¨¡å‹å…ˆç”Ÿæˆ64x64å›¾åƒå†å»é€šè¿‡è¶…åˆ†è¾¨ç‡æ‰©æ•£æ¨¡å‹å»ç”Ÿæˆ256x256ä»¥åŠ1024x1024çš„å›¾åƒï¼Œåœ¨æ­¤è¿‡ç¨‹ä¸­ä½œè€…æåˆ°ä½¿ç”¨noise conditioning augmentationï¼ˆNCAï¼‰ç­–ç•¥ï¼ˆ**å¯¹è¾“å…¥çš„æ–‡æœ¬ç¼–ç åå†å»æ·»åŠ éšæœºå™ªå£°**ï¼‰
![](https://s2.loli.net/2025/07/12/HJm96oPr2AlXICs.webp)

### Dit
> https://github.com/facebookresearch/DiT

![](https://s2.loli.net/2025/07/15/CUisy5TPE24kKaH.webp)

Dit[^11]æ¨¡å‹ç»“æ„ä¸Šï¼Œ1ã€**æ¨¡å‹è¾“å…¥**ï¼Œå°†è¾“å…¥çš„image/latentåˆ‡åˆ†ä¸ºä¸åŒpatchè€Œåå»å¯¹ä¸åŒç¼–ç åçš„patchä¸Šå»æ·»åŠ ä½ç½®ç¼–ç ï¼ˆç›´æ¥ä½¿ç”¨çš„sin-cosä½ç½®ç¼–ç ï¼‰ï¼Œ2ã€**æ—¶é—´æ­¥ä»¥åŠæ¡ä»¶ç¼–ç **ï¼Œå¯¹äºæ—¶é—´æ­¥tä»¥åŠæ¡ä»¶cçš„ç¼–ç è€Œåå°†ä¸¤éƒ¨åˆ†ç¼–ç åçš„å†…å®¹è¿›è¡Œç›¸åŠ ï¼Œåœ¨`TimestepEmbedder`ä¸Šå¤„ç†æ–¹å¼æ˜¯ï¼šç›´æ¥é€šè¿‡**æ­£å¼¦æ—¶é—´æ­¥åµŒå…¥**æ–¹å¼è€Œåå°†ç¼–ç åçš„å†…å®¹é€šè¿‡ä¸¤å±‚linerå¤„ç†ï¼›åœ¨`LabelEmbedder`å¤„ç†æ–¹å¼ä¸Šå°±æ¯”è¾ƒç®€å•ç›´æ¥é€šè¿‡`nn.Embedding`è¿›è¡Œç¼–ç å¤„ç†ã€‚3ã€ä½¿ç”¨Adaptive layer normï¼ˆadaLNï¼‰blockä»¥åŠadaZero-Blockï¼ˆå¯¹æœ‰äº›å‚æ•°åˆå§‹åŒ–ä¸º0ï¼Œå°±å’Œloraä¸­ä¸€æ ·åˆå§‹åŒ–ABä¸º0ï¼Œä¸ºäº†ä¿è¯åç»­æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„ç¨³å®šï¼‰
> åœ¨[layernorm](https://docs.pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)ä¸­ä¸€èˆ¬å½’ä¸€åŒ–å¤„ç†æ–¹å¼ä¸ºï¼š$\text{Norm}(x)=\gamma \frac{x-\mu}{\sqrt{\sigma^2+ \epsilon}}+\beta$ å…¶ä¸­æœ‰ä¸¤ä¸ªå‚æ•° $\gamma$ å’Œ $\beta$ æ˜¯å›ºå®šçš„å¯å­¦ä¹ å‚æ•°ï¼ˆæ¯”å¦‚è¯´ç›´æ¥é€šè¿‡ `nn.Parameter` è¿›è¡Œåˆ›å»ºï¼‰ï¼Œåœ¨æ¨¡å‹åˆå§‹åŒ–æ—¶åˆ›å»ºï¼Œå¹¶åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ã€‚ä½†æ˜¯åœ¨ adaLNä¸­åˆ™æ˜¯ç›´æ¥é€šè¿‡ $\text{Norm}(x)=\gamma(c) \frac{x-\mu}{\sqrt{\sigma^2+ \epsilon}}+\beta(c)$ é€šè¿‡è¾“å…¥çš„æ¡ä»¶cè¿›è¡Œå­¦ä¹ çš„ï¼Œ

### Hunyuan-DiT
> https://huggingface.co/Tencent-Hunyuan/HunyuanDiT

è…¾è®¯çš„Hunyuan-DiT[^8]æ¨¡å‹æ•´ä½“ç»“æ„

![](https://s2.loli.net/2025/07/15/Hum9FCtPbV7do1B.webp)

æ•´ä½“æ¡†æ¶ä¸æ˜¯å¾ˆå¤æ‚ï¼Œ1ã€æ–‡æœ¬ç¼–ç ä¸Šç›´æ¥é€šè¿‡ç»“åˆä¸¤ä¸ªç¼–ç å™¨ï¼šCLIPã€T5ï¼›2ã€VAEåˆ™æ˜¯ç›´æ¥ä½¿ç”¨çš„SD1.5çš„ï¼›3ã€å¼•å…¥2ç»´çš„æ—‹è½¬ä½ç½®ç¼–ç ï¼›4ã€åœ¨Ditç»“æ„ä¸Šï¼ˆå›¾ç‰‡VAEå‹ç¼©è€Œåå»åˆ‡åˆ†æˆä¸åŒpatchï¼‰ï¼Œä½¿ç”¨çš„æ˜¯å †å çš„æ³¨æ„åŠ›æ¨¡å—ï¼ˆåœ¨SD1.5ä¸­ä¹Ÿæ˜¯è¿™ç§ç»“æ„ï¼‰self-attention+cross-attentionï¼ˆæ­¤éƒ¨åˆ†è¾“å…¥æ–‡æœ¬ï¼‰ã€‚è®ºæ–‡é‡Œé¢åšäº†æ”¹è¿›æªæ–½ï¼š1ã€å€Ÿé‰´ä¹‹å‰å¤„ç†ï¼Œè®¡ç®—attentionä¹‹å‰é¦–å…ˆè¿›è¡Œnormå¤„ç†ï¼ˆä¹Ÿå°±æ˜¯å°†normæ‹¿åˆ°attentionå‰é¢ï¼‰ã€‚

ç®€çŸ­äº†è§£ä¸€ä¸‹æ¨¡å‹æ˜¯å¦‚ä½•åšæ•°æ®çš„ï¼š
![](https://s2.loli.net/2025/07/15/dJZETbyHB6SQPKI.webp)


### PixArt
> https://pixart-alpha.github.io/

åä¸ºè¯ºäºšæ–¹èˆŸå®éªŒå®¤æå‡ºçš„ $\text{PixArt}-\alpha$æ¨¡å‹æ•´ä½“æ¡†æ¶å¦‚ä¸‹ï¼š
![](https://s2.loli.net/2025/07/15/cWTtLdONRPC9fnz.webp)

ç›¸æ¯”è¾ƒDitæ¨¡å‹è®ºæ–‡é‡Œé¢ä¸»è¦è¿›è¡Œçš„æ”¹è¿›å¦‚ä¸‹ï¼š
1ã€**Cross-Attention layer**ï¼Œåœ¨DiT blockä¸­åŠ å…¥äº†ä¸€ä¸ªå¤šå¤´äº¤å‰æ³¨æ„åŠ›å±‚ï¼Œå®ƒä½äºè‡ªæ³¨æ„åŠ›å±‚ï¼ˆä¸Šå›¾ä¸­çš„Multi-Head Self
-Attentionï¼‰å’Œå‰é¦ˆå±‚ï¼ˆPointwise Feedforwardï¼‰ä¹‹é—´ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿçµæ´»åœ°å¼•å…¥æ–‡æœ¬åµŒå…¥æ¡ä»¶ã€‚æ­¤å¤–ï¼Œä¸ºäº†åˆ©ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œå°†äº¤å‰æ³¨æ„åŠ›å±‚ä¸­çš„è¾“å‡ºæŠ•å½±å±‚åˆå§‹åŒ–ä¸ºé›¶ï¼Œä½œä¸ºæ’ç­‰æ˜ å°„ï¼Œä¿ç•™äº†è¾“å…¥ä»¥ä¾›åç»­å±‚ä½¿ç”¨ã€‚
2ã€AdaLN-singleï¼Œåœ¨Ditä¸­çš„adaptive normalization layersï¼ˆadaLNï¼‰ä¸­éƒ¨åˆ†å‚æ•°ï¼ˆ27%ï¼‰æ²¡æœ‰èµ·ä½œç”¨ï¼ˆåœ¨æ–‡ç”Ÿå›¾ä»»åŠ¡ä¸­ï¼‰å°†å…¶æ›¿æ¢ä¸ºadaLN-single

### SD3ã€FLUX.1ã€FLUX1.1
> FLUXæ¨¡å‹**å•†ä¸šä¸å¼€æº**å¹¶ä¸”æ¨¡å‹çš„ç»¼åˆè¡¨ç°ä¸Šä¸€èˆ¬è€Œè¨€fluxä¼šæ¯”è¾ƒå¥½ï¼ˆæ¨¡å‹ç”Ÿæˆæ•ˆæœå¯¹æ¯”ï¼š[ğŸ”—](https://medium.com/@tanshaoyu160/15-photorealistic-ai-images-comparison-flux1-1-vs-sd3-5-6a49fbce05db)ï¼‰
> SD3çš„diffuserså®˜æ–¹æ–‡æ¡£ï¼š[StableDiffusion3Pipeline](https://huggingface.co/docs/diffusers/en/api/pipelines/stable_diffusion/stable_diffusion_3#diffusers.StableDiffusion3Pipeline)

https://zhouyifan.net/2024/09/03/20240809-flux1/
SD3[^12]ã€FLUXå¯¹äºè¿™å‡ ç»„æ¨¡å‹çš„å‰ä¸–ä»Šç”Ÿä¸åšä»‹ç»ï¼Œä¸»è¦äº†è§£å…¶æ¨¡å‹ç»“æ„ä»¥åŠè®ºæ–‡é‡Œé¢æ‰€æ¶‰åŠåˆ°åˆ°çš„ä¸€äº›çŸ¥è¯†ç‚¹ã€‚é¦–å…ˆä»‹ç»SD3æ¨¡å‹åœ¨æ¨¡å‹æ”¹è¿›ä¸Š[^16]ï¼š1ã€æ”¹å˜è®­ç»ƒæ—¶å™ªå£°é‡‡æ ·æ–¹æ³•ï¼›2ã€å°†ä¸€ç»´ä½ç½®ç¼–ç æ”¹æˆäºŒç»´ä½ç½®ç¼–ç ï¼›3ã€æå‡ VAE éšç©ºé—´é€šé“æ•°ï¼ˆä½œè€…å®éªŒå‘ç°æœ€å¼€å§‹VAEä¼šå°†æ¨¡å‹**ä¸‹é‡‡æ ·8å€æ•°å¹¶ä¸”å¤„ç†é€šé“ä¸º4çš„ç©ºé—´**ï¼Œä¹Ÿå°±æ˜¯è¯´ $512 \times 512 \times 3 \rightarrow 64\times 64 \times 4$ï¼Œä¸è¿‡åœ¨ **SD3**ä¸­å°†é€šé“æ•°ç”±**4æ”¹ä¸º16**ï¼‰ï¼›4ã€å¯¹æ³¨æ„åŠ› QK åšå½’ä¸€åŒ–ä»¥ç¡®ä¿é«˜åˆ†è¾¨ç‡ä¸‹è®­ç»ƒç¨³å®šã€‚
![](https://s2.loli.net/2025/09/01/R5HI3yLPXBEbtzQ.webp)
å…¶ä¸­SD3æ¨¡å‹çš„æ•´ä½“æ¡†æ¶å¦‚ä¸Šæ‰€è¿°:
**1ã€æ–‡æœ¬ç¼–ç å™¨å¤„ç†**ï¼ˆ[ä»£ç ](https://github.com/huggingface/diffusers/blob/0f252be0ed42006c125ef4429156cb13ae6c1d60/src/diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3.py#L972)ï¼‰ï¼Œåœ¨text encoderä¸ŠSD3ä½¿ç”¨ä¸‰ä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼š`clip-vit-large-patch14`ã€ `laion/CLIP-ViT-bigG-14-laion2B-39B-b160k` ã€ `t5-v1_1-xxl` ï¼Œå¯¹äºè¿™3ä¸ªæ–‡æœ¬ç¼–ç å™¨å¯¹äºæ–‡æœ¬çš„å¤„ç†è¿‡ç¨‹ä¸ºï¼šå°±åƒSDXLä¸­ä¸€æ ·é¦–å…ˆ3ä¸ªç¼–ç å™¨åˆ†åˆ«éƒ½å»å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ï¼Œé¦–å…ˆå¯¹äºä¸¤ä¸ª[CLIPçš„æ–‡æœ¬ç¼–ç ](https://github.com/huggingface/diffusers/blob/0f252be0ed42006c125ef4429156cb13ae6c1d60/src/diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3.py#L289)å¤„ç†è¿‡ç¨‹ä¸ºç›´æ¥é€šè¿‡CLIPè¿›è¡Œ `prompt_embeds = text_encoder(text_input_ids.to(device)...)` è€Œåå»é€‰æ‹© `prompt_embeds.hidden_states[-(clip_skip + 2)]`ï¼ˆé»˜è®¤æ¡ä»¶ä¸‹ `clip_skip=None`ä¹Ÿå°±æ˜¯**ç›´æ¥é€‰æ‹©å€’æ•°ç¬¬äºŒå±‚**ï¼‰é‚£ä¹ˆæœ€åå¾—åˆ°æ–‡æœ¬ç¼–ç çš„ç»´åº¦ä¸ºï¼š`torch.Size([1, 77, 768]) torch.Size([1, 77, 1280])` è€Œ[T5çš„encoder](https://github.com/huggingface/diffusers/blob/0f252be0ed42006c125ef4429156cb13ae6c1d60/src/diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3.py#L233)å°±æ¯”è¾ƒæ£€æŸ¥ç›´æ¥é€šè¿‡encoderè¿›è¡Œç¼–ç ï¼Œé‚£ä¹ˆå…¶ç¼–ç ç»´åº¦ä¸ºï¼š`torch.Size([1, 256, 4096])`ï¼Œè¿™æ ·ä¸€æ¥å°±ä¼šå¾—åˆ°3ç»„çš„æ–‡ç¼–ç ï¼Œå¯¹äºCLIPçš„ç¼–ç ç»“æœç›´æ¥é€šè¿‡`clip_prompt_embeds=torch.cat([prompt_embed, prompt_2_embed], dim=-1)` å³å¯ï¼Œåœ¨å°†å¾—åˆ°åçš„ `clip_prompt_embeds`ç»“æœå†å»å’ŒT5çš„ç¼–ç ç»“æœè¿›è¡Œæ‹¼æ¥ä¹‹å‰ä¼šé¦–å…ˆ `clip_prompt_embeds=torch.nn.functional.pad(clip_prompt_embeds, (0, t5_prompt_embed.shape[-1] - clip_prompt_embeds.shape[-1]))` è€Œåå°†T5çš„æ–‡æœ¬å†…å®¹å’Œ `clip_prompt_embeds`è¿›è¡Œåˆå¹¶ `prompt_embeds = torch.cat([clip_prompt_embeds, t5_prompt_embed], dim=-2)`ã€‚ç”±äºä½¿ç”¨T5æ¨¡å‹å¯¼è‡´æ¨¡å‹çš„å‚æ•°æ¯”è¾ƒå¤§è¿›å¯¼è‡´æ¨¡å‹çš„æ˜¾å­˜å ç”¨è¿‡å¤§ï¼ˆ2080Tiç­‰GPUä¸Šè½»é‡åŒ–çš„éƒ¨ç½²æ¨ç†SD 3æ¨¡å‹ï¼Œå¯ä»¥åªä½¿ç”¨CLIP ViT-L + OpenCLIP ViT-bigGçš„ç‰¹å¾ï¼Œæ­¤æ—¶éœ€è¦**å°†T5-XXLçš„ç‰¹å¾è®¾ç½®ä¸ºzero**ï¼ˆä¸åŠ è½½ï¼‰[^14]ï¼‰ï¼Œé€‰æ‹©**ä¸å»ä½¿ç”¨T5æ¨¡å‹ä¼šå¯¹æ¨¡å‹å¯¹äºæ–‡æœ¬çš„ç†è§£èƒ½åŠ›æœ‰æ‰€é™ä½**ã€‚
![image.png](https://s2.loli.net/2025/09/01/RdQCOmXeMfwUYsh.webp)

> SD3ä½¿ç”¨T5-XXLæ¨¡å‹ã€‚è¿™ä½¿å¾—ä»¥å°‘äº24GBçš„VRAMåœ¨GPUä¸Šè¿è¡Œæ¨¡å‹ï¼Œå³ä½¿ä½¿ç”¨FP16ç²¾åº¦ã€‚å› æ­¤å¦‚æœéœ€è¦ä½¿ç”¨å°±éœ€è¦ï¼š1ã€å°†éƒ¨åˆ†æ¨¡å‹[ä¸‹æ”¾åˆ°CPUä¸Š](https://github.com/huggingface/diffusers/blob/0f252be0ed42006c125ef4429156cb13ae6c1d60/src/diffusers/pipelines/stable_diffusion_3/pipeline_stable_diffusion_3.py#L186)ï¼›2ã€ç›´æ¥å–æ¶ˆT5çš„ä½¿ç”¨ï¼ˆ`StableDiffusion3Pipeline.from_pretrained("stabilityai/stable-diffusion-3-medium-diffusers",text_encoder_3=None,tokenizer_3=None,torch_dtype=torch.float16)`ï¼‰ã€‚
> æ–‡æœ¬ç¼–ç è¿‡ç¨‹ï¼š1ã€CLIPç¼–ç åˆ†åˆ«å¾—åˆ°ï¼š[1, 77, 768]å’Œ[1, 77, 1280]ï¼›2ã€T5ç¼–ç å¾—åˆ°ï¼š[1, 256, 4096]ï¼›3ã€CLIPæ–‡æœ¬ç¼–ç æ‹¼æ¥ï¼š[1, 77, 2048]åœ¨å»å°†å…¶é€šè¿‡padå¡«å……åˆ°å’ŒT5ä¸€è‡´å¾—åˆ°æœ€åCLIPç¼–ç å™¨ç»´åº¦ä¸ºï¼š**[1, 77, 4096]**ï¼›4ã€æœ€åæ–‡æœ¬ç¼–ç ç»´åº¦ï¼š`[1, 333, 4096]`

**2ã€Flow Matchingæ¨¡å¼**ï¼ˆ[åŸç†](https://www.big-yellow-j.top/posts/2025/07/06/DFscheduler.html)ï¼‰ï¼›
**3ã€MM-Ditæ¨¡å‹æ¶æ„**ï¼ˆ[ä»£ç ](https://github.com/huggingface/diffusers/blob/d03240801f2ac2b4d1f49584c1c5628b98583f6a/src/diffusers/models/transformers/transformer_sd3.py#L80)ï¼‰ï¼šè§‚å¯Ÿä¸Šé¢è¿‡ç¨‹ï¼Œæ‰©æ•£æ¨¡å‹è¾“å…¥æ— éå°±æ˜¯3ä¸ªå†…å®¹ï¼š1ã€æ—¶é—´æ­¥ï¼ˆ$y$ï¼‰ï¼›2ã€åŠ å™ªå¤„ç†çš„å›¾åƒï¼ˆ$x$ï¼‰ï¼›3ã€æ–‡æœ¬ç¼–ç ï¼ˆ$c$ï¼‰ã€‚é¦–å…ˆå¯¹äº **æ—¶é—´æ­¥**è€Œè¨€å¤„ç†è¿‡ç¨‹ä¸ºï¼šç›´æ¥é€šè¿‡ Sinä½ç½®ç¼–ç ç„¶åå»å’ŒCLIPï¼ˆä¸¤ä¸ªåˆå¹¶çš„ï¼‰è¿›è¡Œç»„åˆå³å¯å¯¹äºå¦å¤–ä¸¤ä¸ªéƒ¨åˆ†ç›´æ¥é€šè¿‡[ä»£ç ](https://github.com/huggingface/diffusers/blob/d03240801f2ac2b4d1f49584c1c5628b98583f6a/src/diffusers/models/transformers/transformer_sd3.py#L80)è¿›è¡Œç†è§£ï¼š
```python
def forward(
    self,
    hidden_states: torch.Tensor, # åŠ å™ªå£°çš„å›¾ç‰‡ (batch size, channel, height, width)
    encoder_hidden_states: torch.Tensor = None, # æ¡ä»¶ç¼–ç æ¯”å¦‚è¯´ï¼šæ–‡æœ¬prompt (batch size, sequence_len, embed_dims)
    pooled_projections: torch.Tensor = None, # æ± åŒ–åçš„æ¡ä»¶ç¼–ç  (batch size, embed_dims)
    timestep: torch.LongTensor = None, # æ—¶é—´æ­¥ç¼–ç 
    block_controlnet_hidden_states: List = None,
    joint_attention_kwargs: Optional[Dict[str, Any]] = None,
    return_dict: bool = True,
    skip_layers: Optional[List[int]] = None,
) -> Union[torch.Tensor, Transformer2DModelOutput]:
    ...
    height, width = hidden_states.shape[-2:]
    # Step-1 
    hidden_states = self.pos_embed(hidden_states) # ç›´æ¥ä½¿ç”¨ 2Dçš„ä½ç½®ç¼–ç 
    temb = self.time_text_embed(timestep, pooled_projections)
    encoder_hidden_states = self.context_embedder(encoder_hidden_states) # ä¸€å±‚çº¿æ€§æ˜ å°„
    ...
    # Step-2
    for index_block, block in enumerate(self.transformer_blocks):
        is_skip = True if skip_layers is not None and index_block in skip_layers else False
        if torch.is_grad_enabled() and self.gradient_checkpointing and not is_skip:
            ...
        elif not is_skip:
            encoder_hidden_states, hidden_states = block(
                hidden_states=hidden_states,
                encoder_hidden_states=encoder_hidden_states,
                temb=temb,
                joint_attention_kwargs=joint_attention_kwargs,
            )
        ...
    # Step-3
    hidden_states = self.norm_out(hidden_states, temb)
    hidden_states = self.proj_out(hidden_states)
    patch_size = self.config.patch_size
    height = height // patch_size
    width = width // patch_size

    hidden_states = hidden_states.reshape(
        shape=(hidden_states.shape[0], height, width, patch_size, patch_size, self.out_channels)
    )
    hidden_states = torch.einsum("nhwpqc->nchpwq", hidden_states)
    output = hidden_states.reshape(
        shape=(hidden_states.shape[0], self.out_channels, height * patch_size, width * patch_size)
    )
    ...
    if not return_dict:
        return (output,)
    return Transformer2DModelOutput(sample=output)
```
**Step-1**ï¼šé¦–å…ˆå»å°†å›¾åƒ $x$ä½¿ç”¨2D æ­£å¼¦-ä½™å¼¦ä½ç½®ç¼–ç è¿›è¡Œå¤„ç†ï¼Œå¯¹äºæ—¶é—´æ­¥ç›´æ¥sinä½ç½®ç¼–ç ï¼Œå¯¹äºæ¡ä»¶ï¼ˆæ–‡æœ¬promptç­‰ï¼‰ç›´æ¥é€šè¿‡ä¸€å±‚çº¿æ€§ç¼–ç å¤„ç†ã€‚
**Step-2**ï¼šç„¶åå°±æ˜¯ç›´æ¥å»è®¡ç®—Attentionï¼š`encoder_hidden_states, hidden_states = block(hidden_states=hidden_states,encoder_hidden_states=encoder_hidden_states,temb=temb,joint_attention_kwargs=joint_attention_kwargs,)`ï¼Œå¯¹äºè¿™ä¸ª[block](https://github.com/huggingface/diffusers/blob/d03240801f2ac2b4d1f49584c1c5628b98583f6a/src/diffusers/models/attention.py#L570)çš„è®¾è®¡è¿‡ç¨‹ä¸ºï¼š
![](https://s2.loli.net/2025/09/01/gLkSbrt9vfyQ5uJ.webp)

```python
def forward(
    self,
    hidden_states: torch.FloatTensor,
    encoder_hidden_states: torch.FloatTensor,
    temb: torch.FloatTensor,
    joint_attention_kwargs: Optional[Dict[str, Any]] = None,
):
    joint_attention_kwargs = joint_attention_kwargs or {}
    # Aeetntion Step-1
    if self.use_dual_attention:
        norm_hidden_states, gate_msa, shift_mlp, scale_mlp, gate_mlp, norm_hidden_states2, gate_msa2 = self.norm1(
            hidden_states, emb=temb
        )
    else:
        ...

    if self.context_pre_only:
        ...
    else:
        norm_encoder_hidden_states, c_gate_msa, c_shift_mlp, c_scale_mlp, c_gate_mlp = self.norm1_context(
            encoder_hidden_states, emb=temb
        )
    # Attention Step-2
    attn_output, context_attn_output = self.attn(
        hidden_states=norm_hidden_states,
        encoder_hidden_states=norm_encoder_hidden_states,
        **joint_attention_kwargs,
    )
    attn_output = gate_msa.unsqueeze(1) * attn_output
    hidden_states = hidden_states + attn_output
    ...
    norm_hidden_states = self.norm2(hidden_states)
    norm_hidden_states = norm_hidden_states * (1 + scale_mlp[:, None]) + shift_mlp[:, None]
    if self._chunk_size is not None:
        ...
    else:
        ff_output = self.ff(norm_hidden_states)
    ff_output = gate_mlp.unsqueeze(1) * ff_output
    hidden_states = hidden_states + ff_output
    if self.context_pre_only:
        ...

    return encoder_hidden_states, hidden_states
```
è®¡ç®—æ³¨æ„åŠ›è¿‡ç¨‹ä¸­ï¼Œé¦–å…ˆ **Attention Step-1**ï¼šæ­£åˆ™åŒ–å¤„ç†ï¼ˆæ­£å¦‚ä¸Šé¢[Ditä¸­](https://www.big-yellow-j.top/posts/2025/07/06/DFBaseModel.html#:~:text=%E5%9C%A8layernorm%E4%B8%AD%E4%B8%80%E8%88%AC%E5%BD%92%E4%B8%80%E5%8C%96%E5%A4%84%E7%90%86%E6%96%B9%E5%BC%8F%E4%B8%BA)çš„ä¸€æ ·å°†æ¡ä»¶æ‹†åˆ†ä¸ºå‡ ä¸ªå‚æ•°ï¼Œè§‚å¯ŸSD3å›¾ä¸­çš„MMDitè®¾è®¡ï¼Œä¼šå°† **åŠ å™ªå£°å¤„ç†çš„å›¾ç‰‡** å’Œ **æ¡ä»¶ç¼–ç **éƒ½å»ï¼ˆå¤„ç†æ–¹å¼ç›¸åŒï¼‰é€šè¿‡ â€œæ­£åˆ™åŒ–â€ï¼Œåœ¨SD3ä¸­å¤„ç†æ–¹å¼ä¸ºï¼Œç›´æ¥`shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp, shift_msa2, scale_msa2, gate_msa2 = emb.chunk(9, dim=1)` æ‹†åˆ†ä¹‹åå»é€šè¿‡ `LayerNorm`å¤„ç†ä¹‹åå¾—åˆ° `norm_hidden_states` è€Œååœ¨å»è®¡ç®— `norm_hidden_states * (1 + scale_msa[:, None]) + shift_msa[:, None]`ï¼‰ç„¶ååé¢å¤„ç†è¿‡ç¨‹å°±æ¯”è¾ƒç®€å•å’Œä¸Šé¢çš„æµç¨‹å›¾æ˜¯ä¸€æ ·çš„ã€‚
è¿™æ ·ä¸€æ¥ä¸€ä¸ªMMDit blockå°±ä¼šè¿”å›ä¸¤éƒ¨åˆ†ç»“æœ `encoder_hidden_states`, `hidden_states`ï¼ˆåŒº**åˆ«Ditä¹‹é—´åœ¨äºï¼ŒMMDitæ˜¯å°†imageå’Œtextä¸¤ç§æ¨¡æ€ä¹‹é—´çš„ä¿¡æ¯è¿›è¡ŒèåˆäºŒDitåªæ˜¯ä½¿ç”¨åˆ°imgaeä¸€ç§æ¨¡æ€**ï¼‰
**Step-3**å°±æ¯”è¾ƒç®€å•å°±æ˜¯ä¸€äº›normç­‰å¤„ç†ã€‚
**æ€»çš„æ¥è¯´**MMDiT Block çš„è¾“å…¥ä¸»è¦æœ‰ä¸‰éƒ¨åˆ†ï¼š**æ—¶é—´æ­¥åµŒå…¥** $y$ï¼šé€šè¿‡ä¸€ä¸ª MLP æŠ•å½±ï¼Œå¾—åˆ°ä¸€ç»„å‚æ•°ï¼Œç”¨äºè°ƒèŠ‚ Block å†…çš„ LayerNorm / Attention / MLPï¼ˆç±»ä¼¼ FiLM conditioningï¼‰ã€‚**å›¾åƒ token** $x$ï¼šç”±åŠ å™ªå›¾åƒ latent patch embedding å¾—åˆ°ï¼Œå¹¶åŠ ä¸Š 2D æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç ã€‚**æ–‡æœ¬ token** $c$ï¼šæ¥è‡ªæ–‡æœ¬ç¼–ç å™¨çš„è¾“å‡ºï¼Œä¸€èˆ¬å¸¦æœ‰ 1D ä½ç½®ç¼–ç ã€‚**Block å†…éƒ¨æœºåˆ¶**ï¼šå°† $x$ å’Œ $c$ æ‹¼æ¥åœ¨ä¸€èµ·ï¼Œä½œä¸º Transformer çš„è¾“å…¥åºåˆ—ã€‚åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­ï¼Œ$x$ token èƒ½å’Œ $c$ token äº¤äº’ï¼Œä»è€Œå®ç° è·¨æ¨¡æ€èåˆã€‚$y$ï¼ˆtimestep embeddingï¼‰é€šè¿‡æŠ•å½±æä¾›é¢å¤–çš„æ¡ä»¶æ§åˆ¶ã€‚

> **2D æ­£å¼¦-ä½™å¼¦ä½ç½®ç¼–ç **
> ![](https://s2.loli.net/2025/09/01/lwZns5H9vTpeOgU.webp)
> å·¦ä¾§ä¸ºä¸€èˆ¬çš„ä½ç½®ç¼–ç æ–¹å¼ï¼Œä½†æ˜¯æœ‰ä¸€ä¸ªç¼ºç‚¹ï¼šç”Ÿæˆçš„å›¾åƒçš„åˆ†è¾¨ç‡æ˜¯æ— æ³•ä¿®æ”¹çš„ã€‚æ¯”å¦‚å¯¹äºä¸Šå›¾ï¼Œå‡å¦‚é‡‡æ ·æ—¶è¾“å…¥å¤§å°ä¸æ˜¯4x3ï¼Œè€Œæ˜¯4x5ï¼Œé‚£ä¹ˆ0å·å›¾å—çš„ä¸‹é¢å°±æ˜¯5è€Œä¸æ˜¯4äº†ï¼Œæ¨¡å‹è®­ç»ƒæ—¶å­¦ä¹ åˆ°çš„å›¾å—ä¹‹é—´çš„ä½ç½®å…³ç³»å…¨éƒ¨ä¹±å¥—ï¼Œå› æ­¤å°±é€šè¿‡2Dä½ç½®å»ä»£è¡¨æ¯ä¸€å—çš„ä½ç½®ä¿¡æ¯ã€‚

* FLUXæ¨¡å‹è€Œè¨€å…¶ç»“æ„å¦‚ä¸‹

![](https://s2.loli.net/2025/09/01/WTD97u3eFiQwr4d.webp)

åŒºåˆ«SD3æ¨¡å‹åœ¨äºï¼ŒFLUX.1åœ¨æ–‡æœ¬ç¼–ç å™¨é€‰æ‹©ä¸Š**åªä½¿ç”¨äº†2ä¸ªç¼–ç å™¨**ï¼ˆCLIPTextModelã€T5EncoderModelï¼‰å¹¶ä¸”FLUX.1 VAEæ¶æ„ä¾ç„¶ç»§æ‰¿äº†SD 3 VAEçš„**8å€ä¸‹é‡‡æ ·å’Œè¾“å…¥é€šé“æ•°ï¼ˆ16ï¼‰**ã€‚åœ¨FLUX.1 VAEè¾“å‡ºLatentç‰¹å¾ï¼Œå¹¶åœ¨Latentç‰¹å¾è¾“å…¥æ‰©æ•£æ¨¡å‹å‰ï¼Œè¿˜è¿›è¡Œäº† `_pack_latents`æ“ä½œï¼Œä¸€ä¸‹å­å°†Latent**ç‰¹å¾é€šé“æ•°æé«˜åˆ°64ï¼ˆ16 -> 64ï¼‰**ï¼Œæ¢å¥è¯è¯´ï¼ŒFLUX.1ç³»åˆ—çš„æ‰©æ•£æ¨¡å‹éƒ¨åˆ†è¾“å…¥é€šé“æ•°ä¸º64ï¼Œæ˜¯SD 3çš„å››å€ã€‚å¯¹äº `_pack_latents`åšæ³•æ˜¯ä¼šå°†ä¸€ä¸ª $2\times 2$çš„åƒç´ å»è¡¥å……åˆ°é€šé“ä¸­ã€‚
```python
def _pack_latents(latents, batch_size, num_channels_latents, height, width):
    latents = latents.view(batch_size, num_channels_latents, height // 2, 2, width // 2, 2)
    latents = latents.permute(0, 2, 4, 1, 3, 5)
    latents = latents.reshape(batch_size, (height // 2) * (width // 2), num_channels_latents * 4)
    return latents
```
é™¤å»æ”¹å˜textçš„ç¼–ç å™¨æ•°é‡ä»¥åŠVAEçš„é€šé“æ•°é‡ä¹‹å¤–ï¼ŒFLUX.1è¿˜åšäº†å¦‚ä¸‹çš„æ”¹è¿›ï¼šFLUX.1 æ²¡æœ‰åš Classifier-Free Guidance (CFG)ï¼ˆå¯¹äºCFGä¸€èˆ¬åšæ³•å°±æ˜¯ç›´æ¥å»å°†â€œVAEå‹ç¼©çš„å›¾åƒä¿¡æ¯å˜é‡å¤åˆ¶ä¸¤å€â€ `torch.cat([latents] * 2)`ï¼Œæ–‡æœ¬å°±æ˜¯ç›´æ¥å°†negative_promptçš„ç¼–ç è¡¥å……åˆ°æ–‡æœ¬ç¼–ç ä¸­ `torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)`ï¼‰è€Œæ˜¯æŠŠæŒ‡å¼•å¼ºåº¦ guidance å½“æˆäº†ä¸€ä¸ªå’Œæ—¶åˆ» t ä¸€æ ·çš„çº¦æŸä¿¡æ¯ï¼Œä¼ å…¥å»å™ªæ¨¡å‹ transformer ä¸­ã€‚åœ¨transformeræ¨¡å‹ç»“æ„è®¾è®¡ä¸­ï¼ŒSD3æ˜¯**ç›´æ¥å¯¹å›¾åƒåšå›¾å—åŒ–ï¼Œå†è®¾ç½®2Dä½ç½®ç¼–ç ** `PatchEmbed`ï¼Œåœ¨FLUX.1ä¸­ä½¿ç”¨çš„æ˜¯`FluxPosEmbed`ï¼ˆæ—‹è½¬ä½ç½®ç¼–ç ï¼‰
```python
# SD3
self.pos_embed = PatchEmbed(height=sample_size,width=sample_size,patch_size=patch_size,in_channels=in_channels,)
embed_dim=self.inner_dim,pos_embed_max_size=pos_embed_max_size,  # hard-code for now.)
# FLUX.1
self.pos_embed = FluxPosEmbed(theta=10000, axes_dim=axes_dims_rope)
```

### VAEåŸºåº§æ¨¡å‹
å¯¹äºVAEæ¨¡å‹åœ¨ä¹‹å‰çš„[åšå®¢](https://www.big-yellow-j.top/posts/2025/05/11/VAE.html)æœ‰ä»‹ç»è¿‡å…·ä½“çš„åŸç†ï¼Œè¿™é‡Œä¸»è¦å°±æ˜¯ä»‹ç»å‡ ä¸ªå¸¸è§çš„VAEæ¶æ„æ¨¡å‹ï¼ˆä½¿ç”¨è¿‡ç¨‹ä¸­å…¶å®å¾ˆå°‘ä¼šå»ä¿®æ”¹VAEæ¶æ„ï¼Œä¸€èˆ¬éƒ½æ˜¯ç›´æ¥ç”¨SDè‡ªå·±ä½¿ç”¨çš„ï¼‰æ‰€ä»¥å°±ç®€å•å¯¹æ¯”ä¸€ä¸‹ä¸åŒçš„VAEæ¨¡å‹åœ¨å›¾ç‰‡é‡æ„ä¸Šçš„è¡¨ï¼Œä¸»è¦æ˜¯ä½¿ç”¨æ­¤[huggingface](https://huggingface.co/spaces/rizavelioglu/vae-comparison)ä¸Šçš„è¿›è¡Œæ¯”è¾ƒï¼ˆæ¯”è¾ƒçš„æ•°å€¼è¶Šå°è¶Šå¥½ï¼Œå°±æ•°å€¼è€Œè¨€ **CogView4-6B**æ•ˆæœæœ€ä½³ï¼‰ï¼Œä¸‹é¢ç»“æœä¸ºéšä¾¿æŒ‘é€‰çš„ä¸€ä¸ªå›¾ç‰‡è¿›è¡Œæµ‹è¯•ç»“æœï¼š

| æ¨¡å‹åç§°                   | æ•°å€¼   | æ—¶é—´(s)  |
|----------------------------|--------|----------|
| stable-diffusion-v1-4      | 2,059  | 0.5908   |
| eq-vae-ema                 | 1,659  | 0.0831   |
| eq-sdxl-vae                | 1,200  | 0.0102   |
| sd-vae-ft-mse              | 1,204  | 0.0101   |
| sdxl-vae                   |   929  | 0.0105   |
| playground-v2.5            |   925  | 0.0096   |
| stable-diffusion-3-medium  |    24  | 0.1027   |
| FLUX.1                     |    18  | 0.0412   |
| **CogView4-6B**            | **0**  | **0.1265**  |
| FLUX.1-Kontext             |    18  | 0.0098   |

### GANåŸºåº§æ¨¡å‹
- [ ] 1ã€ä»‹ç»å®ŒæˆLaMaæ¨¡å‹åŸºæœ¬ç»“æ„ä»¥åŠåŸºæœ¬ä½¿ç”¨æ–¹å¼
- [ ] 2ã€å°†LaMaå®˜æ–¹çš„æ¶æ„ç»ç’ƒå‡ºæ¥æ–¹ä¾¿ä½¿ç”¨

GANæ¨¡å‹ä¸ªäººåœ¨ä½¿ç”¨ä¸Šç”¨çš„ä¸æ˜¯ç‰¹åˆ«å¤šï¼Œå› æ­¤ä¸»è¦ä»‹ç»ä¸ªäººåœ¨å®é™…ä½¿ç”¨è¿‡ç¨‹ä¸­å¯èƒ½è§åˆ°æ¯”è¾ƒå¤šçš„GANæ¨¡å‹ã€‚lama[^13]æ¨¡å‹ã€StyleGAN1-3æ¨¡å‹
![image.png](https://s2.loli.net/2025/09/01/NQFt5gsrO4pJiS7.webp)

### Qwen image
> å®˜æ–¹blogï¼š[https://qwenlm.github.io/zh/blog/qwen-image/](https://qwenlm.github.io/zh/blog/qwen-image/)
> Qwen Imageå›¾ç‰‡ç¼–è¾‘ï¼š[https://huggingface.co/Qwen/Qwen-Image-Edit](https://huggingface.co/Qwen/Qwen-Image-Edit)
> Qwen Imageï¼š[https://huggingface.co/Qwen/Qwen-Image](https://huggingface.co/Qwen/Qwen-Image)
> Qwen Image Loraå¾®è°ƒ8æ­¥ç”Ÿå›¾ï¼š[https://huggingface.co/lightx2v/Qwen-Image-Lightning](https://huggingface.co/lightx2v/Qwen-Image-Lightning)
> Qwen Imageå›¾ç‰‡ç¼–è¾‘int4é‡åŒ–ç‰ˆæœ¬ï¼š[https://huggingface.co/nunchaku-tech/nunchaku-qwen-image](https://huggingface.co/nunchaku-tech/nunchaku-qwen-image)ï¼Œ[ä»£ç ](https://github.com/nunchaku-tech/nunchaku/blob/main/examples/v1/qwen-image.py)

Qwen image[^18]æ— è®ºæ˜¯å¤šè¡Œæ–‡å­—ã€æ®µè½å¸ƒå±€ï¼Œè¿˜æ˜¯ä¸­è‹±æ–‡ç­‰ä¸åŒè¯­ç§ï¼ŒQwen-Imageéƒ½èƒ½ä»¥æé«˜çš„ä¿çœŸåº¦è¿›è¡Œæ¸²æŸ“ï¼Œå°¤å…¶åœ¨å¤„ç†å¤æ‚çš„ä¸­æ–‡ï¼ˆlogographic languagesï¼‰æ–¹é¢ï¼Œè¡¨ç°è¿œè¶…ç°æœ‰æ¨¡å‹ï¼ˆä¸è¿‡ç›®å‰ï¼š2025.08.29æ¨¡å‹å…¨æƒé‡åŠ è½½çš„è¯ä¸€èˆ¬è®¾å¤‡å¾ˆéš¾ä½¿ç”¨ï¼Œä¸è¿‡åˆé‡åŒ–ç‰ˆæœ¬å¯ä»¥å°è¯•ï¼‰æ¨¡å‹æ•´ä½“ç»“æ„ï¼š
![](https://s2.loli.net/2025/09/01/U7HqQcJxZ96SN3A.webp)
æ•´ä½“æ¡†æ¶ä¸Šè¿˜æ˜¯MMDitç»“æ„å’Œä¸Šé¢çš„SD3éƒ½æ˜¯ä¸€è‡´çš„ï¼Œä¸è¿‡æ¨¡å‹çš„æ”¹è¿›åœ¨äºï¼š1ã€åŒºåˆ«ä¹‹å‰çš„éƒ½æ˜¯ä½¿ç”¨CLIPæ¨¡å‹å»å¯¹é½å›¾ç‰‡-æ–‡æœ¬ä¹‹é—´ä¿¡æ¯ï¼Œåœ¨Qwen Imageä¸­åˆ™æ˜¯ç›´æ¥ä½¿ç”¨**Qwen2.5-VL**ï¼›2ã€å¯¹äºVAEæ¨¡å‹åˆ™æ˜¯ç›´æ¥ä½¿ç”¨**Wan-2.1-VAE**ï¼ˆä¸è¿‡é€‰æ‹©å†»ç»“encoderéƒ¨åˆ†åªå»è®­ç»ƒdecoderéƒ¨åˆ†ï¼‰ï¼›3ã€æ¨¡å‹çš„ç»“æ„è¿˜æ˜¯ä½¿ç”¨MMDitç»“æ„ï¼ŒçŸ¥è¯†å°†ä½ç½®ç¼–ç æ–¹å¼æ”¹ä¸º**Multimodal Scalable RoPE (MSRoPE)**ï¼Œä½ç½®ç¼–ç æ–¹å¼
![](https://s2.loli.net/2025/09/01/QuEY2gWZFzUMlCK.webp)
å¤§è‡´æ¡†æ¶äº†è§£ä¹‹åç»†çœ‹ä»–çš„æ•°æ®æ˜¯å¦‚ä½•æ”¶é›†çš„ä»¥åŠåå¤„ç†çš„ï¼š
![](https://s2.loli.net/2025/09/01/nzrOe2yaBpL5iwF.webp)
å¯¹äºæ”¶é›†åˆ°æ•°æ®ä¹‹åï¼Œè®ºæ–‡é‡Œé¢é€šè¿‡å¦‚ä¸‹æ“ä½œè¿›è¡Œåå¤„ç†ï¼š**1ã€é˜¶æ®µä¸€è¿‡æ»¤æ•°æ®**ï¼šæ¨¡å‹é¢„è®­ç»ƒæ˜¯åœ¨256x256çš„å›¾ç‰‡ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œå› æ­¤ï¼Œè¿‡æ»¤æ‰256x256ä»¥å¤–çš„å›¾ç‰‡è¿˜æœ‰ä¸€äº›ä½è´¨é‡å›¾ç‰‡ç­‰ï¼›**2ã€é˜¶æ®µäºŒå›¾ç‰‡è´¨é‡å¼ºåŒ–**ï¼šä¸»è¦è¿˜æ˜¯è¿‡æ»¤ä¸€äº›ä½è´¨é‡å›¾ç‰‡å¦‚äº®åº¦çº¹ç†ç­‰ï¼›

### ä¸åŒæ¨¡å‹å‚æ•°å¯¹ç”Ÿæˆçš„å½±å“
> https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20

* å‚æ•°`guidance_rescale`å¯¹äºç”Ÿæˆçš„å½±å“

å¼•å¯¼æ‰©æ•£æ¨¡å‹ï¼ˆå¦‚ Classifier-Free Guidanceï¼ŒCFGï¼‰ä¸­ï¼Œç”¨äºè°ƒæ•´æ–‡æœ¬æ¡ä»¶å¯¹ç”Ÿæˆå›¾åƒçš„å½±å“å¼ºåº¦ã€‚å®ƒçš„æ ¸å¿ƒä½œç”¨æ˜¯æ§åˆ¶æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¯¹æ–‡æœ¬æç¤ºçš„â€œæœä»ç¨‹åº¦â€ã€‚å…¬å¼ä¸Šï¼ŒCFG è°ƒæ•´é¢„æµ‹å™ªå£°çš„æ–¹å¼å¦‚ä¸‹ï¼š

$$
\epsilon = \epsilon_{\text{uncond}} + \text{guidance\_scale} \cdot (\epsilon_{\text{cond}} - \epsilon_{\text{uncond}})
$$

å…¶ä¸­ï¼š1ã€$\epsilon_{\text{cond}}$ï¼šåŸºäºæ–‡æœ¬æ¡ä»¶é¢„æµ‹çš„å™ªå£°ã€‚2ã€$\epsilon_{\text{uncond}}$ï¼šæ— æ¡ä»¶ï¼ˆæ— æ–‡æœ¬æç¤ºï¼‰é¢„æµ‹çš„å™ªå£°ã€‚3ã€guidance_scaleï¼šå†³å®šæ¡ä»¶å™ªå£°ç›¸å¯¹äºæ— æ¡ä»¶å™ªå£°çš„æƒé‡ã€‚å¾—åˆ°æœ€åæµ‹è¯•ç»“æœå¦‚ä¸‹ï¼ˆå‚æ•°åˆ†åˆ«ä¸º[1.0, 3.0, 5.0, 7.5, 10.0, 15.0, 20.0]ï¼Œ`prompt = "A majestic lion standing on a mountain during golden hour, ultra-realistic, 8k"`ï¼Œ `negative_prompt = "blurry, distorted, low quality"`ï¼‰ï¼Œå®¹æ˜“å‘ç°æ•°å€¼è¶Šå¤§æ–‡æœ¬å¯¹äºå›¾åƒçš„å½±å“ä¹Ÿå°±è¶Šå¤§ã€‚
![tmp-CFG.png](https://s2.loli.net/2025/08/06/2jk18UISnqKdPZf.webp)
å…¶ä¸­ä»£ç å…·ä½“æ“ä½œå¦‚ä¸‹ï¼Œä»ä»£ç ä¹Ÿå¾ˆå®¹æ˜“å‘ç°ä¸Šé¢è®¡ç®—å…¬å¼ä¸­çš„ uncondä»£è¡¨çš„å°±æ˜¯æˆ‘çš„negative_promptï¼Œä¹Ÿå°±æ˜¯è¯´**CFGåšçš„å°±æ˜¯negative_promptå¯¹ç”Ÿæˆçš„å½±å“**ï¼š
```python
if self.do_classifier_free_guidance:
    prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)
    add_text_embeds = torch.cat([negative_pooled_prompt_embeds, add_text_embeds], dim=0)
    add_neg_time_ids = add_neg_time_ids.repeat(batch_size * num_images_per_prompt, 1)
    add_time_ids = torch.cat([add_neg_time_ids, add_time_ids], dim=0)
prompt_embeds = prompt_embeds.to(device)
```

## Adapters
> https://huggingface.co/docs/diffusers/tutorials/using_peft_for_inference

æ­¤ç±»æ–¹æ³•æ˜¯åœ¨å®Œå¤‡çš„ DF æƒé‡åŸºç¡€ä¸Šï¼Œé¢å¤–æ·»åŠ ä¸€ä¸ªâ€œæ’ä»¶â€ï¼Œä¿æŒåŸæœ‰æƒé‡ä¸å˜ã€‚æˆ‘åªéœ€ä¿®æ”¹è¿™ä¸ªæ’ä»¶ï¼Œå°±å¯ä»¥è®©æ¨¡å‹ç”Ÿæˆä¸åŒé£æ ¼çš„å›¾åƒã€‚å¯ä»¥ç†è§£ä¸ºåœ¨åŸå§‹æ¨¡å‹ä¹‹å¤–æ–°å¢ä¸€ä¸ªâ€œç”Ÿæˆæ¡ä»¶â€ï¼Œé€šè¿‡ä¿®æ”¹è¿™ä¸€æ¡ä»¶å³å¯çµæ´»æ§åˆ¶æ¨¡å‹ç”Ÿæˆå„ç§é£æ ¼æˆ–æ»¡è¶³ä¸åŒéœ€æ±‚çš„å›¾åƒã€‚

### ControlNet
> https://github.com/lllyasviel/ControlNet
> å»ºè®®ç›´æ¥é˜…è¯»ï¼š[https://github.com/lllyasviel/ControlNet/discussions/categories/announcements](https://github.com/lllyasviel/ControlNet/discussions/categories/announcements) æ¥äº†è§£æ›´åŠ å¤šç»†èŠ‚

![](https://s2.loli.net/2025/07/09/Tfji2LMv15tgr6d.webp)

ControlNet[^2]çš„å¤„ç†æ€è·¯å°±å¾ˆç®€å•ï¼Œå†å·¦å›¾ä¸­æ¨¡å‹çš„å¤„ç†è¿‡ç¨‹å°±æ˜¯ç›´æ¥é€šè¿‡ï¼š$y=f(x;\theta)$æ¥ç”Ÿæˆå›¾åƒï¼Œä½†æ˜¯åœ¨ControlNeté‡Œé¢ä¼š **å°†æˆ‘ä»¬æœ€å¼€å§‹çš„ç½‘ç»œç»“æ„å¤åˆ¶** ç„¶åé€šè¿‡åœ¨å…¶å‰åå¼•å…¥ä¸€ä¸ª **zero-convolution** å±‚æ¥â€œæŒ‡å¯¼â€ï¼ˆ $Z$ ï¼‰æ¨¡å‹çš„è¾“å‡ºä¹Ÿå°±æ˜¯è¯´å°†ä¸Šé¢çš„ç”Ÿæˆè¿‡ç¨‹å˜ä¸ºï¼š$y=f(x;\theta)+Z(f(x+Z(c;\theta_{z_1});\theta);\theta_{Z_2})$ã€‚é€šè¿‡å†»ç»“æœ€åˆçš„æ¨¡å‹çš„æƒé‡ä¿æŒä¸å˜ï¼Œä¿ç•™äº†Stable Diffusionæ¨¡å‹åŸæœ¬çš„èƒ½åŠ›ï¼›ä¸æ­¤åŒæ—¶ï¼Œä½¿ç”¨é¢å¤–æ•°æ®å¯¹â€œå¯è®­ç»ƒâ€å‰¯æœ¬è¿›è¡Œå¾®è°ƒï¼Œå­¦ä¹ æˆ‘ä»¬æƒ³è¦æ·»åŠ çš„æ¡ä»¶ã€‚å› æ­¤åœ¨æœ€åæˆ‘ä»¬çš„SDæ¨¡å‹ä¸­å°±æ˜¯å¦‚ä¸‹ä¸€ä¸ªç»“æ„ï¼š

![](https://s2.loli.net/2025/07/09/uVNAEnleRMJ6p4v.webp)

åœ¨è®ºæ–‡é‡Œé¢ä½œè€…ç»™å‡ºä¸€ä¸ªå®é™…çš„æµ‹è¯•æ•ˆæœå¯ä»¥å¾ˆå®¹æ˜“ç†è§£é‡Œé¢æ¡ä»¶cï¼ˆæ¡ä»¶ ğ‘å°±æ˜¯æä¾›ç»™æ¨¡å‹çš„æ˜¾å¼ç»“æ„å¼•å¯¼ä¿¡æ¯ï¼Œ**ç”¨äºåœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ç²¾ç¡®æ§åˆ¶å›¾åƒçš„ç©ºé—´ç»“æ„æˆ–å¸ƒå±€**ï¼Œä¸€èˆ¬æ¥è¯´å¯ä»¥æ˜¯è‰å›¾ã€åˆ†å‰²å›¾ç­‰ï¼‰åˆ°åº•æ˜¯ä¸€ä¸ªä»€ä¹ˆä¸œè¥¿ï¼Œæ¯”å¦‚è¯´å°±æ˜¯ç›´æ¥ç»™å‡ºä¸€ä¸ªâ€œçº¿ç¨¿â€ç„¶åæ¨¡å‹æ¥è¾“å‡ºå›¾åƒã€‚

![](https://s2.loli.net/2025/07/09/rkWH3o1MOaNs6pg.webp)

> **è¡¥å……-1**ï¼šä¸ºä»€ä¹ˆä½¿ç”¨ä¸Šé¢è¿™ç§ç»“æ„
> åœ¨[github](https://github.com/lllyasviel/ControlNet/discussions/188)ä¸Šä½œè€…è®¨è®ºäº†ä¸ºä»€ä¹ˆè¦ä½¿ç”¨ä¸Šé¢è¿™ç§ç»“æ„è€Œéç›´æ¥ä½¿ç”¨mlpç­‰ï¼ˆä½œè€…ç»™å‡ºäº†å¾ˆå¤šæµ‹è¯•å›¾åƒï¼‰ï¼Œæœ€åæ€»ç»“å°±æ˜¯ï¼š**è¿™ç§ç»“æ„å¥½**
> **è¡¥å……-2**ï¼šä½¿ç”¨0å·ç§¯å±‚ä¼šä¸ä¼šå¯¼è‡´æ¨¡å‹æ— æ³•ä¼˜åŒ–é—®é¢˜ï¼Ÿ
> ä¸ä¼šï¼Œå› ä¸ºå¯¹äºç¥ç»ç½‘ç»œç»“æ„å¤§å¤šéƒ½æ˜¯ï¼š$y=wx+b$è®¡ç®—æ¢¯åº¦è¿‡ç¨‹ä¸­å³ä½¿ $w=0$ä½†æ˜¯é‡Œé¢çš„ $xâ‰ 0$æ¨¡å‹çš„å‚æ•°è¿˜æ˜¯å¯ä»¥è¢«ä¼˜åŒ–çš„

#### ControlNetä»£ç æ“ä½œ
> Code: [https://github.com/shangxiaaabb/ProjectCode/tree/main/code/Python/DFModelCode/training_controlnet](https://github.com/shangxiaaabb/ProjectCode/tree/main/code/Python/DFModelCode/training_controlnet)
> æ¨¡å‹æƒé‡ï¼š

**é¦–å…ˆ**ï¼Œç®€å•äº†è§£ä¸€ä¸ªControlNetæ•°æ®é›†æ ¼å¼ï¼Œä¸€èˆ¬æ¥è¯´æ•°æ®ä¸»è¦æ˜¯ä¸‰éƒ¨åˆ†ç»„æˆï¼š1ã€imageï¼ˆå¯ä»¥ç†è§£ä¸ºç”Ÿæˆçš„å›¾åƒï¼‰ï¼›2ã€condiction_imageï¼ˆå¯ä»¥ç†è§£ä¸ºè¾“å…¥ControlNeté‡Œé¢çš„æ¡ä»¶ $c$ï¼‰ï¼›3ã€textã€‚æ¯”å¦‚è¯´ä»¥[raulc0399/open_pose_controlnet](https://huggingface.co/datasets/raulc0399/open_pose_controlnet)ä¸ºä¾‹
![](https://s2.loli.net/2025/07/12/nphNm3OIebFGazr.webp)

**æ¨¡å‹åŠ è½½**ï¼Œä¸€èˆ¬æ¥è¯´æ‰©æ•£æ¨¡å‹å°±åªéœ€è¦åŠ è½½å¦‚ä¸‹å‡ ä¸ªï¼š`DDPMScheduler`ã€`AutoencoderKL`ï¼ˆvaeæ¨¡å‹ï¼‰ã€`UNet2DConditionModel`ï¼ˆä¸ä¸€å®šåŠ è½½æ¡ä»¶Unetæ¨¡å‹ï¼‰ï¼Œé™¤æ­¤ä¹‹å¤–åœ¨ControlNetä¸­è¿˜éœ€è¦åŠ è½½ä¸€ä¸ª`ControlNetModel`ã€‚å¯¹äº`ControlNetModel`ä¸­ä»£ç å¤§è‡´ç»“æ„ä¸ºï¼Œä»£ç ä¸­é€šè¿‡`self.controlnet_down_blocks`æ¥å­˜å‚¨ControlNetçš„ä¸‹é‡‡æ ·æ¨¡å—ï¼ˆ**åˆå§‹åŒ–ä¸º0çš„å·ç§¯å±‚**ï¼‰ã€‚`self.down_blocks`ç”¨æ¥å­˜å‚¨ControlNetä¸­å¤åˆ¶çš„Unetçš„ä¸‹é‡‡æ ·å±‚ã€‚åœ¨`forward`ä¸­å¯¹äºè¾“å…¥çš„æ ·æœ¬ï¼ˆ`sample`ï¼‰é¦–å…ˆé€šè¿‡ `self.down_blocks`é€å±‚å¤„ç†å åŠ åˆ° `down_block_res_samples`ä¸­ï¼Œè€Œåå°±æ˜¯ç›´æ¥å°†å¾—åˆ°ç»“æœå†å»é€šè¿‡ `self.controlnet_down_blocks`æ¯å±‚è¿›è¡Œå¤„ç†ï¼Œæœ€åè¿”å›ä¸‹é‡‡æ ·çš„æ¯å±‚ç»“æœä»¥åŠä¸­é—´å±‚å¤„ç†ç»“æœï¼š`down_block_res_samples`ï¼Œ`mid_block_res_sample`

```python
class ControlNetModel(ModelMixin, ConfigMixin, FromOriginalModelMixin):
    @register_to_config
    def __init__(...):
        ...
        self.down_blocks = nn.ModuleList([])
        self.controlnet_down_blocks = nn.ModuleList([])
        # å°è£…ä¸‹é‡‡æ ·è¿‡ç¨‹ï¼ˆå¯¹åº”ä¸Šé¢æ¨¡å‹å³ä¾§ç»“æ„ï¼‰
        controlnet_block = nn.Conv2d(output_channel, output_channel, kernel_size=1)
        controlnet_block = zero_module(controlnet_block)
        self.controlnet_down_blocks.append(controlnet_block)
        for i, down_block_type in enumerate(down_block_types):
            # down_block_typeså°±æ˜¯Uneté‡Œé¢ä¸‹é‡‡æ ·çš„æ¯ä¸€ä¸ªæ¨¡å—æ¯”å¦‚è¯´ï¼šCrossAttnDownBlock2D
            ...
            down_block = get_down_block(down_block_type) # é€šè¿‡ get_down_block è·å–uetä¸‹é‡‡æ ·çš„æ¨¡å—
            self.down_blocks.append(down_block)
            for _ in range(layers_per_block):
                controlnet_block = nn.Conv2d(output_channel, output_channel, kernel_size=1)
                controlnet_block = zero_module(controlnet_block)
                self.controlnet_down_blocks.append(controlnet_block)
    @classmethod
    def from_unet(cls, unet,...):
        ...
        # é€šè¿‡clså®ä¾‹åŒ–çš„ç±»æœ¬èº«ControlNetModel
        controlnet = cls(...)
        if load_weights_from_unet:
            # å°†å„ç±»æƒé‡åŠ è½½åˆ° controlnet ä¸­
            controlnet.conv_in.load_state_dict(unet.conv_in.state_dict())
            controlnet.time_proj.load_state_dict(unet.time_proj.state_dict())
            ...

        return controlnet
    def forward(...):
        ...
        # æ—¶é—´ç¼–ç 
        t_emb = self.time_proj(timesteps)
        emb = self.time_embedding(t_emb, timestep_cond)
        if self.class_embedding is not None:
            ...
            class_emb = self.class_embedding(class_labels).to(dtype=self.dtype)
            emb = emb + class_emb
        # å¯¹æ¡ä»¶è¿›è¡Œç¼–ç 
        if self.config.addition_embed_type is not None:
            if self.config.addition_embed_type == "text":
                aug_emb = self.add_embedding(encoder_hidden_states)
            elif self.config.addition_embed_type == "text_time":
                time_ids = added_cond_kwargs.get("time_ids")
                time_embeds = self.add_time_proj(time_ids.flatten())
                time_embeds = time_embeds.reshape((text_embeds.shape[0], -1))

                add_embeds = torch.concat([text_embeds, time_embeds], dim=-1)
                add_embeds = add_embeds.to(emb.dtype)
                aug_emb = self.add_embedding(add_embeds)
        emb = emb + aug_emb if aug_emb is not None else emb       

        sample = self.conv_in(sample)
        controlnet_cond = self.controlnet_cond_embedding(controlnet_cond)
        sample = sample + controlnet_cond

        # ä¸‹é‡‡æ ·å¤„ç†
        down_block_res_samples = (sample,)
        for downsample_block in self.down_blocks:
            if ...
                ...
            else:
                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)
            down_block_res_samples += res_samples
        # ä¸­é—´å±‚å¤„ç†
        ...
        # å°†è¾“å‡ºåçš„å†…å®¹å»å’Œ0å·ç§¯è¿›è¡Œå åŠ 
        controlnet_down_block_res_samples = ()
        for down_block_res_sample, controlnet_block in zip(down_block_res_samples, self.controlnet_down_blocks):
            down_block_res_sample = controlnet_block(down_block_res_sample)
            controlnet_down_block_res_samples = controlnet_down_block_res_samples + (down_block_res_sample,)
        ...
        if not return_dict:
            return (down_block_res_samples, mid_block_res_sample)
        ...
```

**æ¨¡å‹è®­ç»ƒ**ï¼Œè®­ç»ƒè¿‡ç¨‹å’ŒDFè®­ç»ƒå·®å¼‚ä¸å¤§ã€‚å°†å›¾åƒé€šè¿‡VAEå¤„ç†ã€äº§ç”Ÿå™ªå£°ã€æ—¶é—´æ­¥ã€å°†å™ªå£°æ·»åŠ åˆ°ï¼ˆVAEå¤„ç†ä¹‹åçš„ï¼‰å›¾åƒä¸­ï¼Œè€Œåé€šè¿‡ `controlnet`å¾—åˆ°æ¯å±‚ä¸‹é‡‡æ ·çš„ç»“æœä»¥åŠä¸­é—´å±‚ç»“æœï¼š`down_block_res_samples, mid_block_res_sample = controlnet(...)`è€Œåå°†è¿™ä¸¤éƒ¨åˆ†ç»“æœå†å»é€šè¿‡unetå¤„ç†
```python
model_pred = unet(
    noisy_latents,
    timesteps,
    encoder_hidden_states=encoder_hidden_states,
    down_block_additional_residuals=[
        sample.to(dtype=weight_dtype) for sample in down_block_res_samples
    ],
    mid_block_additional_residual=mid_block_res_sample.to(dtype=weight_dtype),
    return_dict=False,
)[0]
```

åç»­å°±æ˜¯è®¡ç®—lossç­‰å¤„ç†

**æ¨¡å‹éªŒè¯**ï¼Œç›´æ¥å°±æ˜¯ä½¿ç”¨`StableDiffusionControlNetPipeline`æ¥å¤„ç†äº†ã€‚æœ€åéšæœºæµ‹è¯•çš„éƒ¨åˆ†ä¾‹å­ï¼ˆcontrolnetå¾®è°ƒæ•ˆæœä¸æ˜¯å¾ˆå¥½ï¼‰ï¼š
![output.jpg](https://s2.loli.net/2025/07/22/SNfEiTVXpeZgOIP.webp)

### T2I-Adapter
> https://github.com/TencentARC/T2I-Adapter

![image.png](https://s2.loli.net/2025/07/09/gZLDtFSGr25kCwa.webp)

T2I[^3]çš„å¤„ç†æ€è·¯ä¹Ÿæ¯”è¾ƒç®€å•ï¼ˆT2I-Adap 4 ter Detailsé‡Œé¢å…¶å®å°±å†™çš„å¾ˆæ˜ç™½äº†ï¼‰ï¼Œå¯¹äºè¾“å…¥çš„æ¡ä»¶å›¾ç‰‡ï¼ˆæ¯”å¦‚è¯´è¾¹ç¼˜å›¾åƒï¼‰:512x512ï¼Œé¦–å…ˆé€šè¿‡ pixel unshuffleè¿›è¡Œä¸‹é‡‡æ ·å°†å›¾åƒåˆ†è¾¨ç‡æ”¹ä¸ºï¼š64x64è€Œåé€šè¿‡ä¸€å±‚å·ç§¯+ä¸¤å±‚æ®‹å·®è¿æ¥ï¼Œè¾“å‡ºå¾—åˆ°ç‰¹å¾ $F_c$ä¹‹åå°†å…¶ä¸å¯¹åº”çš„encoderç»“æ„è¿›è¡Œç›¸åŠ ï¼š$F_{enc}+ F_c$ï¼Œå½“ç„¶T2Iä¹Ÿæ”¯æŒå¤šä¸ªæ¡ä»¶ï¼ˆç›´æ¥é€šè¿‡åŠ æƒç»„åˆå°±è¡Œï¼‰

### DreamBooth
> https://huggingface.co/docs/diffusers/v0.34.0/using-diffusers/dreambooth

DreamBooth é’ˆå¯¹çš„ä½¿ç”¨åœºæ™¯æ˜¯ï¼ŒæœŸæœ›ç”ŸæˆåŒä¸€ä¸ªä¸»ä½“çš„å¤šå¼ ä¸åŒå›¾åƒï¼Œ å°±åƒç…§ç›¸é¦†ä¸€æ ·ï¼Œå¯ä»¥ä¸ºåŒä¸€ä¸ªäººæˆ–è€…ç‰©ä½“ç…§å¤šå¼ ä¸åŒèƒŒæ™¯ã€ä¸åŒå§¿æ€ã€ä¸åŒæœè£…çš„ç…§ç‰‡ï¼ˆå’ŒControlNetä¸åŒå»æ·»åŠ æ¨¡å‹ç»“æ„ï¼Œä»…ä»…æ˜¯åœ¨æ–‡æœ¬ Promptï¼‰ã€‚åœ¨è®ºæ–‡[^4]é‡Œé¢ä¸»è¦å‡ºå‘ç‚¹å°±æ˜¯ï¼š1ã€è§£å†³**language drif**ï¼ˆè¯­è¨€åç¦»é—®é¢˜ï¼‰ï¼šæŒ‡çš„æ˜¯æ¨¡å‹é€šè¿‡åè®­ç»ƒï¼ˆå¾®è°ƒç­‰å¤„ç†ä¹‹åï¼‰æ¨¡å‹ä¸§å¤±äº†å¯¹æŸäº›è¯­ä¹‰ç‰¹å¾çš„æ„ŸçŸ¥ï¼Œå°±æ¯”å¦‚è¯´æ‰©æ•£æ¨¡å‹é‡Œé¢ï¼Œæ¨¡å‹é€šè¿‡ä¸æ–­å¾®è°ƒå¯èƒ½å°±ä¸çŸ¥é“â€œç‹—â€æ˜¯ä»€ä¹ˆä»è€Œå¯¼è‡´æ¨¡å‹ç”Ÿæˆé”™è¯¯ã€‚2ã€é«˜æ•ˆçš„ç”Ÿæˆéœ€è¦çš„å¯¹è±¡ï¼Œä¸ä¼šäº§ç”Ÿï¼šç”Ÿæˆé”™è¯¯ã€ç»†èŠ‚ä¸¢å¤±é—®é¢˜ï¼Œæ¯”å¦‚è¯´ä¸‹é¢å›¾åƒä¸­çš„é—®é¢˜ï¼š
![](https://s2.loli.net/2025/07/12/mRaHPOtC23li9Fn.webp)

ä¸ºäº†å®ç°å›¾åƒçš„â€œé«˜æ•ˆè¿ç§»â€ï¼Œä½œè€…ç›´æ¥å°†å›¾åƒï¼ˆæ¯”å¦‚è¯´æˆ‘ä»¬éœ€è¦é£æ ¼åŒ–çš„å›¾ç‰‡ï¼‰ä½œä¸ºä¸€ä¸ªç‰¹æ®Šçš„æ ‡è®°ï¼Œä¹Ÿå°±æ˜¯è®ºæ–‡é‡Œé¢æåˆ°çš„ `a [identifier] [class noun]`ï¼ˆå…¶ä¸­class nounä¸ºç±»åˆ«æ¯”å¦‚æ‰€ç‹—ï¼Œidentifierå°±æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„æ ‡è®°ï¼‰ï¼Œåœ¨promptä¸­åŠ å…¥ç±»åˆ«ï¼Œé€šè¿‡åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹ä¸­å…³äºè¯¥ç±»åˆ«ç‰©å“çš„å…ˆéªŒçŸ¥è¯†ï¼Œå¹¶å°†å…ˆéªŒçŸ¥è¯†ä¸ç‰¹æ®Šæ ‡è®°ç¬¦ç›¸å…³ä¿¡æ¯è¿›è¡Œèåˆï¼Œè¿™æ ·å°±å¯ä»¥åœ¨ä¸åŒåœºæ™¯ä¸‹ç”Ÿæˆä¸åŒå§¿åŠ¿çš„ç›®æ ‡ç‰©ä½“ã€‚å°±æ¯”å¦‚ä¸‹é¢çš„ `fine-tuning`è¿‡ç¨‹é€šè¿‡å‡ å¼ å›¾ç‰‡è®©æ¨¡å‹å­¦ä¹ åˆ° *ç‰¹æ®Šçš„ç‹—*ï¼Œç„¶åå†æ¨ç†é˜¶æ®µæ¨¡å‹å¯ä»¥åˆ©ç”¨è¿™ä¸ª *ç‰¹æ®Šçš„ç‹—*å»ç”Ÿæˆæ–°çš„åŠ¨ä½œã€‚**æ¢è¨€ä¹‹**å°±æ˜¯ï¼ˆä»¥ä¸‹é¢å®é™…DreamBoothä»£ç ä¸ºä¾‹ï¼‰ï¼šé¦–å…ˆé€šè¿‡å‡ å¼  *ç‹®å­ç‹—* å›¾ç‰‡è®©æ¨¡å‹çŸ¥é“ *ç‹®å­ç‹—*å¼ ä»€ä¹ˆæ ·å­ï¼Œç„¶åå†å»ç”Ÿæˆ *ç‹®å­ç‹—*çš„ä¸åŒçš„åŠ¨ä½œã€‚

![](https://s2.loli.net/2025/07/12/hYM1VdykDxALrGo.webp)

åœ¨è®ºæ–‡é‡Œé¢ä½œè€…è®¾è®¡å¦‚ä¸‹çš„Class-specific Prior Preservation Lossï¼ˆå‚è€ƒstackexchangeï¼‰[^5]ï¼š

$$\begin{aligned}
 & \mathbb{E}_{x,c,\epsilon,t}\left[\|\epsilon-\varepsilon_{\theta}(z_{t},t,c)\|_{2}^{2}+\lambda\|\epsilon^{\prime}-\epsilon_{pr}(z_{t^{\prime}}^{\prime},t^{\prime},c_{pr})\|_{2}^{2}\right]
\end{aligned}$$

ä¸Šé¢æŸå¤±å‡½æ•°ä¸­åé¢ä¸€éƒ¨åˆ†å°±æ˜¯æˆ‘ä»¬çš„å…ˆéªŒæŸå¤±ï¼Œæ¯”å¦‚è¯´$c_{pr}$å°±æ˜¯å¯¹ "a dog"è¿›è¡Œç¼–ç ç„¶åè®¡ç®—ç”ŸæˆæŸå¤±ã€‚åœ¨ä»£ç ä¸­ï¼š

```python
if args.with_prior_preservation:
    model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)
    target, target_prior = torch.chunk(target, 2, dim=0)
    # Compute instance loss
    loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
    # Compute prior loss
    prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction="mean")
    # Add the prior loss to the instance loss.
    loss = loss + args.prior_loss_weight * prior_loss
else:
    loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
```

#### DreamBoothä»£ç æ“ä½œ
> ä»£ç ï¼š[https://github.com/shangxiaaabb/ProjectCode/tree/main/code/Python/DFModelCode/training_dreambooth_lora/](https://github.com/shangxiaaabb/ProjectCode/tree/main/code/Python/DFModelCode/training_dreambooth_lora/)
> æƒé‡ï¼š[https://www.modelscope.cn/models/bigyellowjie/SDXL-DreamBooth-LOL/files](https://www.modelscope.cn/models/bigyellowjie/SDXL-DreamBooth-LOL/files)

åœ¨ä»‹ç»DreamBoothä»£ç ä¹‹å‰ï¼Œç®€å•å›é¡¾DreamBoothåŸç†ï¼Œæˆ‘å¸Œæœ›æˆ‘çš„æ¨¡å‹å»å­¦ä¹ ä¸€ç§ç”»é£é‚£ä¹ˆæˆ‘å°±éœ€è¦å‡†å¤‡**æ ·æœ¬å›¾ç‰‡**ï¼ˆå¦‚3-5å¼ å›¾ç‰‡ï¼‰è¿™å‡ å¼ å›¾ç‰‡å°±æ˜¯ä¸“é—¨çš„æ¨¡å‹éœ€è¦å­¦ä¹ çš„ï¼Œä½†æ˜¯ä¸ºäº†é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼ˆæ¨¡å‹åªå­¦ä¹ äº†æˆ‘çš„å›¾ç‰‡å†…å®¹ï¼Œä½†æ˜¯å¯¹ä¸€äº›ç»†èŠ‚ä¸¢æ‰äº†ï¼Œæ¯”å¦‚è¯´æˆ‘æä¾›çš„5å¼ æ²¹ç”»ï¼Œæ¨¡å‹å°±å­¦ä¼šäº†æˆ‘çš„æ²¹ç”»ç”»é£ä½†æ˜¯ä¸ºäº†é˜²æ­¢æ¨¡å‹å¯¹æ›´åŠ å¤šçš„æ²¹ç”»ç»†èŠ‚å¿˜è®°äº†ï¼Œé‚£ä¹ˆæˆ‘å°±å‡†å¤‡`num_epochs * num_samples` å¼ æ²¹ç”»**ç±»å‹å›¾ç‰‡**ç„¶åé€šè¿‡è®¡ç®— `Class-specific Prior Preservation Loss`ï¼‰éœ€è¦å‡†å¤‡ **ç±»å‹å›¾ç‰‡**æ¥è®¡ç®—Class-specific Prior Preservation Lossã€‚ä»£ç å¤„ç†ï¼ˆSDXL+Loraï¼‰ï¼š
DreamBoothä¸­**æ•°æ®å¤„ç†è¿‡ç¨‹**ï¼šç»“åˆä¸Šé¢æè¿°æˆ‘éœ€è¦å‡†å¤‡ä¸¤éƒ¨åˆ†æ•°æ®é›†ï¼ˆå¦‚æœéœ€è¦è®¡ç®—`Class-specific Prior Preservation Loss`ï¼‰åˆ†åˆ«ä¸ºï¼š`instance_data_dir`ï¼ˆä¸ä¹‹å¯¹åº”çš„`instance_prompt`ï¼‰ä»¥åŠ `class_data_dir`ï¼ˆä¸ä¹‹å¯¹åº”çš„ `class_prompt`ï¼‰è€Œåéœ€è¦åšçš„å°±æ˜¯å°†ä¸¤éƒ¨åˆ†æ•°æ®ç»„åˆèµ·æ¥æ„æˆï¼š
```python
batch = {
    "pixel_values": pixel_values,
    "prompts": prompts,
    "original_sizes": original_sizes,
    "crop_top_lefts": crop_top_lefts,
}
```
æ¨¡å‹è®­ç»ƒè¿‡ç¨‹**é¦–å…ˆæ˜¯loraå¤„ç†æ¨¡å‹**ï¼šåœ¨åŸºäºtransformeré‡Œé¢çš„æ¨¡å‹å¾ˆå®¹æ˜“ä½¿ç”¨loraï¼Œæ¯”å¦‚è¯´ä¸‹é¢ä»£ç ä½¿ç”¨loraåŒ…è£¹æ¨¡å‹å¹¶ä¸”å¯¹æ¨¡å‹æƒé‡è¿›è¡Œä¿å­˜ï¼š
```python
from peft import LoraConfig
def get_lora_config(rank, dropout, use_dora, target_modules):
    '''lora config'''
    base_config = {
        "r": rank,
        "lora_alpha": rank,
        "lora_dropout": dropout,
        "init_lora_weights": "gaussian",
        "target_modules": target_modules,
    }
    return LoraConfig(**base_config)
# åŒ…è£¹loraæ¨¡å‹æƒé‡
unet_target_modules = ["to_k", "to_q", "to_v", "to_out.0"]
unet_lora_config = get_lora_config(
    rank= config.rank,
    dropout= config.lora_dropout,
    use_dora= config.use_dora,
    target_modules= unet_target_modules,
)
unet.add_adapter(unet_lora_config)
```
ä¸€èˆ¬çš„è¯è€ƒè™‘SDæ¨¡å‹æƒé‡éƒ½æ¯”è¾ƒå¤§ï¼Œè€Œä¸”æˆ‘ä»¬ä½¿ç”¨loraå¾®è°ƒæ¨¡å‹æ²¡å¿…è¦å¯¹æ‰€æœ‰çš„æ¨¡å‹æƒé‡è¿›è¡Œå­˜å‚¨ï¼Œé‚£ä¹ˆä¸€èˆ¬éƒ½ä¼šå®šä¹‰ä¸€ä¸ª`hook`æ¥å‘Šè¯‰æ¨¡å‹é‚£äº›å‚æ•°éœ€è¦ä¿å­˜ã€åŠ è½½ï¼Œè¿™æ ·ä¸€æ¥ä½¿ç”¨ `accelerator.save_state(save_path)` å°±ä¼šå…ˆå»ä½¿ç”¨ `hook`å¤„ç†å‚æ•°ç„¶åè¿›è¡Œä¿å­˜ã€‚ï¼š
```python
def save_model_hook(models, weights, output_dir):
    if accelerator.is_main_process:
        unet_lora_layers_to_save = None
        
        for model in models:
            if isinstance(model, type(unwrap_model(unet))):
                unet_lora_layers_to_save = convert_state_dict_to_diffusers(get_peft_model_state_dict(model))
            ...
            weights.pop() # å»æ‰ä¸éœ€è¦ä¿å­˜çš„å‚æ•°

        StableDiffusionXLPipeline.save_lora_weights(
            output_dir,
            unet_lora_layers= unet_lora_layers_to_save,
            ...
        )
def load_model_hook(models, input_dir):
    unet_ = None

    while len(models) > 0:
        model = models.pop()

        if isinstance(model, type(unwrap_model(unet))):
            unet_ = model

    lora_state_dict, network_alphas = StableDiffusionLoraLoaderMixin.lora_state_dict(input_dir)

    unet_state_dict = {f"{k.replace('unet.', '')}": v for k, v in lora_state_dict.items() if k.startswith("unet.")}
    unet_state_dict = convert_unet_state_dict_to_peft(unet_state_dict)
    incompatible_keys = set_peft_model_state_dict(unet_, unet_state_dict, adapter_name="default")
    ...
accelerator.register_save_state_pre_hook(save_model_hook)
accelerator.register_load_state_pre_hook(load_model_hook)
```
**å…¶æ¬¡æ¨¡å‹è®­ç»ƒ**ï¼šå°±æ˜¯å¸¸è§„çš„æ¨¡å‹è®­ç»ƒï¼ˆç›´æ¥åœ¨æ ·æœ¬å›¾ç‰‡ï¼š`instance_data_dir`ä»¥åŠæ ·æœ¬çš„promptï¼š`instance_prompt`ä¸Šè¿›è¡Œå¾®è°ƒï¼‰ç„¶åè®¡ç®—losså³å¯ï¼Œå¦‚æœæ¶‰åŠåˆ°`Class-specific Prior Preservation Loss`ï¼ˆé™¤äº†ä¸Šé¢ä¸¤ä¸ªç»„åˆè¿˜éœ€è¦ï¼š`class_data_dir`ä»¥åŠ `class_prompt`ï¼‰é‚£ä¹ˆå¤„ç†è¿‡ç¨‹ä¸ºï¼ˆä»¥SDXLä¸ºä¾‹ï¼‰ï¼Œä¸è¿‡éœ€è¦äº‹å…ˆäº†è§£çš„æ˜¯åœ¨è®¡ç®—è¿™ä¸ªlossä¹‹å‰ä¼šå°†ä¸¤ä¸ªæ•°æ®é›†ä»¥åŠpromptéƒ½**ç»„åˆåˆ°ä¸€èµ·æˆä¸ºä¸€ä¸ªæ•°æ®é›†**ï¼ˆ`instance-image-prompt` ä»¥åŠ `class-image-prompt`ä¹‹é—´æ˜¯åŒ¹é…çš„ï¼‰ï¼š
```python
# æ ·æœ¬å†…å®¹ç¼–ç 
instance_prompt_hidden_states, instance_pooled_prompt_embeds = compute_text_embeddings(config.instance_prompt, text_encoders, tokenizers)
# ç±»å‹å›¾ç‰‡å†…å®¹ç¼–ç 
if config.with_prior_preservation:
    class_prompt_hidden_states, class_pooled_prompt_embeds = compute_text_embeddings(config.class_prompt, text_encoders, tokenizers)
...
prompt_embeds = instance_prompt_hidden_states
unet_add_text_embeds = instance_pooled_prompt_embeds
if not config.with_prior_preservation:
    prompt_embeds = torch.cat([prompt_embeds, class_prompt_hidden_states], dim=0)
    unet_add_text_embeds = torch.cat([unet_add_text_embeds, class_pooled_prompt_embeds], dim=0)
...
model_pred = unet(...)
if config.with_prior_preservation:
    model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)
    target, target_prior = torch.chunk(target, 2, dim=0)
    ...
    prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction="mean")
...
loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
...
loss = loss + config.prior_loss_weight * prior_loss
accelerator.backward(loss)
```
åœ¨è¿™ä¸ªé‡Œé¢ä¹‹æ‰€ä»¥ç”¨ `chunk`æ˜¯å› ä¸ºå¦‚æœè®¡ç®—`Class-specific Prior Preservation Loss`é‡Œé¢çš„æ–‡æœ¬promptæ˜¯ç”±ä¸¤éƒ¨åˆ†æ‹¼æ¥æ„æˆçš„`torch.cat([prompt_embeds, class_prompt_hidden_states], dim=0)`é‚£ä¹ˆå¯ä»¥ç›´æ¥é€šè¿‡chunkæ¥åˆ†
é‚£ä¹ˆè¿™æ ·ä¸€æ¥æ•°æ®ä¸­ä¸€åŠæ¥è‡ªæ ·æœ¬å›¾ç‰‡ä¸€éƒ¨åˆ†æ¥è‡ªç±»å‹å›¾ç‰‡ï¼Œåœ¨æ¨¡å‹å¤„ç†ä¹‹ååœ¨`model_pred`å°±æœ‰ä¸€éƒ¨åˆ†æ˜¯æ ·æœ¬å›¾ç‰‡çš„é¢„æµ‹ï¼Œå¦å¤–ä¸€éƒ¨åˆ†ä¸ºç±»å‹å›¾ç‰‡é¢„æµ‹ã€‚æœ€åæµ‹è¯•çš„ç»“æœä¸ºï¼ˆ`prompt: "A photo of Rengar the Pridestalker in a bucket"`ï¼Œæ¨¡å‹[ä»£ç ](https://github.com/shangxiaaabb/ProjectCode/tree/main/code/Python/DFModelCode/training_dreambooth_lora/)ä»¥åŠ[æƒé‡ä¸‹è½½](https://www.modelscope.cn/models/bigyellowjie/SDXL-DreamBooth-LOL/files)ï¼‰ï¼š

![image.png](https://s2.loli.net/2025/07/15/7xIPMW6SJ1degZj.webp)

<!-- ## ç®€æ˜“Demoä»£ç 
é€šè¿‡æ€»ç»“ä¸Šé¢ä»£ç åœ¨â€œå¾®è°ƒâ€DFæ¨¡å‹ä¸­ä¸€ä¸ªç®€æ˜“çš„ä»£ç æµç¨‹ï¼ˆä»¥å¾®è°ƒSDXLæ¨¡å‹ä¸ºä¾‹ï¼‰ä¸ºï¼ˆSDXLæ¨¡å‹å¯ä»¥ç›´æ¥å‚è€ƒ[training_dreambooth_lora](https://github.com/shangxiaaabb/ProjectCode/tree/main/code/Python/DFModelCode/training_dreambooth_lora)ï¼‰ï¼š
**1ã€åŸºç¡€æ¨¡å‹åŠ è½½**
SDXLåŒºåˆ«SD1.5å…¶å­˜åœ¨ä¸¤ä¸ªæ–‡æœ¬ç¼–ç å™¨å› æ­¤åœ¨åŠ è½½è¿‡ç¨‹ä¸­éœ€è¦åŠ è½½ä¸¤ä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼Œå¹¶ä¸”åŸºç¡€æ¨¡å‹åŠ è½½ä¸»è¦æ˜¯åŠ è½½å¦‚ä¸‹å‡ ä¸ªæ¨¡å‹ï¼ˆå¦‚æœ*ä¸æ¶‰åŠåˆ°æ–‡æœ¬å¯èƒ½å°±ä¸éœ€è¦æ–‡æœ¬ç¼–ç å™¨*ï¼‰ï¼š1ã€æ–‡æœ¬ç¼–ç å™¨ï¼›2ã€VAEæ¨¡å‹ï¼›3ã€Unetæ¨¡å‹ï¼›4ã€è°ƒåº¦å™¨ã€‚é™¤æ­¤ä¹‹å¤–å¯¹äºæ‰€æœ‰çš„æ¨¡å‹éƒ½ä¼šä¸å»å°±è¡Œå‚æ•°æ›´æ–°ã€‚
**2ã€ç²¾åº¦è®¾ç½®** -->


## æ€»ç»“
å¯¹äºä¸åŒçš„æ‰©æ•£ï¼ˆåŸºåº§ï¼‰æ¨¡å‹ï¼ˆSD1.5ã€SDXLã€Imagenï¼‰ç­‰å¤§éƒ¨åˆ†éƒ½æ˜¯é‡‡ç”¨Unetç»“æ„ï¼Œå½“ç„¶ä¹Ÿæœ‰é‡‡ç”¨Ditçš„ï¼Œè¿™ä¸¤ä¸ªæ¨¡å‹ï¼ˆSD1.5ã€SDXLï¼‰ä¹‹é—´çš„å·®å¼‚ä¸»è¦åœ¨äºåè€…ä¼šå¤šä¸€ä¸ªclipç¼–ç å™¨å†æ–‡æœ¬è¯­ä¹‰ä¸Šæ¯”å‰è€…æ›´åŠ æœ‰ä¼˜åŠ¿ã€‚å¯¹äºadapterè€Œè¨€ï¼Œå¯ä»¥ç›´æ¥ç†è§£ä¸ºå†SDçš„åŸºç¡€ä¸Šå»ä½¿ç”¨â€œé£æ ¼æ’ä»¶â€ï¼Œè¿™ä¸ªæ’ä»¶ä¸å»å¯¹SDæ¨¡å‹è¿›è¡Œè®­ç»ƒï¼ˆä»è€Œå®ç°å¯¹å‚æ•°çš„å‡å°ï¼‰ï¼Œå¯¹äºControNetå°±æ˜¯ç›´æ¥å¯¹Unetçš„ä¸‹é‡‡æ ·æ‰€æœ‰çš„æ¨¡å—ï¼ˆå‰åï¼‰éƒ½åŠ ä¸€ä¸ªzero-convè€Œåå°†ç»“æœå†å»åµŒå…¥åˆ°ä¸‹é‡‡ç”¨ä¸­ï¼Œè€ŒT2I-Adapteråˆ™æ˜¯å»å¯¹æ¡ä»¶è¿›è¡Œç¼–ç è€ŒååµŒå…¥åˆ°SDæ¨¡å‹ï¼ˆä¸Šé‡‡ç”¨æ¨¡å—ï¼‰ä¸­ã€‚å¯¹äºderambothå°±æ˜¯ç›´æ¥é€šè¿‡ç»™å®šçš„æ ·æœ¬å›¾ç‰‡å»ç”Ÿâ€œå¾®è°ƒâ€æ¨¡å‹ï¼Œè€Œåé€šè¿‡è®¾è®¡çš„Class-specific Prior Preservation Lossæ¥ç¡®ä¿æ‰€ç”Ÿæˆçš„æ ·æœ¬ç‰¹é‡Œä¸ä¼šå‘ç”Ÿè¿‡æ‹Ÿåˆã€‚

## å‚è€ƒ
[^1]:[https://arxiv.org/pdf/2307.01952](https://arxiv.org/pdf/2307.01952)
[^2]:[https://arxiv.org/pdf/2302.05543](https://arxiv.org/pdf/2302.05543)
[^3]:[https://arxiv.org/pdf/2302.08453](https://arxiv.org/pdf/2302.08453)
[^4]:[https://arxiv.org/pdf/2208.12242](https://arxiv.org/pdf/2208.12242)
[^5]:https://stats.stackexchange.com/questions/601782/how-to-rewrite-dreambooth-loss-in-terms-of-epsilon-prediction
[^6]:[https://arxiv.org/pdf/2205.11487](https://arxiv.org/pdf/2205.11487)
[^8]:[https://arxiv.org/pdf/2405.08748](https://arxiv.org/pdf/2405.08748)
[^9]:[https://arxiv.org/pdf/2506.15742](https://arxiv.org/pdf/2506.15742)
[^10]:[https://arxiv.org/pdf/2310.00426](https://arxiv.org/pdf/2310.00426)
[^11]:[Scalable Diffusion Models with Transformers](https://openaccess.thecvf.com/content/ICCV2023/papers/Peebles_Scalable_Diffusion_Models_with_Transformers_ICCV_2023_paper.pdf)
[^12]: [https://arxiv.org/pdf/2403.03206](https://arxiv.org/pdf/2403.03206)
[^13]: [https://arxiv.org/pdf/2109.07161](https://arxiv.org/pdf/2109.07161)
[^14]: [https://zhuanlan.zhihu.com/p/684068402](https://zhuanlan.zhihu.com/p/684068402)
[^15]: [https://zhouyifan.net/2024/09/03/20240809-flux1/](https://zhouyifan.net/2024/09/03/20240809-flux1/)
[^16]: [https://zhouyifan.net/2024/07/14/20240703-SD3/](https://zhouyifan.net/2024/07/14/20240703-SD3/)
[^17]: [https://stability.ai/news/stable-diffusion-3-research-paper](https://stability.ai/news/stable-diffusion-3-research-paper)
[^18]: [https://arxiv.org/pdf/2508.02324](https://arxiv.org/pdf/2508.02324)