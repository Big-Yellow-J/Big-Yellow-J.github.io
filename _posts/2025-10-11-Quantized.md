---
layout: mypost
title: æ¨¡å‹é‡åŒ–æ“ä½œâ€”â€”â€”â€”GPTQå’ŒAWQé‡åŒ–
categories: é‡åŒ–éƒ¨ç½²
address: æ­¦æ±‰ğŸ¯
extMath: true
show_footer_image: true
tags:
- æ¨¡å‹é‡åŒ–
description: æ¨¡å‹é‡åŒ–æŠ€æœ¯æ˜¯å°†é«˜ç²¾åº¦ï¼ˆå¦‚FP16/FP32ï¼‰æ¨¡å‹æƒé‡è½¬åŒ–ä¸ºä½æ¯”ç‰¹ï¼ˆå¦‚INT8ã€INT4ï¼‰çš„å‹ç¼©æ–¹æ³•ï¼Œåˆ†é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰å’Œåé‡åŒ–ï¼ˆPTQï¼‰ä¸¤å¤§ç±»ï¼Œæ ¸å¿ƒæ˜¯æ•°å€¼ç²¾åº¦æ ¡å‡†ä¸è½¬åŒ–ã€‚GPTQä½œä¸ºLLMåé‡åŒ–æŠ€æœ¯ï¼Œé€šè¿‡â€œé‡åŒ–-è¡¥å¿-é‡åŒ–-è¡¥å¿â€è¿­ä»£æµç¨‹ï¼Œåˆ†å—æ‹†åˆ†æƒé‡çŸ©é˜µï¼Œé€åˆ—é‡åŒ–å¹¶è®¡ç®—è¯¯å·®è¡¥å¿ï¼Œåˆ©ç”¨HessiançŸ©é˜µè¾…åŠ©è¯¯å·®ä¼ æ’­ï¼Œå®ç°ä½æ¯”ç‰¹ï¼ˆ3-4ä½ï¼‰å‹ç¼©ã€‚AWQåˆ™åŸºäºæ¿€æ´»å€¼åˆ†å¸ƒæŒ‘é€‰æ˜¾è‘—æƒé‡ï¼ˆ0.1%-1%ï¼‰ï¼Œå¯¹æ‰€æœ‰æƒé‡ä½æ¯”ç‰¹é‡åŒ–ï¼Œæ˜¾è‘—æƒé‡ä¹˜å¤§scaleé™ä½è¯¯å·®ï¼Œéæ˜¾è‘—æƒé‡ä¹˜å°scaleï¼Œé€šè¿‡ç½‘æ ¼æœç´¢ç¡®å®šæœ€ä½³scaleï¼Œä¿æŒç²¾åº¦å¹¶å‡å°‘å†…å­˜å ç”¨ã€‚ä¸¤è€…å‡å±PTQï¼Œæœ‰æ•ˆå¹³è¡¡æ¨¡å‹å¤§å°ã€å†…å­˜å ç”¨ä¸æ¨ç†å‡†ç¡®æ€§ã€‚
---

## æ¨¡å‹é‡åŒ–æŠ€æœ¯
ç®€å•äº†è§£å‡ ä¸ªæ¦‚å¿µï¼š
**é‡åŒ–**ï¼šæ˜¯ä¸€ç§æ¨¡å‹å‹ç¼©çš„å¸¸è§æ–¹æ³•ï¼Œå°†æ¨¡å‹æƒé‡ä»é«˜ç²¾åº¦ï¼ˆå¦‚FP16æˆ–FP32ï¼‰é‡åŒ–ä¸ºä½æ¯”ç‰¹ä½ï¼ˆå¦‚INT8ã€INT4ï¼‰ã€‚å¸¸è§çš„é‡åŒ–ç­–ç•¥å¯ä»¥åˆ†ä¸ºPTQå’ŒQATä¸¤å¤§ç±»ã€‚**é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ**ï¼ˆQuantization-Aware Trainingï¼‰ï¼šåœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­è¿›è¡Œé‡åŒ–ï¼Œä¸€èˆ¬æ•ˆæœä¼šæ›´å¥½ä¸€äº›ï¼Œä½†éœ€è¦é¢å¤–è®­ç»ƒæ•°æ®å’Œå¤§é‡è®¡ç®—èµ„æºã€‚**åé‡åŒ–**ï¼ˆPost-Training Quantization, PTQï¼‰ï¼šåœ¨æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œå¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œæ— éœ€é‡æ–°è®­ç»ƒã€‚
å› æ­¤å¯¹äºé‡åŒ–è¿‡ç¨‹æ€»ç»“ä¸ºï¼š**å°†æ•°å€¼ç²¾åº¦è¿›è¡Œâ€œæ ¡å‡†â€**ï¼ˆæ¯”å¦‚FP32è½¬åŒ–åˆ°INT8ï¼Œä¸¤ç§è¡¨è¿°èŒƒå›´ä¸åŒï¼Œå› æ­¤å°±éœ€è¦å°†å‰è€…æ ¡å‡†åˆ°åè€…èŒƒå›´ï¼‰ï¼Œ**å¯¹â€œæ ¡å‡†â€æ•°æ®è¿›è¡Œç²¾åº¦è½¬åŒ–**ã€‚å¯¹äºçº¿æ€§é‡åŒ–ä¸‹ï¼Œæµ®ç‚¹æ•°ä¸å®šç‚¹æ•°ä¹‹é—´çš„è½¬æ¢å…¬å¼å¦‚ä¸‹ï¼š$Q=\frac{R}{S}+Z;R=(Q-Z)*S$ï¼Œå…¶ä¸­R è¡¨ç¤ºé‡åŒ–å‰çš„æµ®ç‚¹æ•°ã€Q è¡¨ç¤ºé‡åŒ–åçš„å®šç‚¹æ•°ã€Sï¼ˆScaleï¼‰è¡¨ç¤ºç¼©æ”¾å› å­çš„æ•°å€¼ã€Zï¼ˆZeroï¼‰è¡¨ç¤ºé›¶ç‚¹çš„æ•°å€¼ã€‚
**æ¨¡å‹é‡åŒ–å…·ä½“å®ç°è¿‡ç¨‹**ï¼ˆç›´æ¥ä½¿ç”¨ï¼š[https://zhuanlan.zhihu.com/p/646210009](https://zhuanlan.zhihu.com/p/646210009)ä¸­çš„æè¿°ï¼‰ï¼š
å¯¹ç§°é‡åŒ–ä¸­ï¼Œé›¶ç‚¹ Z = 0ï¼Œä¸€èˆ¬ä¸è®°å½•ï¼Œæˆ‘ä»¬åªéœ€è¦å…³å¿ƒå¦‚ä½•æ±‚è§£ Scaleã€‚ç”±äº weight å‡ ä¹ä¸å­˜åœ¨å¼‚å¸¸å€¼ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç›´æ¥å– Scale ä¸ºä¸€ä¸ª layer æˆ– block å†…æ‰€æœ‰å‚æ•°çš„æœ€å¤§ç»å¯¹å€¼ï¼Œäºæ˜¯æ‰€æœ‰çš„å‚æ•°éƒ½åœ¨ [-1, 1] çš„åŒºé—´å†…ã€‚éšåï¼Œè¿™äº›å‚æ•°å°†æ‰¾åˆ°æœ€è¿‘çš„é‡åŒ–æ ¼ç‚¹ï¼Œå¹¶è½¬åŒ–æˆå®šç‚¹æ•°ã€‚
![](https://s2.loli.net/2025/10/12/Jly87qpuXLHVWPT.webp)
**æ¨èè¿›ä¸€æ­¥é˜…è¯»**ï¼š[https://www.big-yellow-j.top/posts/2025/12/29/SDAcceralate.html](https://www.big-yellow-j.top/posts/2025/12/29/SDAcceralate.html)
### GPTQé‡åŒ–æŠ€æœ¯
GPTQ[^1]æ˜¯ä¸€ç§ç”¨äºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰çš„åè®­ç»ƒé‡åŒ–æŠ€æœ¯ã€‚å®ƒé€šè¿‡å°†æ¨¡å‹æƒé‡ä»é«˜ç²¾åº¦ï¼ˆå¦‚FP16æˆ–FP32ï¼‰å‹ç¼©åˆ°ä½æ¯”ç‰¹ï¼ˆå¦‚3-4ä½æ•´æ•°ï¼‰æ¥å‡å°‘æ¨¡å‹å¤§å°å’Œå†…å­˜å ç”¨ï¼ŒåŒæ—¶ä¿æŒè¾ƒé«˜çš„æ¨ç†å‡†ç¡®æ€§ã€‚ä¸€èˆ¬è€Œè¨€å¯¹äºé‡åŒ–è¿‡ç¨‹ä¸ºï¼šå¯¹äºç»™å®šçš„æƒé‡çŸ©é˜µ$W\in R^{n\times m}$ï¼Œ**é‡åŒ–è¿‡ç¨‹**å°±æ˜¯éœ€è¦æ‰¾åˆ°ä¸€ä¸ªä½æ¯”ç‰¹çš„çŸ©é˜µ$\hat{W}$ä½¿å¾—ï¼š

$$
\min_{\hat{w}}\Vert WX-\hat{W}X\Vert^2_F
$$

å…¶ä¸­$X$ä¸ºè¾“å…¥å‘é‡ï¼Œ$\Vert. \Vert_F$ä¸ºFrobeniusèŒƒæ•°ã€‚æŒ‰ç…§è®ºæ–‡é‡Œé¢çš„æè¿°GPTQæ•´ä¸ªè¿‡ç¨‹ä¸ºï¼š
![](https://s2.loli.net/2025/10/12/zTrLfJi3HXyt9jm.webp)
> å®é™…ä½¿ç”¨LLMCompressorè¿›è¡Œæ¨¡å‹é‡åŒ–è¿‡ç¨‹ä¸­ï¼Œ$\lambda$å¯¹åº”å‚æ•°`dampening_frac`å¯èƒ½ï¼ˆ$W8A8$ï¼‰ä¼šå‡ºç°ï¼š`Failed to invert hessian due to numerical instability. Consider increasing GPTQModifier.dampening_frac, increasing the number of calibration samples, or shuffling the calibration dataset`å…¶ä¸»è¦åŸå› æ˜¯è®¡ç®—HessiançŸ©é˜µå‡ºç°ä¸¥é‡ç—…æ€ï¼ˆill-conditionedï¼‰æˆ–æ¥è¿‘å¥‡å¼‚/éæ­£å®šæ—¶ï¼ŒCholesky åˆ†è§£å°±ä¼šå¤±è´¥ï¼ŒæŠ›å‡ºæ•°å€¼ä¸ç¨³å®šé”™è¯¯ã€‚å› æ­¤å°±å¯ä»¥æ ¹æ®é‡Œé¢å»ºè®®ï¼šå¢åŠ æ•°æ®ã€å¢åŠ $\lambda$çš„å€¼

å¯¹äºå…·ä½“æ•°å­¦åŸç†çš„æè¿°å‚è€ƒæ–‡ç« [^2][^3]ï¼ˆæ•°å­¦åŸç†æ¨èç›´æ¥çœ‹ï¼š[GPTQè¯¦ç»†è§£è¯»](https://zhuanlan.zhihu.com/p/1941146483756897225)ï¼‰ï¼Œç®€å•æ€»ç»“ä¸€ä¸‹ä¸Šé¢è¿‡ç¨‹å°±æ˜¯ï¼š1ã€æ¯è¡Œç‹¬ç«‹è®¡ç®—äºŒé˜¶æµ·æ£®çŸ©é˜µã€‚2ã€æ¯è¡ŒæŒ‰é¡ºåºè¿›è¡Œé€ä¸ªå‚æ•°é‡åŒ–ï¼Œä»è€Œå¯ä»¥å¹¶è¡Œè®¡ç®—ã€‚3ã€æŒ‰blockç»´åº¦è¿›è¡Œæ›´æ–°ï¼Œå¯¹å‰©ä½™å‚æ•°è¿›è¡Œå»¶è¿Ÿæ›´æ–°å¼¥è¡¥ã€‚4ã€å¯¹é€†æµ·æ£®çŸ©é˜µä½¿ç”¨choleskyåˆ†è§£ï¼Œç­‰ä»·æ¶ˆé™¤è¿­ä»£ä¸­çš„çŸ©é˜µæ›´æ–°è®¡ç®—ã€‚**å®ƒçš„æ ¸å¿ƒæµç¨‹å…¶å®å°±æ˜¯é‡åŒ–-è¡¥å¿-é‡åŒ–-è¡¥å¿çš„è¿­ä»£**ï¼ˆå…·ä½“è¿‡ç¨‹è§æµç¨‹å›¾ä¸­**å†…éƒ¨å¾ªç¯**ï¼šé¦–å…ˆé‡åŒ–$W_{:,j}$ï¼Œè€Œåå»è®¡ç®—è¯¯å·®å¹¶ä¸”è¡¥å……åˆ° $W_{:,j:(i+B)}$ï¼‰ï¼Œå…·ä½“çš„ä»£ç å®ç°è¿‡ç¨‹ï¼ˆ[å®˜æ–¹GPTQ-Github](https://github.com/IST-DASLab/gptq)ï¼‰ä¸»è¦æ˜¯å¯¹å…¶ä¸­LlamaAttentionå’ŒLlamaMLPå±‚ä¸­çš„Linearå±‚[æƒé‡è¿›è¡Œé‡åŒ–](https://github.com/IST-DASLab/gptq/blob/2d65066eeb06a5c9ff5184d8cebdf33662c67faf/llama.py#L75C1-L84C1)ã€‚ä»£ç å¤„ç†è¿‡ç¨‹[^4]ï¼š
**é¦–å…ˆ**ã€è®¡ç®—HessiançŸ©é˜µï¼ˆå› ä¸ºåç»­è®¡ç®—æŸå¤±å’Œè¡¥å¿æƒé‡éœ€è¦ï¼Œå› æ­¤æå‰è®¡ç®—çŸ©é˜µï¼‰
è¿™ä¸ªçŸ©é˜µè¿‘ä¼¼ï¼š$H_F=2X_FX_F^T$ï¼ˆ$X$æ˜¯**ç»è¿‡å‰é¢å‡ å±‚ç¥ç»ç½‘ç»œä¹‹åï¼Œåˆ°è¾¾è¢«é‡åŒ–å±‚çš„æ¿€æ´»**ï¼‰ã€‚å®ç°æ–¹å¼æ˜¯åœ¨æ¯ä¸€å±‚Layerä¸Šæ³¨å†Œhookï¼Œé€šè¿‡hookçš„æ–¹å¼åœ¨layer forwardåä½¿ç”¨calibration dataçš„inputæ¥ç”ŸæˆHessiançŸ©é˜µï¼Œè¿™ç§è®¡ç®—æ–¹å¼å¸¸è§äºé‡åŒ–æµç¨‹ä¸­æ ¡å‡†æ•°æ®çš„å¤„ç†
```python
def add_batch(name):
    def tmp(_, inp, out):
        # å‡è®¾è¿‡ç¨‹ä¸ºï¼šx â†’ Linear(W) â†’ ReLU
        # x â†’inp[0].data Linearå±‚è¾“å‡ºâ†’out
        gptq[name].add_batch(inp[0].data, out.data)
    return tmp
handles = []
# æ·»åŠ hook
for name in subset:
    handles.append(subset[name].register_forward_hook(add_batch(name)))
# å¤„ç†æ ·æœ¬è®¡ç®—æ•°æ®
for j in range(args.nsamples):
    outs[j] = layer(inps[j].unsqueeze(0), attention_mask=attention_mask, position_ids=position_ids)[0]
# å»é™¤hook
for h in handles:
    h.remove()
```
åœ¨`add_batch`ä¸­å…·ä½“ä¸ºäº†åˆ©ç”¨æ‰€æœ‰çš„æ ¡å‡†æ•°æ®ï¼Œè¿™é‡Œé€šè¿‡è¿­ä»£çš„æ–¹å¼å°†æ¯ç»„æ•°æ®è®¡ç®—çš„HessiançŸ©é˜µå€¼è¿›è¡Œæ±‚å’Œç„¶åå–å¹³å‡ï¼Œä»£ç å®ç°æ˜¯è¿­ä»£é€æ¸å¹³å‡å åŠ çš„è¿‡ç¨‹ï¼ŒHessiançŸ©é˜µæ±‚è§£å…¬å¼ï¼š$H_F=2X_FX_F^T$
```python
# å‡è®¾è¿‡ç¨‹ä¸ºï¼šx â†’ Linear(W) â†’ ReLU
# x â†’inp[0].data Linearå±‚è¾“å‡ºâ†’out
#gptq[name].add_batch(inp[0].data, out.data)
def add_batch(self, inp, out):
    ...
    if len(inp.shape) == 2:
        inp = inp.unsqueeze(0)
    tmp = inp.shape[0]
    if isinstance(self.layer, nn.Linear) or isinstance(self.layer, transformers.Conv1D):
        if len(inp.shape) == 3:
            inp = inp.reshape((-1, inp.shape[-1]))
        inp = inp.t()
    if isinstance(self.layer, nn.Conv2d):
        unfold = nn.Unfold(
            self.layer.kernel_size,
            dilation=self.layer.dilation,
            padding=self.layer.padding,
            stride=self.layer.stride
        )
        inp = unfold(inp)
        inp = inp.permute([1, 0, 2])
        inp = inp.flatten(1)
    self.H *= self.nsamples / (self.nsamples + tmp)
    self.nsamples += tmp
    inp = math.sqrt(2 / self.nsamples) * inp.float()
    self.H += inp.matmul(inp.t())
```
**å…¶æ¬¡**ã€é€å±‚weighté‡åŒ–
```python
for name in subset:
    gptq[name].fasterquant(
        percdamp=args.percdamp, groupsize=args.groupsize, actorder=args.act_order, static_groups=args.static_groups
    )
    quantizers['model.layers.%d.%s' % (i, name)] = gptq[name].quantizer
    gptq[name].free()
```
ä¸»è¦æ˜¯é€šè¿‡é€å±‚ä½¿ç”¨`fasterquant`æ–¹æ³•ä½œä¸ºå…¥å£æ¥è¿›è¡Œé‡åŒ–å¤„ç†ã€‚`fasterquant` ç”¨å±‚çš„æƒé‡çŸ©é˜µ W å’Œä¹‹å‰æ”¶é›†åˆ°çš„æ¿€æ´» Gramï¼ˆæˆ–è¿‘ä¼¼ Hessianï¼‰H æ¥åšæŒ‰åˆ—ï¼ˆæŒ‰ blockï¼‰è´ªå¿ƒé‡åŒ–ã€‚å®ƒå…ˆæŠŠ H ç»è¿‡é˜»å°¼å¹¶é€šè¿‡ Cholesky/é€†æ“ä½œå¾—åˆ°ç”¨äºæŠ•å½±/è¡¥å¿çš„å› å­ï¼ˆç§°ä¸º Hinvï¼‰ï¼Œç„¶åæŒ‰ block å†…é€åˆ—é‡åŒ–ï¼šå¯¹ç¬¬ j åˆ—é‡åŒ–åè®¡ç®—è¯¯å·® e_jï¼Œç”¨ Hinv çš„ç›¸åº”è¡Œ/åˆ—æŠŠè¿™ä¸ªè¯¯å·®æŒ‰ Schur è¡¥æ–¹å¼æŠ•å½±/ä¼ æ’­åˆ°è¯¥ block å†…å‰©ä½™åˆ—å¹¶åœ¨ block å¤–ä¸€æ¬¡æ€§ä¼ æ’­åˆ°åç»­åˆ—ï¼Œä»è€Œå®ç° GPTQ çš„è¯¯å·®è¡¥å¿ç­–ç•¥ã€‚åœ¨`fasterquant`æ–¹æ³•ä¸­ä¸»è¦è¿›è¡Œäº†é‡åŒ–çš„è®¡ç®—è¿‡ç¨‹ï¼Œå…·ä½“å®ç°è¿‡ç¨‹ä¸ºï¼ˆæ ¸å¿ƒä»£ç ï¼‰ï¼š
```python
def fasterquant(
    self, blocksize=128, percdamp=.01, groupsize=-1, actorder=False, static_groups=False
):
    W = self.layer.weight.data.clone()
    if isinstance(self.layer, nn.Conv2d):
        W = W.flatten(1)
    if isinstance(self.layer, transformers.Conv1D):
        W = W.t()
    W = W.float()

    tick = time.time()

    if not self.quantizer.ready():
        self.quantizer.find_params(W, weight=True)

    # self.H æ˜¯ä¸Šä¸€æ­¥ä¸­è®¡ç®—å¾—åˆ°çš„HessiançŸ©é˜µ
    H = self.H
    del self.H
    dead = torch.diag(H) == 0
    H[dead, dead] = 1
    W[:, dead] = 0

    ...
    # åˆå§‹åŒ– losses 0çŸ©é˜µ
    Losses = torch.zeros_like(W)
    Q = torch.zeros_like(W)

    damp = percdamp * torch.mean(torch.diag(H))
    diag = torch.arange(self.columns, device=self.dev)
    H[diag, diag] += damp
    H = torch.linalg.cholesky(H)
    H = torch.cholesky_inverse(H)
    H = torch.linalg.cholesky(H, upper=True)
    Hinv = H
    # é€Blockå¤„ç†
    # self.columns = W.shape[1]
    for i1 in range(0, self.columns, blocksize):
        i2 = min(i1 + blocksize, self.columns)
        count = i2 - i1

        W1 = W[:, i1:i2].clone()
        Q1 = torch.zeros_like(W1)
        Err1 = torch.zeros_like(W1)
        Losses1 = torch.zeros_like(W1)
        Hinv1 = Hinv[i1:i2, i1:i2]
        # Blockå†…éƒ¨é‡åŒ–
        for i in range(count):
            w = W1[:, i]
            d = Hinv1[i, i]

            if groupsize != -1:
                if not static_groups:
                    if (i1 + i) % groupsize == 0:
                        self.quantizer.find_params(W[:, (i1 + i):(i1 + i + groupsize)], weight=True)
                else:
                    idx = i1 + i
                    if actorder:
                        idx = perm[idx]
                    self.quantizer = groups[idx // groupsize]

            q = quantize(
                w.unsqueeze(1), self.quantizer.scale, 
                self.quantizer.zero, self.quantizer.maxq
            ).flatten()
            Q1[:, i] = q
            Losses1[:, i] = (w - q) ** 2 / d ** 2

            err1 = (w - q) / d
            W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))
            Err1[:, i] = err1

        Q[:, i1:i2] = Q1
        Losses[:, i1:i2] = Losses1 / 2

        W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])
    torch.cuda.synchronize()
    ...

    if actorder:
        Q = Q[:, invperm]

    if isinstance(self.layer, transformers.Conv1D):
        Q = Q.t()
    self.layer.weight.data = Q.reshape(self.layer.weight.shape).to(self.layer.weight.data.dtype)
```
å¯¹äºä¸Šé¢è¿‡ç¨‹ä¸»è¦æ˜¯çœ‹ä¸¤ä¸ªforå¾ªç¯çš„é‡Œé¢å†…å®¹ï¼Œé¦–å…ˆç¬¬ä¸€ä¸ªforå¾ªç¯å»æ ¹æ®blockå»å°†æƒé‡çŸ©é˜µWè¿›è¡Œ**åˆ†å—æ‹†åˆ†**ï¼ˆ`W1 = W[:, i1:i2].clone()`ï¼‰ï¼Œæ¥ä¸‹æ¥ç¬¬äºŒä¸ªforå¾ªç¯ä¾æ¬¡å»å¯¹ç¬¬1å—ä¸­æ¯åˆ—è¿›è¡Œé‡åŒ–ï¼Œç¬¬iåˆ—è¿›è¡Œé‡åŒ–ï¼ˆ`quantize`ï¼‰å¤„ç†ï¼ˆ`q = quantize(...)`ï¼‰ï¼Œè€Œåå»è®¡ç®—losså¹¶ä¸”å»å¯¹å…¶ä»–çš„åˆ—ï¼ˆ`i:`ï¼‰è®¡ç®—`W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))`ï¼Œåœ¨å¤„ç†å®Œæ¯•ç¬¬1å—ä¹‹åå†å»å°†åé¢å—çš„åˆ—è¿›è¡Œ**è¯¯å·®è¡¥å¿**ï¼ˆ`W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])`ï¼‰ï¼Œè¿™æ ·æ•´ä¸ªè¿‡ç¨‹å°±å®Œæˆäº†ã€‚
```python
# é‡åŒ–å‡½æ•°
def quantize(x, scale, zero, maxq):
    if maxq < 0:
        return (x > scale / 2).float() * scale + (x < zero / 2).float() * zero
    q = torch.clamp(torch.round(x / scale) + zero, 0, maxq)
    return scale * (q - zero)
```
**æœ€å**ã€é‡åŒ–æ¨¡å‹ä¿å­˜
ä¹‹å‰çš„æ­¥éª¤ä¸­é‡åŒ–å’Œåé‡åŒ–åè®¡ç®—loseéƒ½æ˜¯æµ®ç‚¹ä½æ•°çš„ï¼Œæ‰€ä»¥å¹¶æ²¡æœ‰ç”Ÿæˆwbitä½formatçš„æ•°å€¼å†…å®¹ï¼Œåœ¨llama_packæ–¹æ³•ä¸­é€šè¿‡modelå’Œä¹‹å‰å¾—åˆ°çš„quantizer(scale, zero)æ¥ç”Ÿæˆwbitä½æ•°è¡¨è¾¾æ ¼å¼çš„é‡åŒ–æ¨¡å‹ï¼Œå…¶å®šä¹‰å¦‚ä¸‹æ‰€ç¤º
```python
def llama_pack3(model, quantizers):
    layers = find_layers(model)
    layers = {n: layers[n] for n in quantizers}
    make_quant3(model, quantizers)
    qlayers = find_layers(model, [Quant3Linear])
    for name in qlayers:
        quantizers[name] = quantizers[name].cpu()
        # ä½¿ç”¨ Quant3Linear è¿›è¡Œpackå¤„ç†
        qlayers[name].pack(layers[name], quantizers[name].scale, quantizers[name].zero)
    return model
# å°†modelä¸­æ¯ä¸€å±‚éƒ½æ›¿æ¢ä¸º Quant3Linear
def make_quant3(module, names, name='', faster=False):
    if isinstance(module, Quant3Linear):
        return
    for attr in dir(module):
        tmp = getattr(module, attr)
        name1 = name + '.' + attr if name != '' else attr
        if name1 in names:
            setattr(module, attr, Quant3Linear(tmp.in_features, tmp.out_features, faster=faster))
    for name1, child in module.named_children():
        make_quant3(child, names, name + '.' + name1 if name != '' else name1, faster=faster)
...
if args.wbits < 16 and not args.nearest:
    quantizers = llama_sequential(model, dataloader, DEV)
if args.save:
    llama_pack3(model, quantizers)
```
å…¶ä¸­quantizersæ¥è‡ªé‡åŒ–åçš„è¿”å›ï¼Œå®ƒæ˜¯ä¸€ä¸ªdicté‡Œé¢ä¿å­˜äº†æ¯ä¸€ä¸ªå±‚å’Œå®ƒå¯¹åº”çš„quantizerã€scaleã€zeroã€group_idxç­‰ä¿¡æ¯ï¼Œå…¶ä¸­quantizeræ˜¯layer-levelçš„ï¼Œzeroå’Œscaleæ˜¯group-levelçš„ã€‚
> quantizersçš„ç»“æœä¸ºï¼š`quantizers['model.layers.%d.%s' % (i, name)] = (gptq[name].quantizer.cpu(), scale.cpu(), zero.cpu(), g_idx.cpu(), args.wbits, args.groupsize)`

Quant3Linearå…·ä½“å¤„ç†è¿‡ç¨‹ï¼ˆ[ä»£ç ](https://github.com/IST-DASLab/gptq/blob/2d65066eeb06a5c9ff5184d8cebdf33662c67faf/quant.py#L137C1-L200C68)ï¼‰ï¼Œé€šè¿‡qweightã€zeroså’Œscalesã€biasç­‰å±æ€§æ¥ä¿å­˜é‡åŒ–åçš„ä½æ¯”ç‰¹ä¿¡æ¯ã€‚ï¼š
```python
# qlayers[name].pack(layers[name], quantizers[name].scale, quantizers[name].zero)
class Quant3Linear(nn.Module): 
    def __init__(self, infeatures, outfeatures, faster=False):
        super().__init__()
        self.register_buffer('zeros', torch.zeros((outfeatures, 1)))
        self.register_buffer('scales', torch.zeros((outfeatures, 1)))
        self.register_buffer('bias', torch.zeros(outfeatures))
        self.register_buffer(
            'qweight', torch.zeros((infeatures // 32 * 3, outfeatures), dtype=torch.int)
        )
        self.faster = faster

    def pack(self, linear, scales, zeros):
        self.zeros = zeros * scales
        self.scales = scales.clone()
        if linear.bias is not None:
            self.bias = linear.bias.clone()

        intweight = torch.round((linear.weight.data + self.zeros) / self.scales).to(torch.int)
        intweight = intweight.t().contiguous()
        intweight = intweight.numpy().astype(np.uint32)
        qweight = np.zeros(
            (intweight.shape[0] // 32 * 3, intweight.shape[1]), dtype=np.uint32
        )

        i, row = 0, 0
        while row < qweight.shape[0]:
            # æŠŠ 32 ä¸ª 3-bit æ•´æ•°æŒ‰ä½è¿ç»­æ‰“åŒ…åˆ° 3 ä¸ª uint32
            ...
        qweight = qweight.astype(np.int32)
        self.qweight = torch.from_numpy(qweight) 
```
å¯¹äºä¸Šè¿°æ‰“åŒ…ï¼ˆ3-bitæ‰“åŒ…ï¼‰å¤„ç†è¿‡ç¨‹ä¸ºï¼š`qweight = np.zeros((intweight.shape[0] // 32 * 3, intweight.shape[1]), dtype=np.uint32)`æ¯ 32 ä¸ª intweight çš„è¡Œä½¿ç”¨ 3 ä¸ª uint32 è¡Œæ¥å­˜å‚¨ï¼Œä¸è¿‡å€¼å¾—æ³¨æ„çš„æ˜¯ä»¥ int32 çš„å½¢å¼å­˜å‚¨é‡åŒ–æƒé‡ï¼Œä½†è¿™ å¹¶ä¸ä»£è¡¨æ¯ä¸ªæƒé‡å  32 bitã€‚è¿™é‡Œçš„ int32 æ˜¯ä¸€ä¸ªæ‰“åŒ…å®¹å™¨ï¼ˆbit-packing containerï¼‰ï¼Œé‡Œé¢å¡äº†å¤šä¸ªä½ bitï¼ˆæ¯”å¦‚ 3 bitï¼‰çš„æƒé‡å€¼ã€‚
### AWQé‡åŒ–æŠ€æœ¯
AWQé‡åŒ–[^5]ï¼ˆé€å±‚é‡åŒ–æ–¹æ³•ï¼Œéœ€è¦æ¯å±‚çš„è¾“å…¥æ¿€æ´»æ¥è®¡ç®— scale å’Œ clip å€¼ï¼‰æ˜¯ä¸€ç§åŸºäº**æ¿€æ´»å€¼åˆ†å¸ƒæŒ‘é€‰æ˜¾è‘—æƒé‡**è¿›è¡Œé‡åŒ–çš„æ–¹æ³•ï¼Œå…¶ä¸ä¾èµ–äºä»»ä½•åå‘ä¼ æ’­æˆ–é‡å»ºï¼Œå› æ­¤å¯ä»¥å¾ˆå¥½åœ°ä¿æŒLLMåœ¨ä¸åŒé¢†åŸŸå’Œæ¨¡å¼ä¸Šçš„æ³›åŒ–èƒ½åŠ›ï¼Œè€Œä¸ä¼šè¿‡æ‹Ÿåˆåˆ°æ ¡å‡†é›†ï¼Œå±è®­ç»ƒåé‡åŒ–å¤§ç±»ï¼Œè®ºæ–‡é‡Œé¢å‡ºå‘ç‚¹å°±æ˜¯æ¨¡å‹çš„æƒé‡å¹¶ä¸åŒç­‰é‡è¦ï¼Œ**ä»…æœ‰0.1%-1%çš„å°éƒ¨åˆ†æ˜¾è‘—æƒé‡å¯¹æ¨¡å‹è¾“å‡ºç²¾åº¦å½±å“è¾ƒå¤§**ã€‚å› æ­¤å¦‚æœèƒ½æœ‰åŠæ³•åªå¯¹0.1%~1%è¿™ä¸€å°éƒ¨åˆ†æƒé‡ä¿æŒåŸæ¥çš„ç²¾åº¦(FP16)ï¼Œå¯¹å…¶ä»–æƒé‡è¿›è¡Œä½æ¯”ç‰¹é‡åŒ–ï¼Œå°±å¯ä»¥åœ¨ä¿æŒç²¾åº¦å‡ ä¹ä¸å˜çš„æƒ…å†µä¸‹ï¼Œå¤§å¹…é™ä½æ¨¡å‹å†…å­˜å ç”¨ï¼Œå¹¶æå‡æ¨ç†é€Ÿåº¦ã€‚
![](https://s2.loli.net/2025/10/14/PGa2p3Ac9xCTD7I.webp)
ä½†æ˜¯å¦‚æœéƒ¨åˆ†ç”¨FP16è€Œå…¶ä»–çš„ç”¨INT3è¿™æ ·å°±ä¼šå¯¼è‡´ç¡¬ä»¶ä¸Šå­˜å‚¨å›°éš¾ï¼ˆå›¾bæƒ…å†µï¼‰ï¼Œå› æ­¤ä½œè€…ä½¿ç”¨çš„æ“ä½œå°±æ˜¯ï¼š**å¯¹æ‰€æœ‰æƒé‡å‡è¿›è¡Œä½æ¯”ç‰¹é‡åŒ–ï¼Œä½†æ˜¯ï¼Œåœ¨é‡åŒ–æ—¶ï¼Œå¯¹äºæ˜¾è‘—æƒé‡ä¹˜ä»¥è¾ƒå¤§çš„scaleï¼Œç›¸å½“äºé™ä½å…¶é‡åŒ–è¯¯å·®ï¼›åŒæ—¶ï¼Œå¯¹äºéæ˜¾è‘—æƒé‡ï¼Œä¹˜ä»¥è¾ƒå°çš„scaleï¼Œç›¸å½“äºç»™äºˆæ›´å°‘çš„å…³æ³¨**ã€‚å› æ­¤ä»£ç å…³æ³¨ç‚¹å°±æ˜¯æ‰¾åˆ°è¿™ä¸ªscaleå€¼
> **åŸºäºæ¿€æ´»å€¼åˆ†å¸ƒæŒ‘é€‰æ–¹æ³•**ï¼š**æ¿€æ´»å€¼æŒ‡çš„æ˜¯**ä¸æƒé‡çŸ©é˜µè¿ç®—çš„è¾“å…¥å€¼ï¼Œæ¯”å¦‚è¯´ï¼š$V=W_vX$å…¶ä¸­çš„ $X$å°±æ˜¯æƒé‡ $W_v$çš„æ¿€æ´»å€¼ï¼ŒæŒ‰æ¿€æ´»å€¼ç»å¯¹å€¼å¤§å°ç”±å¤§åˆ°å°æ’åºï¼Œç»å¯¹å€¼è¶Šå¤§è¶Šæ˜¾è‘—ï¼Œé€‰æ‹©å‰0.1%~1%çš„å…ƒç´ ä½œä¸ºæ˜¾è‘—æƒé‡ã€‚
> å…·ä½“ä»£ç è¿‡ç¨‹ï¼ˆ[Github-Code](https://github.com/mit-han-lab/llm-awq/blob/d6e797a42b9ef7778de8ee2352116e0f48a78d61/awq/quantize/pre_quant.py#L102)ï¼‰

é¦–å…ˆæ˜¯è·å– æ¨¡å‹ç¬¬ä¸€å±‚çš„è¾“å…¥æ¿€æ´»å€¼ï¼Œä¾›åç»­çš„é€å±‚é‡åŒ–ä½¿ç”¨ï¼Œä»£ç æ•´ä½“æµç¨‹å¦‚ä¸‹ï¼ˆæ ¸å¿ƒä»£ç æ ¼å¼ï¼‰ï¼š
```python
@torch.no_grad()
def run_awq(model,enc,w_bit,q_config,n_samples=512,seqlen=512,auto_scale=True,mse_range=True,calib_data="pileval",):
    ...

    layers = get_blocks(model)
    samples = get_calib_dataset(...)
    # å¾—åˆ°ç¬¬ä¸€å±‚çš„æ¿€æ´»å€¼
    inps = []
    layer_kwargs = {}

    layers[0] = layers[0].cuda()
    ...
    class Catcher(nn.Module):
        def __init__(self, module):
            super().__init__()
            self.module = module

        def forward(self, inp, **kwargs):
            inps.append(inp)
            layer_kwargs.update(kwargs)
            raise ValueError
    layers[0] = Catcher(layers[0])
    try:
        if model.__class__.__name__ == "LlavaLlamaModel":
            model.llm(samples.to(next(model.parameters()).device))
        ...
    except ValueError:
        pass
    ...
    layers[0] = layers[0].module
    inps = inps[0]
    layers[0] = layers[0].cpu()
    ...
```
**è€Œå**ã€é€å±‚è¿›è¡Œé‡åŒ–å¤„ç†ï¼Œåœ¨AWQé‡åŒ–è¿‡ç¨‹ä¸­éœ€è¦è®°å½•ä¸¤éƒ¨åˆ†é‡åŒ–å€¼`scale`ï¼ˆ[auto_sclae.py](https://github.com/mit-han-lab/llm-awq/blob/main/awq/quantize/auto_scale.py)ï¼‰ å’Œ `clip`ï¼ˆ[auto_clip.py](https://github.com/mit-han-lab/llm-awq/blob/main/awq/quantize/auto_clip.py)ï¼‰ä¸¤éƒ¨åˆ†å…·ä½“æºç å¤„ç†è¿‡ç¨‹éƒ½æ˜¯ç›¸ä¼¼çš„å…ˆå»è®¡ç®—scaleå€¼è€Œåå°†scaleå€¼åº”ç”¨ï¼Œåœ¨è®¡ç®—ä¸¤éƒ¨åˆ†å€¼ä¹‹å‰å’ŒGPTQå¤„ç†ç›¸ä¼¼å»è®°å½•forwardè¿‡ç¨‹ï¼Œå…·ä½“ä»£ç ä¸ºï¼š
```python
for i in tqdm.tqdm(range(len(layers))):
    layer = layers[i]
    layer = layer.cuda()
    named_linears = get_named_linears(layer)

    def cache_input_hook(m, x, y, name, feat_dict):
        x = x[0]
        x = x.detach().cpu()
        feat_dict[name].append(x)
    input_feat = defaultdict(list)
    handles = []
    for name in named_linears:
        handles.append(
            named_linears[name].register_forward_hook(
                functools.partial(cache_input_hook, name=name, feat_dict=input_feat)
            )
        )
    inps = inps.to(next(layer.parameters()).device)
    inps = layer(inps, **layer_kwargs)[0]
    for h in handles:
        h.remove()
    input_feat = {k: torch.cat(v, dim=0) for k, v in input_feat.items()}
```
å…¶ä¸­cache_input_hookè¿‡ç¨‹å°±æ˜¯ç›´æ¥è®°å½•æ¯å±‚layerä¸­çš„linearå±‚çš„è¾“å…¥å€¼å¹¶ä¸”å°†å…¶è®°å½•åˆ°input_featä¸­ã€‚
**scaleå¤„ç†è¿‡ç¨‹**ä»£ç å¦‚ä¸‹ï¼š
```python
elif isinstance(module, (LlamaDecoderLayer, Qwen2DecoderLayer)):
    # attention input
    scales_list.append(
        _auto_get_scale(
            prev_op=module.input_layernorm,
            layers=[
                module.self_attn.q_proj,
                module.self_attn.k_proj,
                module.self_attn.v_proj,
            ],
            inp=input_feat["self_attn.q_proj"],
            module2inspect=module.self_attn,
            kwargs=module_kwargs,
        )
    )
'''
_auto_get_scale ä¸­æ ¸å¿ƒé€»è¾‘æ˜¯ä½¿ç”¨ search_module_scale å¹¶ä¸”å…¶ä¸­4ä¸ªå‚æ•°åˆ†åˆ«å¯¹åº”
block = module2inspect=module.self_attn 
linears2scale = [module.self_attn.q_proj, module.self_attn.k_proj, module.self_attn.v_proj]
x = input_feat["self_attn.q_proj"]
'''
def _search_module_scale(block, linears2scale: list, x, kwargs={}):
    # blockï¼šå¯¹åº”block linears2scaleï¼šå¯¹åº”çº¿æ€§å±‚
    x = x.to(next(block.parameters()).device)
    # è®°å½•æœªé‡åŒ–çš„è¾“å‡ºç»“æœ
    with torch.no_grad():
        org_out = block(x, **kwargs)
        ...

    x_max = get_act_scale(x) # x.abs().view(-1, x.shape[-1]).mean(0)

    best_error = float("inf")
    best_ratio = -1
    best_scales = None

    n_grid = 20
    history = []

    org_sd = {k: v.cpu() for k, v in block.state_dict().items()}
    for ratio in range(n_grid):
        ratio = ratio * 1 / n_grid
        scales = x_max.pow(ratio).clamp(min=1e-4).view(-1)
        scales = scales / (scales.max() * scales.min()).sqrt()

        for fc in linears2scale:
            fc.weight.mul_(scales.view(1, -1).to(fc.weight.device))
            fc.weight.data = w_quantize_func(fc.weight.data) / (scales.view(1, -1))
        out = block(x, **kwargs)
        if isinstance(out, tuple):
            out = out[0]

        loss = ((org_out - out).float().pow(2).mean().item())
        history.append(loss)
        is_best = loss < best_error
        if is_best:
            best_error = loss
            best_ratio = ratio
            best_scales = scales
        # æ¢å¤åˆ°æœ€åˆçŠ¶æ€
        block.load_state_dict(org_sd)
    ...
    best_scales = best_scales.view(-1)
    ...
    return best_scales.detach()
```
> **å¯¹æ‰€æœ‰æƒé‡å‡è¿›è¡Œä½æ¯”ç‰¹é‡åŒ–ï¼Œä½†æ˜¯ï¼Œåœ¨é‡åŒ–æ—¶ï¼Œå¯¹äºæ˜¾è‘—æƒé‡ä¹˜ä»¥è¾ƒå¤§çš„scaleï¼Œç›¸å½“äºé™ä½å…¶é‡åŒ–è¯¯å·®ï¼›åŒæ—¶ï¼Œå¯¹äºéæ˜¾è‘—æƒé‡ï¼Œä¹˜ä»¥è¾ƒå°çš„scaleï¼Œç›¸å½“äºç»™äºˆæ›´å°‘çš„å…³æ³¨**

å…¶å®å¯¹äºä¸Šé¢è¿‡ç¨‹å°±æ˜¯ç›´æ¥é€šè¿‡ç½‘æ ¼æœç´¢ç­–ç•¥é€šè¿‡å¾—åˆ°çš„`x_max=x.abs().view(-1, x.shape[-1]).mean(0)`å»ä¸æ–­å°è¯•scaleså»è®©lossæœ€å°ï¼Œä»è€Œå¾—åˆ°scaleå€¼ã€‚å¯¹äºå…¶ä¸­çš„é‡åŒ–å¤„ç†è¿‡ç¨‹[w_quantize_func](https://github.com/mit-han-lab/llm-awq/blob/d6e797a42b9ef7778de8ee2352116e0f48a78d61/awq/quantize/quantizer.py#L61)ï¼Œæ ¸å¿ƒæ˜¯è®¡ç®— $q=clip(round(\frac{w}{s}â€‹)+z,q_{min}â€‹,q_{max}â€‹)$ï¼š
```python
'''
w_quantize_func(fc.weight.data) / (scales.view(1, -1))
w å¯¹åº” fc.weight.data) / (scales.view(1, -1)
'''
def pseudo_quantize_tensor(w, n_bit=8, zero_point=True, q_group_size=-1, inplace=False, get_scale_zp=False):
    org_w_shape = w.shape
    if q_group_size > 0:
        assert org_w_shape[-1] % q_group_size == 0
        w = w.reshape(-1, q_group_size)
    assert w.dim() == 2
    if zero_point: 
        max_val = w.amax(dim=1, keepdim=True)
        min_val = w.amin(dim=1, keepdim=True)
        max_int = 2**n_bit - 1
        min_int = 0
        scales = (max_val - min_val).clamp(min=1e-5) / max_int
        zeros = (-torch.round(min_val / scales)).clamp_(min_int, max_int)
    else:  ... # å¯¹ç§°é‡åŒ–
    ...
    if inplace:...
    else:
        w = (
            torch.clamp(torch.round(w / scales) + zeros, min_int, max_int) - zeros
        ) * scales
    assert torch.isnan(w).sum() == 0
    w = w.reshape(org_w_shape)
    if get_scale_zp:...
    else:
        return w
```
å¯¹äºä¸Šé¢è¿‡ç¨‹æ€»ç»“å°±æ˜¯ï¼šæŠŠ w çº¿æ€§æ˜ å°„åˆ°ä¸€ä¸ªç”± bit ä½æ•°ï¼ˆn_bitï¼‰å†³å®šçš„å›ºå®šæ•´æ•°åŒºé—´ï¼ˆq_min åˆ° q_maxï¼‰ï¼Œå…¶ä¸­scale å†³å®šç¼©æ”¾æ¯”ä¾‹ï¼Œzero_point å†³å®šæ˜ å°„åç§»
## æ€»ç»“
GPTQé‡åŒ–æŠ€æœ¯æ€»ç»“ï¼šæ ¸å¿ƒæµç¨‹å…¶å®å°±æ˜¯**é‡åŒ–-è¡¥å¿-é‡åŒ–-è¡¥å¿çš„è¿­ä»£**ï¼Œé¦–å…ˆé€šè¿‡å¯¹æ¨¡å‹æƒé‡$W$é¦–å…ˆå»å¯¹$W$è¿›è¡Œ**åˆ†å—æ‹†åˆ†**å¾—åˆ°ä¸åŒçš„blockå†å»åˆ°æ¯ä¸€ä¸ªblocké‡Œé¢å»æŒ‰ç…§æ¯iåˆ—è¿›è¡Œé‡åŒ–ï¼ˆ`quantize`ï¼‰å¤„ç†ï¼ˆ`q = quantize(...)`ï¼‰ï¼Œè€Œåå»è®¡ç®—losså¹¶ä¸”å»å¯¹å…¶ä»–çš„åˆ—ï¼ˆ`i:`ï¼‰è®¡ç®—`W1[:, i:] -= err1.unsqueeze(1).matmul(Hinv1[i, i:].unsqueeze(0))`ï¼Œåœ¨å¤„ç†å®Œæ¯•ç¬¬1å—ä¹‹åå†å»å°†åé¢å—çš„åˆ—è¿›è¡Œè¯¯å·®è¡¥å¿ï¼ˆ`W[:, i2:] -= Err1.matmul(Hinv[i1:i2, i2:])`ï¼‰ï¼Œè¿™æ ·å°±å¾—åˆ°äº†scales, zerosè¿™ä¿¡æ¯ï¼Œåœ¨å»ä½¿ç”¨è¿™äº›ä¿¡æ¯å»å¯¹æ¨¡å‹æƒé‡è¿›è¡Œè½¬åŒ–`intweight = torch.round((linear.weight.data + self.zeros) / self.scales).to(torch.int)`ï¼Œæœ€åå°±æ˜¯ç”¨32 ä¸ªintweightçš„è¡Œä½¿ç”¨ 3 ä¸ª uint32 è¡Œæ¥å­˜å‚¨ï¼Œæ¨ç†è¿‡ç¨‹çš„è¯ï¼š$y = Wx + b\rightarrow yâ‰ˆx(s_j(q-z_j))+b$
AWQé‡åŒ–æŠ€æœ¯æ€»ç»“ï¼šæ ¸å¿ƒæµç¨‹å°±æ˜¯**å¯¹æ‰€æœ‰æƒé‡å‡è¿›è¡Œä½æ¯”ç‰¹é‡åŒ–ï¼Œä½†æ˜¯ï¼Œåœ¨é‡åŒ–æ—¶ï¼Œå¯¹äºæ˜¾è‘—æƒé‡ä¹˜ä»¥è¾ƒå¤§çš„scaleï¼Œç›¸å½“äºé™ä½å…¶é‡åŒ–è¯¯å·®ï¼›åŒæ—¶ï¼Œå¯¹äºéæ˜¾è‘—æƒé‡ï¼Œä¹˜ä»¥è¾ƒå°çš„scaleï¼Œç›¸å½“äºç»™äºˆæ›´å°‘çš„å…³æ³¨**ï¼Œå¯¹äºè¿™ä¸ªscaleå€¼çš„å¯»æ‰¾ç›´æ¥è®¡ç®—æ¯ä¸€å±‚çš„è¾“å…¥â€œæ¿€æ´»å€¼â€ï¼ˆ`x.abs().view(-1, x.shape[-1]).mean(0)`ï¼‰è€Œåå¯¹è¿™ä¸ªæ¿€æ´»å€¼ä¸æ–­è¿›è¡Œscaleå¤„ç†å°†å…¶é€šè¿‡`w_quantize_func`æ“ä½œåº”ç”¨åˆ°æ¨¡å‹çš„å±‚ä¸Šè¿›è€Œå¾—åˆ°é‡åŒ–åçš„æ¨¡å‹æƒé‡ï¼Œç„¶åå»è®¡ç®—å’Œæ²¡æœ‰é‡åŒ–çš„æƒé‡losså¾—åˆ°æœ€ä½³scale
## ä»£ç æ“ä½œ
> [Github-code](https://github.com/shangxiaaabb/ProjectCode/blob/main/code/Python/DFModelCode/DF_acceralate/quant_LLM.ipynb)
> [æ¨¡å‹ONNXéƒ¨ç½²æŠ€æœ¯](https://github.com/shangxiaaabb/ProjectCode/blob/main/code/Python/ONNX_TensoRT/ModelDeployment.ipynb)

ç›´æ¥ä½¿ç”¨`llmcompressor`æ¥é‡åŒ–æ¨¡å‹ï¼ˆå…·ä½“åœ°å€ï¼š[llmcompressor](https://docs.vllm.ai/projects/llm-compressor/en/latest/getting-started/install/#prerequisites)ï¼‰æ”¯æŒé‡åŒ–ç±»å‹ï¼š
![](https://s2.loli.net/2025/11/11/KUrHF4IiLC7WaXf.webp)
**æ¨èè¿›ä¸€æ­¥é˜…è¯»**ï¼š[https://www.big-yellow-j.top/posts/2025/12/29/SDAcceralate.html](https://www.big-yellow-j.top/posts/2025/12/29/SDAcceralate.html)
## å‚è€ƒ
[^1]: [https://github.com/IST-DASLab/gptq](https://github.com/IST-DASLab/gptq)
[^2]: [https://zhuanlan.zhihu.com/p/646210009](https://zhuanlan.zhihu.com/p/646210009)
[^3]: [https://zhuanlan.zhihu.com/p/629517722](https://zhuanlan.zhihu.com/p/629517722)
[^4]: [https://zhuanlan.zhihu.com/p/697860995](https://zhuanlan.zhihu.com/p/697860995)
[^5]: [https://arxiv.org/pdf/2306.00978](https://arxiv.org/pdf/2306.00978)