---
layout: mypost
title: å¤šæ¨¡æ€æ¨¡å‹â€”â€”QwenVL2.5çš„å¾®è°ƒä»¥åŠå¼ºåŒ–å­¦ä¹ ä»£ç æ“ä½œ
categories: å¤šæ¨¡æ€
address: æ­¦æ±‰ğŸ¯
extMath: true
show_footer_image: true
tags:
- å¤šæ¨¡æ€
- QwenVL
description: æœ¬æ–‡è¯¦ç»†è§£æQwenVL2.5æ¨¡å‹çš„å¤„ç†æµç¨‹åŠå¾®è°ƒæ–¹æ³•ï¼ŒåŒ…æ‹¬æ¨¡æ¿åŒ–è¾“å…¥ï¼ˆé€šè¿‡processor.apply_chat_templateå¤„ç†å¯¹è¯messagesï¼Œå«<|im_start|>ç­‰æ ‡è®°æ¨¡æ‹Ÿç”¨æˆ·/assistantå¯¹è¯ï¼‰ã€ç¼–ç è¾“å…¥ï¼ˆå›¾åƒå¤„ç†é‡‡ç”¨smart_resizeåŠ¨æ€è°ƒæ•´åˆ†è¾¨ç‡ç¡®ä¿å¯è¢«patch_sizeæ•´é™¤ï¼Œç»å½’ä¸€åŒ–åè½¬ä¸ºVitçš„patchåºåˆ—ï¼›æ–‡æœ¬é€šè¿‡tokenizerç¼–ç ï¼‰ã€æ¨¡å‹å¤„ç†ï¼ˆè§†è§‰Transformerå¯¹pixel_valuesè¿›è¡ŒConv3då¤„ç†ç”Ÿæˆç‰¹å¾ï¼Œç»“åˆwindow-attentionè®¡ç®—ï¼‰ã€‚åŒæ—¶ï¼Œé˜è¿°äº†SFTå¾®è°ƒæµç¨‹ï¼šæ•°æ®å±‚é¢æ„å»ºå¯¹è¯æ¨¡æ¿ç”Ÿæˆinput_idsã€pixel_valuesç­‰è¾“å…¥ï¼Œæ¨¡å‹å±‚é¢é‡‡ç”¨QLoRAä¼˜åŒ–å¹¶ç»“åˆgradient_checkpointingç­‰æ˜¾å­˜ä¼˜åŒ–ç­–ç•¥ã€‚å¼ºåŒ–å­¦ä¹ éƒ¨åˆ†æ¶µç›–DPOï¼ˆå¤„ç†ä¸‰å…ƒç»„æ•°æ®è®¡ç®—chosen/rejected_logpsï¼Œé€šè¿‡KLæ•£åº¦ç­‰è®¡ç®—lossï¼‰å’ŒGRPOï¼ˆæ— éœ€ref_modelï¼Œåˆ©ç”¨reward_functionåŠé«˜ç†µè¿‡æ»¤ä¼˜åŒ–lossï¼‰ï¼Œä¸ºQwenVL2.5-3Bçš„å®é™…åº”ç”¨ä¸æ€§èƒ½æå‡æä¾›æŠ€æœ¯æŒ‡å¯¼ã€‚
---

ä»ä»£ç è§’åº¦å»ç†è§£QwenVL2.5æ˜¯å¦‚ä½•å¤„ç†ï¼Œä»¥åŠç»“åˆå®é™…æ“ä½œç†è§£å¦‚ä½•å»å¯¹ä¸€ä¸ªQwenVL2.5-3Bè¿›è¡ŒSFTå’Œå¼ºåŒ–å­¦ä¹ å¤„ç†ã€‚
ç®€å•äº†è§£ä¸€ä¸‹QwenVL2.5æ¨¡å‹çš„æ•´ä¸ªå¤„ç†è¿‡ç¨‹ï¼Œæ¨¡å‹æ•´ä½“è¿‡ç¨‹å¤§è‡´ä¸ºï¼š1ã€é¦–å…ˆæ˜¯é€šè¿‡æ¨¡æ¿åŒ–å¤„ç†æˆ‘çš„æ¨¡å‹çš„è¾“å…¥ï¼ˆimage+textï¼‰ï¼›2ã€å°†è¾“å…¥è½¬åŒ–ä¸ºç¼–ç å½¢å¼ï¼ˆæ¯”å¦‚æ–‡æœ¬tokenizerå¤„ç†ç­‰ï¼‰ï¼›3ã€å‡ºå…¥æ¨¡å‹å¤„ç†è¾“å…¥ç„¶åæ¨¡å‹è¾“å‡ºï¼›4ã€è§£ç è¾“å‡ºå†…å®¹ã€‚æ•´ä½“ä¸»è¦æ˜¯ä¸Šè¿°4ä¸ªè¿‡ç¨‹ï¼Œå› æ­¤ä¸‹é¢é€ä¸€äº†è§£ä¸€ä¸‹æ¨¡å‹åˆ°åº•åœ¨åšä»€ä¹ˆã€‚
å†…å®¹è¾ƒå¤šå¯¹äºå¼ºåŒ–å­¦ä¹ éƒ¨åˆ†ä¹‹é—´çœ‹æœ€åçš„æ€»ç»“éƒ¨åˆ†å³å¯ï¼š
1ã€[trlæ¡†æ¶ä¸‹PPOä»£ç æ€»ç»“](https://www.big-yellow-j.top/posts/2025/08/29/QwenVLCode.html#:~:text=%E4%B8%80%E8%88%AC%E5%BE%97%E5%88%B0%E7%9A%84%E6%98%AF-,RL%2DPPO%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93,-RL%E7%AE%97%E6%B3%95%E5%AF%B9%E6%AF%94)ï¼›
2ã€[trlæ¡†æ¶ä¸‹DPOä»£ç æ€»ç»“](https://www.big-yellow-j.top/posts/2025/08/29/QwenVLCode.html#:~:text=%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93-,%E9%A6%96%E5%85%88,-%E5%AF%B9%E4%BA%8E%E6%88%91%E4%BB%AC%E7%9A%84)ï¼›
3ã€[trlæ¡†æ¶ä¸‹GRPOä»£ç æ€»ç»“](https://www.big-yellow-j.top/posts/2025/08/29/QwenVLCode.html#:~:text=%E6%9C%80%E5%90%8E%E7%9A%84%E5%80%BC%E3%80%82-,RL%2DGRPO%E5%A4%84%E7%90%86%E8%BF%87%E7%A8%8B%E6%80%BB%E7%BB%93,-%E5%AF%B9%E4%BA%8E%E4%B8%8A%E9%9D%A2loss)
## QwenVLçš„åŸºæœ¬ä½¿ç”¨
### 1ã€æ¨¡æ¿åŒ–æ¨¡å‹è¾“å…¥
```python
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
```
æ‰€è°“æ¨¡æ¿åŒ–æ¨¡å‹çš„è¾“å…¥ï¼Œå¾ˆå®¹æ˜“ç†è§£ï¼ˆé€šè¿‡`processor.apply_chat_template`æŠŠ**å¯¹è¯ messages è½¬æˆæ¨¡å‹èƒ½ç†è§£çš„ prompt**ï¼Œä¸è¿‡å€¼å¾—æ³¨æ„çš„æ˜¯ä¸åŒæ¨¡å‹å¯èƒ½å¤„ç†çš„æ–¹å¼ä¸åŒï¼‰ï¼Œå°±æ˜¯å°†æˆ‘çš„å†…å®¹â€œå¡«å……â€åˆ°æ¨¡æ¿ä¸­æ¨¡æ‹Ÿå¯¹è¯å†…å®¹ï¼Œæ¯”å¦‚è¯´ä¸Šé¢å¤„ç†å¾—åˆ°çš„ä¸€ä¸ªç®€å•ç»“æœå°±æ˜¯ï¼š
```python
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>
<|im_start|>assistant
```
ä¸€èˆ¬åœ¨**data_loaderé‡Œé¢å°±ä¼šæå‰å°†æˆ‘ä»¬çš„æ¨¡å‹éœ€è¦çš„è¾“å…¥å¤„ç†å¥½**ï¼Œæ¯”å¦‚è¯´æˆ‘ä»¬å®šä¹‰å¦‚ä¸‹çš„æ¨¡æ¿
```python
def format_data(self, image, text, prompt):
    # self.SYSTEM_MESSAGE = """You are a helpful assistant."""
    return [
        {
            "role": "system",
            "content": [{"type": "text", "text": self.SYSTEM_MESSAGE}],
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text",
                    "text": prompt
                },
            ],
        },
        {
            "role": "assistant",
            "content": [{"type": "text", "text": text}],
        },
    ]
""" 
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>This is a prompt<|im_end|>
<|im_start|>assistant
This is a text<|im_end|>
<|im_start|>assistant
"""

```
å¯¹äºä¸Šé¢å†…å®¹è¾“å‡ºç†è§£ï¼Œé¦–å…ˆ `<|im_start|>....<|im_end|>`ä¸€èˆ¬æ˜¯ä¸€ç»„â€œå‘è¨€â€çš„å¼€å§‹å’Œç»“æŸæ ‡è®°ï¼Œè€Œåé‡Œé¢å†…å®¹å°±æ˜¯æˆ‘ä»¬çš„æ–‡æœ¬/å›¾åƒå†…å®¹ï¼Œ`user`/ `assistant`/ `system` åˆ™æ˜¯åˆ†åˆ«ä»£è¡¨ï¼šç”¨æˆ·ã€æ¨¡å‹ã€è§’è‰²ï¼ˆå‘Šè¯‰æ¨¡å‹ä»Šå¤©æ‰®ä»€ä¹ˆè§’è‰²ï¼‰ã€‚`<|vision_start|>...<|vision_end|>`ï¼šè¡¨ç¤ºå›¾åƒè¾“å…¥çš„å ä½ç¬¦ï¼Œå‘Šè¯‰æ¨¡å‹è¿™é‡Œæœ‰ä¸€æ®µè§†è§‰ä¿¡æ¯ã€‚`<|image_pad|>`ï¼šå›¾åƒå®é™…çš„ embedding ä¼šåœ¨è¿™é‡Œæ›¿æ¢ï¼ˆå¡«å……ï¼‰ï¼Œä¸æ˜¯æ–‡å­—ï¼Œè€Œæ˜¯å›¾åƒç¼–ç åçš„å‘é‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ `assistant`åé¢çš„å†…å®¹å°±æ˜¯ **æ¨¡å‹éœ€è¦è¾“å‡ºçš„æ–‡æœ¬å†…å®¹**ã€‚ä¸Šé¢è¿‡ç¨‹å¾ˆå®¹æ˜“ç†è§£ï¼Œåªä¸è¿‡éœ€è¦æ³¨æ„å¦‚ä¸‹é—®é¢˜ï¼Œå› ä¸ºQwenVL2.5å¯¹äºåˆ†è¾¨ç‡æ˜¯å­˜åœ¨å¤„ç†ï¼ˆä¸€èˆ¬ç›´æ¥é€šè¿‡`smart_resize`å¤„ç†ï¼Œåç»­æœ‰ä»‹ç»ï¼‰ï¼Œå› æ­¤å¦‚æœæ¶‰åŠåˆ°ç›®æ ‡è¯†åˆ«ï¼Œå¯èƒ½éœ€è¦æå‰å°†åæ ‡è¿›è¡Œè½¬æ¢é¿å…åˆ†è¾¨ç‡ä¸åŒå¯¼è‡´bboxå¯¹åº”ä¸ä¸Šçš„é—®é¢˜

### 2ã€ç¼–ç æ¨¡æ¿è¾“å…¥
```python
image_inputs, video_inputs = process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)
```
ç¼–ç æ¨¡æ¿è¾“å…¥å°±æ¯”è¾ƒç®€å•ï¼Œå› ä¸ºæˆ‘çš„è¾“å…¥éƒ½æ˜¯æ–‡æœ¬/å›¾ç‰‡ï¼Œæ­¤è¿‡ç¨‹å°±æ˜¯éœ€è¦å°†è¿™äº›å†…å®¹è½¬åŒ–ä¸ºç¼–ç å½¢å¼ï¼ˆæ¯”å¦‚tokenizerå¤„ç†ç­‰ï¼‰ï¼Œå¤„ç†æ–¹å¼å¦‚ä¸‹ï¼š
* 1ã€[process_vision_info](https://github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L352):è¿”å›æˆ‘çš„å›¾åƒ/è§†é¢‘è¾“å‡ºï¼ˆéƒ½å­˜å‚¨åœ¨listä¸­ï¼‰

é¦–å…ˆæ˜¯è¿‡[extract_vision_info](https://github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L334)ä»æˆ‘ä¸Šé¢çš„å†…å®¹ä¸­æå–å‡ºå›¾ç‰‡/è§†é¢‘ï¼ˆ`[{'type': 'image', 'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}]`ï¼‰æå–å®Œæ¯•ä¹‹åå°±æ˜¯äº¤ç»™å¤„ç†å›¾ç‰‡/è§†é¢‘çš„å‡½æ•°è¿›è¡Œå¤„ç†
**å›¾ç‰‡å¤„ç†è¿‡ç¨‹**ï¼ˆ[`fetch_image`](https://github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L97)ï¼‰æ­¤è¿‡ç¨‹ä¹Ÿä¼šæ¯”è¾ƒç®€å•ï¼Œé¦–å…ˆå»åˆ¤æ–­ç±»å‹ï¼ˆæ˜¯`Image.Image`å¯¹è±¡/å›¾ç‰‡é“¾æ¥ç­‰ï¼‰ç„¶åæ‰“å¼€å›¾ç‰‡ï¼Œè€Œåå°±æ˜¯**ç¡®å®šå›¾ç‰‡åˆ†è¾¨ç‡å°ºå¯¸**ï¼Œæœ‰ä¸¤ç§`smart_resize`å¤„ç†æ–¹å¼ï¼Œç¬¬ä¸€ç§æ˜¯ç›´æ¥é€šè¿‡ï¼š`resized_height` å’Œ `resized_width`æ¥ç¡®å®šæ”¹å˜ï¼Œå¦å¤–ä¸€ç§ç›´æ¥é€šè¿‡ `min_pixels` å’Œ `max_pixels` æ¥å¤„ç†å›¾åƒå°ºå¯¸ã€‚å¯¹äº`smart_rezie`å‡½æ•°å¤„ç†è¿‡ç¨‹ä¸ºï¼š
```python
def smart_resize(
    height: int, width: int, factor: int = IMAGE_FACTOR, min_pixels: int = MIN_PIXELS, max_pixels: int = MAX_PIXELS)
    # IMAGE_FACTOR= 28
    if max(height, width) / min(height, width) > MAX_RATIO:
        ...
    h_bar = max(factor, round_by_factor(height, factor)) # round(number / factor) * factor
    w_bar = max(factor, round_by_factor(width, factor))
    if h_bar * w_bar > max_pixels:
        beta = math.sqrt((height * width) / max_pixels)
        h_bar = floor_by_factor(height / beta, factor) # æŒ‰æ¯”ä¾‹ç¼©å°å¹¶å‘ä¸‹å–æ•´  math.floor(number / factor) * factor
        w_bar = floor_by_factor(width / beta, factor)
    elif h_bar * w_bar < min_pixels:
        beta = math.sqrt(min_pixels / (height * width))
        h_bar = ceil_by_factor(height * beta, factor) # æŒ‰æ¯”ä¾‹æ”¾å¤§å¹¶å‘ä¸Šå–æ•´ math.ceil(number / factor) * factor
        w_bar = ceil_by_factor(width * beta, factor)
    return h_bar, w_bar
```
ä¸Šé¢3ä¸ªå°çš„å­å‡½æ•°è¡¨ç¤ºï¼šè®¡ç®—factorå€æ•°ã€å‘ä¸Šå–æ•´è®¡ç®—å€æ•°ã€å‘ä¸‹å–æ•´è®¡ç®—å€æ•°ï¼Œå¯¹äºsmart_resizeï¼ˆå»å®ç°åŠ¨æ€åˆ†è¾¨ç‡ï¼‰å‡½æ•°ï¼š**é€šè¿‡å››èˆäº”å…¥çš„æ–¹å¼ï¼Œé‡æ–°è®¾ç½®å›¾ç‰‡çš„ h å’Œ w å€¼ï¼Œç¡®ä¿å®ƒä»¬å¯ä»¥è¢«28æ•´é™¤**ï¼Œè¿™æ ·ä¸€æ¥å°±å¾—åˆ°äº†å›¾åƒçš„éœ€è¦ä¿®æ”¹çš„å°ºå¯¸äº†ï¼Œæ¯”å¦‚è¯´ï¼š
è¾“å…¥: ä¸€å¼  1000x500 çš„å›¾åƒ
è®¡ç®—åŸºç¡€å°ºå¯¸ï¼šround(1000/28)=36, round(500/28)=18 â†’ 1008x504
æ£€æŸ¥åƒç´ æ•°ï¼š1008*504 = 508,032 > MAX_PIXELS(200,704)
è®¡ç®—ç¼©æ”¾ç³»æ•°ï¼šbeta = sqrt(1000*500/200704) â‰ˆ 1.58
æœ€ç»ˆå°ºå¯¸ï¼šfloor(1000/1.58)=632, floor(500/1.58)=316 â†’ 616x308ï¼ˆ28çš„å€æ•°ï¼‰
**è§†é¢‘å¤„ç†è¿‡ç¨‹**ï¼ˆ[fetch_video](https://github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L277)ï¼‰å¯¹äºè§†é¢‘å¤„ç†å’Œå›¾åƒå¤„ç†ç›¸ç±»ä¼¼æ‰“å¼€-->æ”¹å˜å°ºå¯¸ã€‚åªä¸è¿‡åœ¨æ‰“å¼€è¿‡ç¨‹ä¸­QwenLV2.5å¤„ç†è¿‡ç¨‹ä¸ºï¼š
```python
def fetch_video(ele: dict, image_factor: int = IMAGE_FACTOR, return_video_sample_fps: bool = False):
    if isinstance(ele["video"], str):
        video_reader_backend = get_video_reader_backend()
        try:
            video, sample_fps = VIDEO_READER_BACKENDS[video_reader_backend](ele)
        except Exception as e:
            logger.warning(f"video_reader_backend {video_reader_backend} error, use torchvision as default, msg: {e}")
            video, sample_fps = VIDEO_READER_BACKENDS["torchvision"](ele)
    ...
```
å¯¹äº`VIDEO_READER_BACKENDS`è®¾è®¡äº†3ä¸­ä¸åŒèŒƒå¼ï¼š1ã€[_read_video_decord](https://github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L226)ï¼›2ã€[_read_video_torchvision](https://github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L183)ï¼›3ã€_read_video_torchcodecã€‚
* `_read_video_decord`

```python
def _read_video_decord(
    ele: dict,
) -> (torch.Tensor, float):
    """read video using decord.VideoReader

    Args:
        ele (dict): a dict contains the configuration of video.
        support keys:
            - video: the path of video. support "file://", "http://", "https://" and local path.
            - video_start: the start time of video.
            - video_end: the end time of video.
    Returns:
        torch.Tensor: the video tensor with shape (T, C, H, W).
    """
    import decord
    video_path = ele["video"]
    st = time.time()
    vr = decord.VideoReader(video_path)
    total_frames, video_fps = len(vr), vr.get_avg_fps()
    start_frame, end_frame, total_frames = calculate_video_frame_range(
        ele,
        total_frames,
        video_fps,
    ) # å¾—åˆ°è§†é¢‘çš„å¼€å§‹ ç»“æŸ æ€»ç»“å¤šå°‘å¸§
    nframes = smart_nframes(ele, total_frames=total_frames, video_fps=video_fps)
    idx = torch.linspace(start_frame, end_frame, nframes).round().long().tolist()
    video = vr.get_batch(idx).asnumpy()
    video = torch.tensor(video).permute(0, 3, 1, 2)  # Convert to TCHW format
    ...
    sample_fps = nframes / max(total_frames, 1e-6) * video_fps
    return video, sample_fps
```
å¯¹äºå…¶ä¸­çš„ `calculate_video_frame_range`å‡½æ•°å¤„ç†è¿‡ç¨‹ä¹Ÿå¾ˆç®€å•ï¼ˆç›´æ¥å»è®¡ç®—è§†é¢‘å¼€å§‹ã€ç»“æŸã€æ€»å…±å¤šå°‘å¸§ï¼‰ï¼Œè€Œåç±»ä¼¼åŠ¨æ€åˆ†è¾¨ç‡ï¼ˆsmart_resizeä¸­æˆç«‹ç›¸ç±»ä¼¼çš„ï¼‰å¯¹äºè§†é¢‘ä¼šé€šè¿‡æ™ºèƒ½è§†é¢‘å¸§æ•°è®¡ç®—ç®—æ³•ï¼ˆsmart_nframesï¼‰ï¼Œç”¨äº**ç¡®å®šä»è§†é¢‘ä¸­æå–å¤šå°‘å¸§ä½œä¸ºæ¨¡å‹è¾“å…¥**ï¼Œå¤„ç†è¿‡ç¨‹ä¸ºï¼šç¬¬ä¸€ç§ç›´æ¥é€šè¿‡`round_by_factor(ele["nframes"], FRAME_FACTOR)`æ¥å¾—åˆ°å¸§æ•°ï¼›ç¬¬äºŒç§å¤„ç†æ–¹å¼ä¸ºï¼ˆFPS_MIN_FRAMES = 4ã€FRAME_FACTOR = 2ã€FPS_MAX_FRAMES = 768ã€FPS = 2.0ï¼‰ï¼š
```python
fps = ele.get("fps", FPS)
min_frames = ceil_by_factor(ele.get("min_frames", FPS_MIN_FRAMES), FRAME_FACTOR)
max_frames = floor_by_factor(ele.get("max_frames", min(FPS_MAX_FRAMES, total_frames)), FRAME_FACTOR)
nframes = total_frames / video_fps * fps
nframes = min(min(max(nframes, min_frames), max_frames), total_frames)
nframes = floor_by_factor(nframes, FRAME_FACTOR)

"""
config = {"nframes": 24}
result = smart_nframes(config, total_frames=100, video_fps=30)
# è¾“å‡ºï¼š24ï¼ˆç›´æ¥ä½¿ç”¨é…ç½®å€¼ï¼‰

config = {"fps": 10, "min_frames": 16, "max_frames": 32}
result = smart_nframes(config, total_frames=100, video_fps=30)
# è®¡ç®—ï¼š100/30*10 â‰ˆ 33.33 â†’ çº¦æŸåˆ°32 â†’ å¯¹é½åˆ°32ï¼ˆFRAME_FACTOR=8çš„å€æ•°ï¼‰
"""
```
* 2ã€[processor](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py#L48)ï¼šå»å°†å›¾ç‰‡/æ–‡æœ¬è¿›è¡Œç¼–ç 

å…¶ä¸­å¯¹äºæ–‡æœ¬ç¼–ç ç›´æ¥é€šè¿‡ `self.tokenizer` æ¥å¤„ç†ï¼Œè€Œå¯¹äºå›¾åƒç›´æ¥é€šè¿‡ `self.image_processor`æ¥å¤„ç†ã€‚é¦–å…ˆåœ¨ [ä»£ç ](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py#L48)ä¸­å¾ˆå®¹æ˜“çœ‹åˆ°ä½¿ç”¨çš„å›¾åƒ/æ–‡æœ¬å¤„ç†æ–¹å¼`image_processor_class = "AutoImageProcessor"` å¯¹äºæ–‡æœ¬å¤„ç†æ–¹å¼ `tokenizer_class = ("Qwen2Tokenizer", "Qwen2TokenizerFast")`ã€‚
å¯¹äº**å›¾ç‰‡å¤„ç†æ–¹å¼**çš„ `Qwen2VLImageProcessor`ï¼ˆ[ä»£ç ](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py#L87)ï¼‰çš„å¤„ç†æ€è·¯ï¼š
```python
class Qwen2VLImageProcessor(BaseImageProcessor):
    def __init(...):
        ...
    def _preprocess(self, images, ...):
        ...
        height, width = get_image_size(images[0], channel_dim=input_data_format)
        resized_height, resized_width = height, width
        processed_images = []
        # Step-1
        for image in images:
            if do_resize:
                resized_height, resized_width = smart_resize(
                    height,
                    width,
                    factor=self.patch_size * self.merge_size,
                    min_pixels=self.min_pixels,
                    max_pixels=self.max_pixels,
                )
                image = resize(
                    image, size=(resized_height, resized_width), resample=resample, input_data_format=input_data_format
                )
            if do_rescale:
                image = self.rescale(image,...)
            if do_normalize:
                image = self.normalize(image,...)
        # Step-2
        patches = np.array(processed_images)
        if data_format == ChannelDimension.LAST:
            patches = patches.transpose(0, 3, 1, 2)
        if patches.shape[0] % self.temporal_patch_size != 0:
            # è§†é¢‘è¡¥å¸§å¤„ç†
            repeats = np.repeat(patches[-1][np.newaxis], self.temporal_patch_size - 1, axis=0)
            patches = np.concatenate([patches, repeats], axis=0)
        # è®¡ç®—ä¸åŒ patch ç½‘æ ¼å¤§å°
        channel = patches.shape[1]
        grid_t = patches.shape[0] // self.temporal_patch_size
        grid_h, grid_w = resized_height // self.patch_size, resized_width // self.patch_size

        patches = patches.reshape(
            grid_t,
            self.temporal_patch_size,
            channel,
            grid_h // self.merge_size,
            self.merge_size,
            self.patch_size,
            grid_w // self.merge_size,
            self.merge_size,
            self.patch_size,
        )
        patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)
        flatten_patches = patches.reshape(
            grid_t * grid_h * grid_w, channel * self.temporal_patch_size * self.patch_size * self.patch_size
        )

        return flatten_patches, (grid_t, grid_h, grid_w)
```
å¯¹äºä¸Šé¢å¤„ç†è¿‡ç¨‹ä¸­ï¼Œ**é¦–å…ˆ**å¯¹äº `_preprocess`ä¸»è¦æ˜¯å¯¹å›¾åƒè¿›è¡Œä¸€äº›é¢„å¤„ç†ï¼š1ã€do_resizeï¼šæ”¹å˜å›¾ç‰‡å¤§å°ï¼ˆç›´æ¥é€šè¿‡`smrt_resize`è¿›è¡Œå¤„ç†ï¼‰2ã€do_rescaleï¼šåƒç´ ç¼©å‡åˆ°0-1ä¹‹é—´ï¼›3ã€do_normalizeï¼šå¯¹å›¾ç‰‡è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼ˆé€šé“ç»´åº¦ï¼‰ï¼›**è€Œå**ç›´æ¥å¯¹äºé¢„å¤„ç†åçš„å›¾åƒç›´æ¥è¿›è¡Œåˆ‡å‰²å¤„ç†ä¸ºä¸åŒçš„patchè¾“å…¥åˆ°Vitä¸­ï¼Œæ¯”å¦‚å‡è®¾æˆ‘ä»¬çš„ `patches=(1,3,1024,1024)` é‚£ä¹ˆé¦–å…ˆè®¡ç®—ä¸åŒ patchç½‘æ ¼å¤§å°ï¼ˆtemporal_patch_size=2ï¼Œpatch_size=16ï¼Œmerge_size=2ï¼Œresized_height=1024 å€¼å¾—æ³¨æ„çš„æ˜¯temporal_patch_size=2ä¼šå°†å›¾ç‰‡å¤„ç†ä¸º 2 3 1024 1024ï¼‰é‚£ä¹ˆè®¡ç®—å¾—åˆ°ç½‘ç»œpatchå¤§å°ä¸ºï¼ˆgrid_h= grid_w= 64ï¼‰ï¼š1x64x64ï¼Œè€Œååˆ†åˆ«å°†ä¸åŒç»´åº¦ä¿¡æ¯ï¼ˆt h wï¼‰è¿›è¡Œåˆ’åˆ†ï¼Œä¹Ÿå°±æ˜¯å°† ï¼ˆ2ï¼Œ3ï¼Œ1024ï¼Œ1024ï¼‰-->ï¼ˆ1ï¼Œ2ï¼Œ3ï¼Œ32ï¼ˆ64//2ï¼‰ï¼Œ2ï¼Œ16ï¼Œ32ï¼ˆ64//2ï¼‰ï¼Œ2ï¼Œ16ï¼‰æœ€åå†å»äº¤æ¢ç»´åº¦å¹¶ä¸”è¿›è¡Œåˆå¹¶å³å¯ã€‚
**å›é¡¾ä¸€ä¸‹QwenVL2.5çš„å›¾ç‰‡å¤„ç†è¿‡ç¨‹**ï¼šé¦–å…ˆæ˜¯å»å¯¹å›¾ç‰‡è¿›è¡Œæ”¹å˜å°ºå¯¸ï¼ˆä¿è¯å›¾ç‰‡æœ€åå¯ä»¥æ•´é™¤patch_sizeï¼‰/ç¼©æ”¾/å½’ä¸€åŒ–ã€‚è€Œåå°±æ˜¯ç›´æ¥å°†å›¾ç‰‡å¤„ç†ä¸ºvitèƒ½å¤Ÿå¤„ç†çš„â€œåºåˆ—è¾“å…¥â€å¾—åˆ°çš„ç»´åº¦ä¸ºï¼š`[grid_t * grid_h * grid_w, channel * temporal_patch_size(2) * patch_size(14) * patch_size(14)]`ã€‚
> **è¡¥å……ä¸€**ï¼šå›¾ç‰‡è¾“å…¥å…·ä½“ä¾‹å­è¯´æ˜
> å‡è®¾é»˜è®¤å‚æ•°ä¸ºï¼špatch_size= 14, temporal_patch_size= 2, merge_size= 2
> å›¾åƒè¾“å…¥ä¸ºï¼ˆé€šè¿‡process_vision_infoæå‰å¤„ç†ä¹‹åçš„ç»´åº¦ï¼‰ï¼š(1092, 1568) 
> é¦–å…ˆè®¡ç®— `resized_height, resized_width = smart_resize`å¾—åˆ° 812 1176
> é¦–å…ˆè®¡ç®—ï¼šgrid_t=1ï¼Œgrit_h=812//14=58ï¼Œgrid_w=1176//14=84é‚£ä¹ˆè®¡ç®—å¾—åˆ°ä¸º 4872å¦å¤–ä¸€é¡¹ä¸º 1176ä¹Ÿå°±æ˜¯æœ€åå›¾åƒå¤„ç†å¾—åˆ°çš„è¾“å‡ºä¸ºï¼š`(1*58*84, 14*14*2*3)=(4872,1176)`
> **è¡¥å……äºŒ**ï¼šå¯¹äº smart_resizeå¿«é€Ÿä¼°ç®—æœ€åå¤§å°ï¼š
> å…ˆ round åˆ° factor çš„å€æ•°
> å¦‚æœè¶…å‡º max_pixels â†’ é™¤ä»¥ sqrt(HW/max_pixels)ï¼Œfloor â†’ factor å€æ•°
> å¦‚æœå°äº min_pixels â†’ ä¹˜ä»¥ sqrt(min_pixels/HW)ï¼Œceil â†’ factor å€æ•°
> å…¶å®ä¹Ÿå°±æ˜¯ï¼š**é¦–å…ˆå°†å›¾åƒå¤„ç†åˆ°ä¸ºfactorå€æ•°çš„åˆ†è¾¨ç‡ï¼Œè€Œåå»åˆ¤æ–­å’Œmax_pixelså’Œmin_pixelsä¹‹é—´å¤§å°ï¼Œå¤§äºå‰è€…å°±ç¼©å°ï¼Œå°äºå‰è€…å°±æ”¾å¤§**

æœ€åé€šè¿‡ä¸€ç³»åˆ—ç¼–ç ä¹‹åå¾—åˆ°è¾“å‡ºï¼š
```python
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)
"""
input_ids: torch.Size([1, 1243])
attention_mask: torch.Size([1, 1243])
pixel_values: torch.Size([4872, 1176])
image_grid_thw: torch.Size([1, 3])
"""
```

### 3ã€æ¨¡å‹è¾“å…¥å¤„ç†
```python
generated_ids = model.generate(**inputs, max_new_tokens=128)
```
æ•´ä½“[æ¨¡å‹](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L1724)è¾“å…¥å¤„ç†ï¼Œè¾“å…¥æ¨¡å‹ä¹Ÿå°±æ˜¯ä¸Šé¢ç¼–ç æ¨¡æ¿è¾“å…¥å‡ ä¸ªéƒ¨åˆ†ï¼Œåªä¸è¿‡ä¸»è¦å°±æ˜¯å¦‚ä¸‹å‡ ä¸ªå¤„ç†ï¼šé¦–å…ˆæ˜¯æ¨¡å‹å¤„ç†è¾“å…¥ `input_ids` ä»¥åŠæˆ‘çš„å›¾åƒ `pixel_values`ï¼ˆ`inputs_embeds = self.model.embed_tokens(input_ids)` [ä»£ç ](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L1790)ï¼‰ï¼Œè€Œåå°†è¾“å…¥è¿›è¡Œä½ç½®ç¼–ç å¤„ç†ï¼ˆ[ä»£ç ](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L1838)ï¼‰ï¼Œæœ€åè¾“å‡ºæ¨¡å‹ç»“æœï¼ˆ[ä»£ç ](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L1861)ï¼‰ï¼Œå¯¹äºQwenVL2.5å®Œæ•´æ¨¡å‹ç»“æ„ï¼š
```python
Qwen2_5_VLForConditionalGeneration(
  (model): Qwen2_5_VLModel(
    (visual): Qwen2_5_VisionTransformerPretrainedModel(
      (patch_embed): Qwen2_5_VisionPatchEmbed(
        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
      )
      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()
      (blocks): ModuleList(
        (0-31): 32 x Qwen2_5_VLVisionBlock(
          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)
          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)
          (attn): Qwen2_5_VLVisionAttention(
            (qkv): Linear(in_features=1280, out_features=3840, bias=True)
            (proj): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (mlp): Qwen2_5_VLMLP(
            (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)
            (up_proj): Linear(in_features=1280, out_features=3420, bias=True)
            (down_proj): Linear(in_features=3420, out_features=1280, bias=True)
            (act_fn): SiLU()
          )
        )
      )
      (merger): Qwen2_5_VLPatchMerger(
        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)
        (mlp): Sequential(
          (0): Linear(in_features=5120, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=5120, out_features=2048, bias=True)
        )
      )
    )
    (language_model): Qwen2_5_VLTextModel(
      (embed_tokens): Embedding(151936, 2048)
      (layers): ModuleList(
        (0-35): 36 x Qwen2_5_VLDecoderLayer(
          (self_attn): Qwen2_5_VLAttention(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
            (k_proj): Linear(in_features=2048, out_features=256, bias=True)
            (v_proj): Linear(in_features=2048, out_features=256, bias=True)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): Qwen2_5_VLRotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)
            (up_proj): Linear(in_features=2048, out_features=11008, bias=False)
            (down_proj): Linear(in_features=11008, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((2048,), eps=1e-06)
      (rotary_emb): Qwen2_5_VLRotaryEmbedding()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)
)
```
* **é¦–å…ˆ**ï¼šå¯¹äºè§†è§‰éƒ¨åˆ†å¤„ç†ï¼ˆ`Qwen2_5_VisionTransformerPretrainedModel`ï¼‰

> å¯¹äºè§†è§‰æ¨¡å‹ä¸»è¦éœ€è¦å¤„ç†çš„å°±æ˜¯ `pixel_values`ï¼Œå‡è®¾è¾“å…¥çš„ `pixel_values`ä¿¡æ¯ä¸ºï¼š`[4872, 1176]`ï¼Œimage_grid_thwä¸ºï¼š [1, 84, 58]ï¼ˆå°±æ˜¯å¯¹åº”grid_tã€grid_hã€grid_wè¿™ä¸‰ä¸ªæ•°å€¼ï¼‰

ä¸»è¦åŒ…æ‹¬å¦‚ä¸‹å‡ ä¸ªæ¨¡å—ï¼š
1ã€[Qwen2_5_VisionPatchEmbed](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L88)ï¼šä¸»è¦è¿›è¡Œå¤„ç†é€šè¿‡ä¸€ä¸ª `Conv3d`å¤„ç†ï¼Œå¤„ç†[è¿‡ç¨‹](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L105C4-L111C29)ä¹Ÿå°±æ˜¯è¯´é¦–å…ˆå°†è¾“å…¥çš„ç»´åº¦è¿›è¡Œä¿®æ”¹å¾—åˆ°ï¼š`view(-1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size)` --> (4872,1176)-->(4872,3,2,14,14)è€Œåå†å»é€šè¿‡å·ç§¯å¤„ç†å¾—åˆ° (4872,1280,1,1,1)æœ€åå¾—åˆ°ï¼š**(4872,1280)**ï¼Œä¹Ÿå°±å¯¹åº”ç€ï¼š`(grid_t*grid_h*grid_w, hiddend_size)`ï¼›
2ã€Qwen2_5_VisionRotaryEmbeddingï¼›
3ã€[Qwen2_5_VLVisionAttention](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L233)ï¼šé¦–å…ˆå»åˆ’åˆ†[window_size](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L465)è¿™ä¸€æ­¥ç›´æ¥æ ¹æ®è®¡ç®—å¾—åˆ°çš„ï¼š`[grid_t, grid_h, grid_w]`å»åˆ’åˆ†windowsï¼Œæ¯”å¦‚è¯´åœ¨ä¸Šè¿°ä¾‹å­ä¸­ï¼Œå¾—åˆ°çš„cu_seqlens = [0,64,128,...,4872]ï¼Œè€Œåå†å»é€šè¿‡å¦‚ä¸‹å¤„ç†ï¼š
```python
lengths = cu_seqlens[1:] - cu_seqlens[:-1]
splits = [
    torch.split(tensor, lengths.tolist(), dim=2) for tensor in (query_states, key_states, value_states)
]
```
å»åˆ’åˆ†qã€kã€vï¼ˆå½¢çŠ¶éƒ½ä¸ºï¼š[1, 16, 4872, 80]ï¼‰ç„¶åè®¡ç®—æ³¨æ„åŠ›ï¼Œè€Œåé€šè¿‡[Qwen2_5_VLPatchMerger](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L146)å°†ç»“æœåˆå¹¶èµ·æ¥ã€‚
**å…·ä½“è®¡ç®—è¿‡ç¨‹**ï¼Œé¦–å…ˆæ˜¯å¦‚ä½•å¾—åˆ°cu_seqlensï¼Œå› ä¸ºæˆ‘ä»¬å¾—åˆ°çš„gird_thw=(1, 84, 58)ä¹Ÿå°±æ˜¯è¯´æ€»å…±æœ‰84*58=4872ä¸ªtokenå»è®¡ç®—å…¨å±€æ³¨æ„åŠ›ï¼Œé‚£ä¹ˆè¿™å°±ä¼šå¯¼è‡´è®¡ç®—æ³¨æ„åŠ›çš„æ¶ˆè€—è¿‡å¤§ï¼Œå› æ­¤å¯ä»¥å…ˆå»åˆ‡åˆ†æˆå°çš„windowç„¶åå°å—å†…éƒ¨æ³¨æ„åŠ›è®¡ç®—ã€‚å› æ­¤é¦–å…ˆè®¡ç®—â€œå—â€çš„å¤§å°ï¼š`vit_merger_window_size = self.window_size // self.spatial_merge_size // self.patch_size`å¾—åˆ°ç»“æœä¸º: 4ï¼ˆ112/2/14ï¼‰ä¹Ÿå°±æ˜¯è¯´æ¯å—å¤§å°ä¸ºï¼š4x4=16ï¼Œä½†æ˜¯ä¸ä¸€å®šæˆ‘çš„grid_hå’Œgrid_wå¯èƒ½æ•´é™¤4ï¼Œå› æ­¤å°±éœ€è¦å»è®¡ç®—å¡«å……æ•°é‡ `vit_merger_window_size - llm_grid_h % vit_merger_window_size` åˆ†åˆ«å¾—åˆ° 4å’Œ2å› æ­¤å¡«å……åçš„hå’Œwä¸ºï¼š88,60è¿™æ ·ä¸€æ¥è®¡ç®—å¾—åˆ°windowæ•°é‡ä¸ºï¼š88//4 * 60//4=330æ¯ä¸ªçª—å£çš„tokensæ•°é‡ï¼š16

### 4ã€å›¾åƒå¤„ç†è¿‡ç¨‹æ€»ç»“
**æ€»ç»“ä¸Šè¿°å›¾åƒå¤„ç†è¿‡ç¨‹**ï¼šå¯¹äºä»»æ„è¾“å…¥å›¾åƒé¦–å…ˆé€šè¿‡smart_resizeï¼ˆé¦–å…ˆå°†å›¾åƒæ”¹å˜åˆ° factorçš„å€æ•°ï¼Œç„¶åå»åˆ¤æ–­å’Œmin_pixelså’Œmax_pixelsä¹‹é—´å¤§å°ï¼Œç„¶åè¿›è¡Œæ‰©å¤§ï¼Œç¼©å°ï¼‰è¿›è¡Œå¤„ç†ä¿è¯éƒ½å¯ä»¥æ•´é™¤patch_sizeï¼ˆ14ï¼‰ç„¶åä¸¢åˆ° `processor`ä¸­è¿›è¡Œå¤„ç†ä¸»è¦æ˜¯å¯¹å›¾åƒå½’ä¸€åŒ–ã€æ­£åˆ™åŒ–ã€æ”¹å˜ç»´åº¦ï¼ˆè¿˜ä¼šé€šè¿‡smart_resizeåœ¨å¤„ç†ä¸€æ¬¡ï¼‰ï¼Œå¤„ç†ä¹‹åå†å»ç¡®å®šä»–çš„ `grid_t, grid_h, grid_w`ï¼ˆå¯¹äºè¿™3ä¸ªå‚æ•°ç¡®å®šï¼šç›´æ¥é€šè¿‡ ç¬¬äºŒæ¬¡smart_resizeå¤„ç†ä¹‹åçš„ç»“æœé™¤ patch_sizeå³å¯ï¼‰ä¹Ÿå°±æ˜¯tokensæ•°é‡ï¼Œè€Œåå°†å›¾åƒå†…å®¹é€šè¿‡ conv3då¤„ç†å¾—åˆ°ï¼š`(grid_t* grid_h* grid_w, hidden_size)`ï¼Œæœ€åå°±æ˜¯è®¡ç®—window_attentionï¼ˆé¦–å…ˆç¡®å®šwidow_sizeç´¢å¼•ï¼Œé€šè¿‡ç´¢å¼•è¿›è¡Œåˆ‡åˆ†ï¼Œæœ€åè®¡ç®—æ³¨æ„åŠ›ï¼‰
> è¡¥å……ï¼šå¯¹äºwindow-attentionå¯ä»¥ç”¨å·ç§¯çš„æ€è·¯å»ç†è§£ï¼Œæ¯”å¦‚è¯´æˆ‘å¾—åˆ°â€œå›¾åƒâ€ï¼š`(grid_t, grid_h, grid_w)` æˆ‘æå‰è®¡ç®—æˆ‘çš„â€œå·ç§¯æ ¸â€å¤§å°ï¼ˆ`vit_merger_window_size = self.window_size // self.spatial_merge_size // self.patch_size`ï¼‰ä¸ºäº†ä¿è¯æˆ‘çš„ â€œå›¾åƒâ€å¯ä»¥è¢«å·ç§¯æ ¸å¤„ç†å°±éœ€è¦åšä¸€éƒ¨åˆ†å¡«å……ï¼Œè€Œåç”¨è¿™ä¸ªâ€œå·ç§¯æ ¸â€å»åˆ’åˆ†æˆä¸åŒâ€œå°å—â€åœ¨åˆ°è¿™ä¸ªå°å—é‡Œé¢è®¡ç®—æ³¨æ„åŠ›ã€‚

### 5ã€ä½ç½®ç¼–ç 

## QwenVLçš„å¾®è°ƒè¿‡ç¨‹
æ‰€æœ‰çš„ä»£ç ï¼š[https://github.com/shangxiaaabb/Docparse-QwenVL](https://github.com/shangxiaaabb/Docparse-QwenVL)

> **è¡¥å……ä¸€ï¼šèŠ‚çº¦æ˜¾å­˜å¯ä»¥è¿›è¡Œçš„æ“ä½œ**
> 1ã€ä½¿ç”¨`gradient_checkpointing`ï¼š`model.gradient_checkpointing_enable()`
> 2ã€ä½¿ç”¨ `qlora`è¿›è¡Œä¼˜åŒ–
> 3ã€ä½¿ç”¨ `AdamW8bit` è€Œä¸æ˜¯ `AdamW` 
> 4ã€ä½¿ç”¨ `xformers` ï¼ˆ`model.enable_xformers_memory_efficient_attention()`ï¼‰ï¼Œä¸è¿‡éœ€è¦æ³¨æ„çš„æ˜¯ QwenVL2.5ä¸æ”¯æŒä½¿ç”¨ `xformers`ï¼ˆé™¤æ­¤ä¹‹å¤–å®‰è£…ä¹Ÿæ¯”è¾ƒå¤æ‚ï¼‰
> 5ã€é¿å…æ˜¾å­˜ç¢ç‰‡ï¼ˆä¸è¦è¿‡åº¦çš„å»è¯„ä¼°æ¨¡å‹ï¼‰ï¼Œå¯ä»¥ä½¿ç”¨ `gc.collect() torch.cuda.empty_cache()` å»é€‚å½“çš„å‡å°ç¼“å­˜å‹åŠ›ï¼Œå¯¹äºä¸éœ€è¦çš„å†…å®¹ï¼ˆä¸­é—´å€¼ï¼‰ç›´æ¥é€šè¿‡ `del xx` å¤„ç†æ‰

### SFT å¤„ç†
https://www.f22labs.com/blogs/complete-guide-to-fine-tuning-qwen2-5-vl-model/

#### SFTæ•°æ®å¤„ç†è¿‡ç¨‹
é¦–å…ˆå‡è®¾æ•°æ®ï¼ˆé€šè¿‡jsonlè¿›è¡Œå­˜å‚¨ï¼‰è¾“å…¥æ ¼å¼ä¸ºï¼š
```json
{"image": 
    "845c2f9b-0583-4127-82a6-47c4c1c3ceb7.jpg", 
"prefix": 
    "QwenVL HTML", 
"suffix": 
    "<body><h2 data-bbox=......"
}
```
æ„å»ºdata_loaderåªéœ€è¦æ³¨æ„å¦‚ä¸‹å‡ ä¸ªæµç¨‹å³å¯ï¼š
**é¦–å…ˆæ„å»ºæˆ‘çš„è¾“å…¥æ¨¡æ¿**ã€‚è¿™ä¸€æ­¥ä¸»è¦æ˜¯å°†æˆ‘çš„æ•°æ®è¿›è¡Œè¯»å–ï¼Œç„¶åå»æ„å»ºæˆQwenVL2.5ï¼ˆæˆ–è€…å…¶ä»–å¤§æ¨¡å‹çš„å¯¹è¯å½¢å¼ï¼‰ï¼Œæ¯”å¦‚è¯´ï¼š
```python
def format_data(self, image, entry, text, prompt):
    return [
        {
            "role": "system",
            "content": [{"type": "text", "text": self.SYSTEM_MESSAGE}],
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text",
                    "text": (
                        "Must output the layout of the image strictly in HTML format. "
                        "Must follow the example below:\n"
                        "<h2 data-bbox='x1 y1 x2 y2'>Text</h2>\n"
                        "<p data-bbox='x1 y1 x2 y2'>Text</p>")
                },
            ],
        },
        {
            "role": "assistant",
            "content": [{"type": "text", "text": text}],
        },
    ]
```
ç„¶åå°±åªéœ€è¦å°†å‚æ•°ä¸¢åˆ°è¿™ä¸ªå‡½æ•°é‡Œé¢å°±å¯ä»¥è‡ªåŠ¨åŒ–çš„å°†æ•°æ®å¤„ç†å¥½ï¼ˆè¡¥å……ä¸€ç‚¹ï¼Œå¯¹äºä¸Šé¢å‡ ä¸ªå‚æ•°ï¼Œä¸€èˆ¬æ¥è¯´å…¶ä¸­`text`å°±æ˜¯æˆ‘çš„æ¨¡å‹éœ€è¦è¾“å‡ºçš„labelï¼Œè€Œåå…¶ä»–çš„å†…å®¹å°±æ˜¯æ¨¡å‹çš„è¾“å…¥ï¼‰ï¼Œå…¶æ¬¡å°±åªéœ€è¦å°†**è¾“å…¥è¿›è¡Œç¼–ç **å³å¯ä¹Ÿå°±æ˜¯è¯´ç›´æ¥é€šè¿‡ï¼š
```python
image_inputs, _ = process_vision_info(messages)
encoding = self.processor(
    text=[text],
    images= image_inputs,
    return_tensors="pt",
    padding= False,
    truncation=True,
    max_length= self.max_length
)
```
è¿™æ ·å°±ä¼šçš„å¾—åˆ°æ¨¡å‹çš„è¾“å…¥å†…å®¹ï¼Œä¸€èˆ¬æ¥è¯´å¾—åˆ°çš„æ˜¯ï¼š`input_ids`: æ–‡æœ¬ç¼–ç å†…å®¹ï¼ˆä¸€èˆ¬æ¥è¯´ä¼šç›´æ¥å°† input_idsè¿›è¡Œå¤åˆ¶ä½œä¸ºæˆ‘ä»¬çš„ labelsï¼Œå½“ç„¶ä¹Ÿå¯ä»¥ç›´æ¥å¯¹ä¸è¾“å…¥è§£æï¼Œåªéœ€è¦æ¨¡å‹é‚£éƒ¨åˆ†ä½œä¸ºlabelsï¼‰ï¼Œ`attention_mask`ï¼Œ`pixel_values`: å›¾ç‰‡åƒç´ ç¼–ç ç»“æœ`image_grid_thw`: æˆ‘çš„tokensæ•°é‡ï¼ˆ`grid_t*grid_h*grid_w`ï¼‰ã€‚
ä¸è¿‡ä¸Šé¢å¤„ç†è¿‡ç¨‹åªæ˜¯é’ˆå¯¹ä¸€å¼ å›¾ç‰‡è¿›è¡Œå¤„ç†å»æ„å»ºå¯¹è¯ä¿¡æ¯ï¼Œå¦‚æœéœ€è¦**å¤„ç†å¤šç»„å›¾ç‰‡åŒæ—¶è¿›è¡Œè¾“å…¥**ï¼ˆæ¯”å¦‚è¯´3å¼ å›¾ç‰‡è¿›è¡Œæ’åºï¼Œè®©QwenVLè¾“å‡ºï¼‰é‚£ä¹ˆå¤„ç†è¿‡ç¨‹åªéœ€è¦ä¿®æ”¹ `content`å³å¯ï¼ˆåœ¨contenté‡Œé¢æŒ‡å®šå¤šä¸ªå›¾ç‰‡å³å¯ï¼‰
```python
"content": [
            {
                "type": "image",
                "image": "./tmp/7.png",
            },
            {
                "type": "image",
                "image": "./tmp/1.png",
            },
            {"type": "text", "text": "..."},
        ],
```

#### SFTæ¨¡å‹å¤„ç†
ä¸€èˆ¬æ¥è¯´å¦‚æœç›´æ¥ä½¿ç”¨loraå»å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå¤„ç†ä¹Ÿæ¯”è¾ƒç®€ç­”ï¼š
```python
target_modules = ['q_proj', 'v_proj']
lora_config = LoraConfig(
    task_type= config.lora_task_type,
    target_modules= target_modules,
    r= config.lora_rank,
    lora_alpha= config.lora_alpha,
    lora_dropout= config.lora_dropout,
)
model = get_peft_model(model, lora_config)
```
è¿™æ ·ä¸€æ¥æ¨¡å‹å°±ä¼šè¢«loraâ€œåŒ…è£¹â€ï¼Œå¾®è°ƒè¿‡ç¨‹ä¹Ÿå°±æ˜¯ä¼˜åŒ–loraçš„å‚æ•°ï¼Œä¸è¿‡å¦‚æœéœ€è¦ä½¿ç”¨`qlora`ï¼ˆloraé‡åŒ–ç‰ˆæœ¬ï¼‰å†æ¨¡å‹åŠ è½½è¿‡ç¨‹ä¸­éœ€è¦ä½¿ç”¨å‚æ•° `quantization_config`ï¼š
```python
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)
...
if model_name == 'Qwen/Qwen2.5-VL-3B-Instruct':
    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
        model_name, 
        torch_dtype= torch.bfloat16, 
        cache_dir= config.cache_dir,
        quantization_config= bnb_config if config.lora_type== 'qlora' else None,
    )
```
å¯¹äºæ¨¡å‹è®­ç»ƒä»¥åŠå‚æ•°ä¼˜åŒ–è¿‡ç¨‹å°±æ¯”è¾ƒç®€å•ï¼š
```python
for step, batch in enumerate(train_loader):
    outputs = model(**batch)
    loss = outputs.loss
```
å¾—åˆ°çš„æ‰€æœ‰çš„å†…å®¹å¯ä»¥ç›´æ¥å…¨éƒ¨ä¸¢åˆ°modelé‡Œé¢ï¼Œä»–ä¼šè‡ªåŠ¨è®¡ç®—losså€¼ï¼Œå¯¹äº`outputs = model(**batch)`æ¨¡å‹[è¿”å›](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L1397)å¾—åˆ°ç»“æœä¸ºï¼š
`loss`: Optional[torch.FloatTensor]ï¼šæ¨¡å‹è®¡ç®—å¾—åˆ°çš„lossï¼ˆç›´æ¥è®¡ç®—äº¤å‰ç†µæŸå¤±å¾—åˆ°ï¼‰ï¼Œå¦‚æœè¾“å…¥å†…å®¹ä¸­æ²¡æœ‰labelsï¼ˆå°±æ˜¯æ¨¡å‹è¾“å‡ºé‚£æ®µæ–‡æœ¬ï¼‰é‚£ä¹ˆå°±ä¸ä¼šå»è®¡ç®—loss
`logits`: Optional[torch.FloatTensor]ï¼šæ¨¡å‹è¾“å‡ºç»“æœ
`past_key_values`: Optional[list[torch.FloatTensor]]ï¼šTransformer è§£ç å™¨çš„ KV ç¼“å­˜ï¼ˆæ¯ä¸€å±‚çš„æ³¨æ„åŠ› key å’Œ valueï¼‰
`hidden_states`: Optional[tuple[torch.FloatTensor]]ï¼šæ¯ä¸€å±‚çš„ hidden state (batch_size, seq_len, hidden_size)
`attentions`: Optional[tuple[torch.FloatTensor]]ï¼šæ¯ä¸€å±‚æ³¨æ„åŠ›æƒé‡ (batch_size, num_heads, seq_len, seq_len)
`rope_deltas`: Optional[torch.LongTensor]ï¼šæ—‹è½¬ä½ç½®ç¼–ç  RoPEï¼ˆRotary Position Embeddingï¼‰çš„åç§»é‡

### RL å¤„ç†
> å¼ºåŒ–å­¦ä¹ æ¡†æ¶å¾ˆå¤šï¼Œ1ã€huggingface-trl: [https://github.com/huggingface/trl](https://github.com/huggingface/trl)ï¼›2ã€å­—èŠ‚è·³åŠ¨-verl: [https://github.com/volcengine/verl](https://github.com/volcengine/verl)ï¼›3ã€OpenRLHFï¼š[https://github.com/OpenRLHF/OpenRLHF](https://github.com/OpenRLHF/OpenRLHF)

å¼ºåŒ–å­¦ä¹ å¤„ç†è¿‡ç¨‹ï¼ˆç›´æ¥ä½¿ç”¨ trlï¼ˆ**ä½¿ç”¨ç‰ˆæœ¬ï¼š0.22.1**ï¼‰åº“ï¼Œå®ƒé‡Œé¢æä¾›äº†[å¤šç§è„šæœ¬](https://github.com/huggingface/trl/blob/main/examples/scripts/dpo_vlm.py)ï¼‰å¯¹äºå¤šæ¨¡æ€/å¤§è¯­è¨€æ¨¡å‹ä½¿ç”¨RLä¸­æ¯”è¾ƒå¸¸è§çš„çš„æ•°æ®ç±»å‹ï¼šä¸€èˆ¬å°±æ˜¯æŠ›å‡ºé—®é¢˜ï¼Œè€Œåç»™å‡ºé€‰é¡¹è®©æ¨¡å‹è¿›è¡Œé€‰æ‹©ã€‚æ­¤ç±»æ•°æ®é›†ä¸€èˆ¬æ ¼å¼ä¸ºï¼š
```python
{"images": [], "prompt": [], "chosen": [], "rejected": []}
# å½“ç„¶è¿™ä¸ª images ä¹Ÿå¯ä»¥æ›¿æ¢ä¸ºæ–‡æœ¬é—®é¢˜ "question"
```
æ¯”å¦‚è¯´æ•°æ®é›†ï¼š[HuggingFaceH4/rlaif-v_formatted](https://huggingface.co/datasets/HuggingFaceH4/rlaif-v_formatted/viewer/default/train?row=0&views%5B%5D=train)ä»–çš„æ•°æ®ç»“æ„å¦‚ä¸‹ï¼š
![image.png](https://s2.loli.net/2025/09/05/O8E94bqdysHGxV6.webp)
ç›´æ¥çœ‹trlä¸­å¦‚ä½•å®ç°[QwenVL-DPO](https://github.com/huggingface/trl/blob/main/examples/scripts/dpo_vlm.py)è¿‡ç¨‹ä»£ç ï¼š
```python
from trl import (
    DPOConfig,
    DPOTrainer,
    ModelConfig,
    ScriptArguments,
    TrlParser,
    get_kbit_device_map,
    get_peft_config,
    get_quantization_config,
)
...
dataset = load_dataset(
    script_args.dataset_name,
    name=script_args.dataset_config,
    streaming=script_args.dataset_streaming,
)
...
# ref_model å’Œ model éƒ½æ˜¯ç›´æ¥ä½¿ç”¨QwenVL
trainer = DPOTrainer(
    model,
    ref_model,
    args=training_args,
    train_dataset=dataset[script_args.dataset_train_split],
    eval_dataset=dataset[script_args.dataset_test_split] if training_args.eval_strategy != "no" else None,
    processing_class=processor,
    peft_config=peft_config,
)
```
åˆæ¬¡ä¹‹å¤–ï¼ŒRLå°±å’ŒSFTä¸€æ ·éœ€è¦è®©æ¨¡å‹å»æŒ‰ç…§æˆ‘çš„æ•°æ®è¿›è¡Œè¾“å‡ºï¼Œå› æ­¤å¤„ç†ä¹Ÿå°±æ˜¯ç›´æ¥`logits=model(**model_inputs).logits`å¾—åˆ°æ¨¡å‹æœ€åè¾“å‡ºï¼ˆè§ç›¸å½“äºæ¯ä¸ªè¯çš„æ¦‚ç‡ï¼‰
#### RL-DPOå¤„ç†ä»£ç 
é¦–å…ˆåœ¨ä»£ç ï¼ˆ`DPOTrainer`ï¼‰ä¸»è¦æ˜¯é€šè¿‡ç»§æ‰¿ `Trainer`ï¼ˆ[ä»£ç ](https://huggingface.co/docs/transformers/en/main_classes/trainer)åŒ…è£¹å¥½äº†å„ç§å¤„ç†è¿‡ç¨‹æ¯”å¦‚æ•°æ®åŠ è½½æ¨¡å‹è¯„ä¼°ç­‰å„é¡¹å¤„ç†è¿‡ç¨‹ï¼‰ç›´æ¥çœ‹ `DPOTrainer`é‡Œé¢çš„ `get_batch_loss_metrics`ï¼ˆå®Œæ•´æ¨¡å‹è¾“å…¥ç„¶åè¾“å‡ºlossï¼‰ï¼š
```python
def get_batch_loss_metrics(self, model, batch, train_eval):
    ...
    if ...:
        ...
    else:
        model_output = self.concatenated_forward(model, batch)
        if "ref_chosen_logps" in batch and "ref_rejected_logps" in batch:
            ref_chosen_logps = batch["ref_chosen_logps"]
            ref_rejected_logps = batch["ref_rejected_logps"]
        else:
            ref_chosen_logps, ref_rejected_logps = self.compute_ref_log_probs(batch)

        losses = 0
        chosen_rewards = 0
        rejected_rewards = 0

        for idx, loss_type in enumerate(self.loss_type):
            _losses, _chosen_rewards, _rejected_rewards = self.dpo_loss(
                model_output["chosen_logps"],
                model_output["rejected_logps"],
                ref_chosen_logps,
                ref_rejected_logps,
                loss_type,
                model_output,
            )

            weight = self.loss_weights[idx] if self.loss_weights else 1.0
            losses = losses + _losses * weight
            chosen_rewards = chosen_rewards + _chosen_rewards * weight
            rejected_rewards = rejected_rewards + _rejected_rewards * weight
    return losses.mean(), ...
```
å¯¹äºDPOTraineré‡Œé¢data_loaderå¤„ç†è¿‡ç¨‹ä¸ºï¼Œé¦–å…ˆå¯¹äº `dataset`ä¼šé€šè¿‡ `processing_class`ï¼ˆä¸€èˆ¬æ¥è¯´ä¹Ÿå°±æ˜¯å¯¹äºæ–‡æœ¬ç›´æ¥ä½¿ç”¨ tokenizerï¼Œäº¦æˆ–è€…ç›´æ¥ä½¿ç”¨ `AutoProcessor.from_pretrained(...)`ï¼‰è¿›è¡Œå¤„ç†ï¼Œä¹Ÿå°±æ˜¯è¯´ä¼šæå‰å°†æ•°æ®processorå¤„ç†ï¼ˆå’ŒSFTå¤„ç†æ–¹å¼ç›¸åŒï¼‰é‚£ä¹ˆå°±ä¼šå¾—åˆ° `self.train_dataset`ï¼Œé‚£ä¹ˆæ¥ä¸‹æ¥å°±æ˜¯ç›´æ¥å»é€šè¿‡[ä»£ç ](https://github.com/huggingface/trl/blob/8534f0edf8608ad6bcbea9beefae380fa60ded77/trl/trainer/dpo_trainer.py#L455)ï¼ˆåŠ è½½train_loaderæ•°æ®ï¼‰ï¼Œå…¶ä¸­å¤„ç†æ–¹å¼ä¸ºï¼š`ref_chosen_logp, ref_rejected_logp = self.compute_ref_log_probs(padded_batch)` å¯¹äº [`compute_ref_log_probs`](https://github.com/huggingface/trl/blob/8534f0edf8608ad6bcbea9beefae380fa60ded77/trl/trainer/dpo_trainer.py#L758)é‡Œé¢å¤„ç†è¿‡ç¨‹ä¸ºï¼šç›´æ¥å»é€šè¿‡ model/ref_modelå»å¤„ç†ï¼š`self.concatenated_forward`ï¼ˆ[ä»£ç ](https://github.com/huggingface/trl/blob/8534f0edf8608ad6bcbea9beefae380fa60ded77/trl/trainer/dpo_trainer.py#L961)ï¼‰å¾—åˆ°æ¨¡å‹è¾“å‡ºï¼š `model_output`ï¼Œè€Œåå†å»ä½¿ç”¨ `self.dpo_loss`å»è®¡ç®—æŸå¤±ã€‚

* `self.concatenated_forward`å¤„ç†è¿‡ç¨‹ [Github-ä»£ç ](https://github.com/huggingface/trl/blob/8534f0edf8608ad6bcbea9beefae380fa60ded77/trl/trainer/dpo_trainer.py#L961)ï¼ˆå®é™…è§£é‡Šä½¿ç”¨ **trl:0.22.1ç‰ˆæœ¬ä»£ç **å’Œgithubæœ‰å·®å¼‚ï¼‰

```python
def concatenated_forward(model, batch, is_ref_model):
    concatenated_batch = self.concatenated_inputs(batch, padding_value=self.padding_value)
    prompt_input_ids = concatenated_batch["prompt_input_ids"]         # é—®é¢˜æ–‡æœ¬
    prompt_attention_mask = concatenated_batch["prompt_attention_mask"]
    completion_input_ids = concatenated_batch["completion_input_ids"] # å›ç­”æ–‡æœ¬ åŒæ—¶æ‹¼æ¥äº†chosen_input_ids å’Œ rejected_input_ids
    completion_attention_mask = concatenated_batch["completion_attention_mask"]
    if self.is_encoder_decoder:
        labels = completion_input_ids
        labels[completion_attention_mask == 0] = self.label_pad_token_id
        outputs = model(
                    input_ids=prompt_input_ids,
                    attention_mask=prompt_attention_mask,
                    labels=labels,  # we need the labels for the logits to be returned
                    **model_kwargs,
                )
        logits = outputs.logits
        loss_mask = completion_attention_mask.bool()
    else:
        # Process-1
        input_ids = torch.cat((prompt_input_ids, completion_input_ids), dim=1)
        ...
        outputs = model(input_ids, **model_kwargs)
        logits = outputs.logits
    # Process-2
```
**Process-1**ï¼šé¦–å…ˆæ˜¯å°†æ–‡æœ¬å’Œå›ç­”è¿›è¡Œæ‹¼æ¥ï¼Œè€Œåå»åˆ¤æ–­å¦‚æœæŒ‡å®š `max_length`é‚£ä¹ˆå°±å»æ ¹æ® `truncation_mode`ï¼ˆæå¤´/å»å°¾ï¼šä¿ç•™åºåˆ—æœ«å°¾ï¼Œç§»é™¤å¼€å¤´å¤šä½™éƒ¨åˆ†ï¼‰å»è£å‡è¾“å…¥ä»¥åŠç§»é™¤å¡«å……å’Œé™åˆ¶è®¡ç®—èŒƒå›´æ¥ä¼˜åŒ–å†…å­˜å’Œæ€§èƒ½æœ€åä¸¢åˆ°æ¨¡å‹ä¸­è¿›è¡Œå¤„ç†ã€‚
> æå¤´å»å°¾è¿‡ç¨‹
> `keep_start`ï¼šä¿ç•™åºåˆ—å¼€å¤´ã€‚å…ˆè°ƒç”¨ flush_leftï¼ˆ**æ‰€æœ‰æœ‰æ•ˆçš„tokenå·¦ç§»åŠ¨å»é™¤ä¸­é—´padding**ï¼‰ã€‚ç„¶åæˆªæ–­åˆ° max_lengthï¼ˆ[:, :self.max_length]ï¼‰ã€‚`[0, 0, x, x, x, x] â†’ flush_left` å `[x, x, x, x]`ï¼Œè‹¥ max_length=3ï¼Œåˆ™æˆªæ–­ä¸º `[x, x, x]`
> keep_endï¼šä¿ç•™åºåˆ—æœ«å°¾ã€‚å…ˆè°ƒç”¨ flush_rightï¼ˆ**å°†æ‰€æœ‰æœ‰æ•ˆtokenå‘å³ç§»åŠ¨ï¼Œå‰é¢å¡«å……padding**ï¼‰ã€‚æˆªæ–­åˆ°æœ€å max_length ä¸ª tokenï¼ˆ[:, -self.max_length:]ï¼‰ã€‚å†æ¬¡è°ƒç”¨ flush_leftï¼Œç¡®ä¿å·¦ä¾§æ— å¡«å……ã€‚`[0, 0, x, x, x, x] â†’ flush_right` å `[0, 0, x, x]`ï¼Œæˆªæ–­å `[x, x]`ï¼Œflush_left åä¿æŒä¸å˜ã€‚

å›é¡¾ä¸€ä¸‹`self.concatenated_forward`ï¼ˆæ¨¡å‹å¤„ç†ï¼‰æ•´ä¸ªè¿‡ç¨‹ï¼šé¦–å…ˆæ˜¯å°†`chosen_input_ids` å’Œ `rejected_input_ids`ä¸¤éƒ¨åˆ†è¿›è¡Œ**æ‹¼æ¥**ï¼ˆ`self.concatenated_inputs`åšçš„ï¼Œäºæ­¤åŒæ—¶å¯¹äºå…¶ä»–å†…å®¹ä¹Ÿéƒ½ä¼šæ‹¼æ¥æˆä¸¤éƒ¨åˆ†ï¼‰ä½œä¸º**æˆ‘ä»¬æ¨¡å‹çš„å›ç­”**ã€‚è€Œåä¸¢åˆ°**æ¨¡å‹ä¸­è¿›è¡Œå¤„ç†**ï¼ˆå¯¹äº `is_encoder_decoder` å¯ä»¥ç›´æ¥ç»™æ¨¡å‹å¤„ç†ï¼Œå¦‚æœä¸æ˜¯é‚£ä¹ˆå°±é€šè¿‡**æˆªæ–­è£å‰ªç­‰å¤„ç†æ¥èŠ‚çº¦å­˜å‚¨åœ¨ç”±æ¨¡å‹å¤„ç†**ï¼‰å¾—åˆ° `logits`ï¼Œå»é€šè¿‡logits, labelå¾—åˆ°æ¯ä¸ªtokençš„å¯¹æ•°æ¦‚ç‡ï¼š`all_logps`ï¼Œè€Œåå†å»åˆ¤æ–­æ˜¯å¦è¿›è¡Œä¼˜åŒ–ç­–ç•¥ï¼š `ipo` æˆ–è€… `ld_alpha`ï¼ˆé•¿åº¦å»æ•åŒ–ï¼‰å»ä¼˜åŒ–å¾—åˆ°çš„ `all_logps`ï¼ˆå¯¹å…¶ç›´æ¥åˆ‡åˆ†å°±å¯ä»¥å¾—åˆ°ï¼š`chosen_logps` å’Œ `rejected_logps`ï¼‰

* `self.dpo_loss`è®¡ç®—æŸå¤±è¿‡ç¨‹ [Github-ä»£ç ](https://github.com/huggingface/trl/blob/8534f0edf8608ad6bcbea9beefae380fa60ded77/trl/trainer/dpo_trainer.py#L844)ï¼ˆå®é™…è§£é‡Šä½¿ç”¨ **trl:0.22.1ç‰ˆæœ¬ä»£ç **å’Œgithubæœ‰å·®å¼‚ï¼‰
  
```python
model_output = self.concatenated_forward(model, batch)
if "ref_chosen_logps" in batch and "ref_rejected_logps" in batch:
    # ç›´æ¥ä½¿ç”¨æ•°æ®é‡Œé¢çš„çš„ç»“æœ
    ref_chosen_logps = batch["ref_chosen_logps"]
    ref_rejected_logps = batch["ref_rejected_logps"]
else:
    # ç›¸å¯¹äºç›´æ¥åœ¨ç”¨æ¨¡å‹å¤„ç†ä¸€ä¸‹å¾—åˆ°ç»“æœ
    ref_chosen_logps, ref_rejected_logps = self.compute_ref_log_probs(batch)
_losses, _chosen_rewards, _rejected_rewards = self.dpo_loss(
    model_output["chosen_logps"],
    model_output["rejected_logps"],
    ref_chosen_logps,
    ref_rejected_logps,
    loss_type,
    model_output,)
```
> `if "ref_chosen_logps" in batch and "ref_rejected_logps" in batch:` ç›´æ¥ä½¿ç”¨æ•°æ®é‡Œé¢çš„ç»“æœè¿‡ç¨‹ä¸€æ ·çš„è¿˜æ˜¯é€šè¿‡æ¨¡å‹ `self.compute_ref_log_probs(batch)`ï¼ˆè¿™ä¸ªè¿˜æ˜¯è°ƒç”¨äº† `self.concatenated_forward`ï¼‰å»å¾—åˆ°chosen_logps å’Œ rejected_logpsç»“æœã€‚
> å¯¹äº dpo_loss é‡Œé¢model_ å’Œ ref_ è¿™ä¸¤éƒ¨åˆ†ç†è®ºä¸Šæ˜¯ä¸¤ä¸ªä¸åŒçš„æ¨¡å‹çš„è¾“å‡ºç»“æœï¼Œä½†æ˜¯å¦‚æœæ²¡æœ‰æŒ‡å®š ref_model é‚£ä¹ˆç›´æ¥å°±éƒ½ç›´æ¥ä½¿ç”¨ model å³å¯

å¯¹äºDPOçš„losså¤„ç†è¿‡ç¨‹å°±æ¯”è¾ƒç®€å•ï¼Œåœ¨trlä¸­æä¾›3ç§è®¡ç®—æ–¹å¼ï¼š
**1ã€Alphaæ•£åº¦è®¡ç®—**
![image.png](https://s2.loli.net/2025/09/05/sonSkV1aNPZdD9H.webp)

**2ã€KLæ•£åº¦è®¡ç®—**
![image.png](https://s2.loli.net/2025/09/05/UOpNRbKQcxa18dL.webp)

**3ã€JSæ•£åº¦è®¡ç®—**
![image.png](https://s2.loli.net/2025/09/05/OhCBN8q7y4lzGtx.webp)

åœ¨è®¡ç®—å¾—åˆ°ä¸åŒæ–¹å¼å¾—åˆ°çš„ç»“æœï¼šlogitsç„¶åå†å»æ ¹æ®ä¸åŒ `loss_type`å»åšå¤„ç†ï¼ˆæ¯”å¦‚è¯´ï¼š`loss_type == "sigmoid"` å¤„ç†è¿‡ç¨‹ä¸ºï¼š`losses = (-F.logsigmoid(self.beta * logits) * (1 - self.label_smoothing)- F.logsigmoid(-self.beta * logits) * self.label_smoothing)`ï¼‰
#### RL-DPOå¤„ç†è¿‡ç¨‹æ€»ç»“
**é¦–å…ˆ**å¯¹äºæˆ‘ä»¬çš„æ•°æ®é›†ï¼ˆå‡è®¾ä¸º3å…ƒç»„ï¼š[é—®é¢˜, æ¥å—å›ç­”, æ‹’ç»å›ç­”]ï¼‰é¦–å…ˆå°±æ˜¯å»é€šè¿‡ `processor`ï¼ˆæ¯”å¦‚Qwen2.5vlå¯ä»¥ç›´æ¥ loadï¼‰å»ç¼–ç æˆ‘çš„æ‰€æœ‰å†…å®¹ï¼ˆè¿™ä¸€æ­¥å’ŒSFTè¿‡ç¨‹ç›¸ä¼¼ï¼‰ï¼Œ**è€Œå**å°±æ˜¯å»é€šè¿‡`self.concatenated_forward`è¿™ä¸ªå‡½æ•°å°†æˆ‘ä»¬çš„3å…ƒç»„è¿›è¡Œæ‹¼æ¥å¾—åˆ°ï¼š[é—®é¢˜,é—®é¢˜], [æ¥å—å›ç­”, æ‹’ç»å›ç­”]è€Œåå¾—åˆ°æ¨¡å‹çš„è¾“å…¥ä¸ºï¼š[é—®é¢˜+æ¥å—å›ç­”, é—®é¢˜+æ‹’ç»å›ç­”]ï¼Œå°†è¾“å…¥ç›´æ¥äº¤ç»™çš„æ¨¡å‹ï¼ˆç”±äºè§å†…å®¹ç›´æ¥æ‹¼æ¥èµ·æ¥ï¼Œå¯èƒ½ä¼šä¼˜åŒ–æ¨¡å‹çš„è¾“å…¥/å‡ºé•¿åº¦è¿‡é•¿å¯¼è‡´çˆ†æ˜¾å­˜ï¼Œå› æ­¤è¾“å…¥ä¹‹å‰ä¼šç”±ä¸€äº›è£å‰ªå¤„ç†æ“ä½œï¼‰å»å¾—åˆ°è¾“å‡ºï¼š`logits`ï¼Œè€Œåé€šè¿‡logits, labelå¾—åˆ°æ¯ä¸ªtokençš„å¯¹æ•°æ¦‚ç‡ï¼š`all_logps`ï¼Œï¼ˆé€šè¿‡å¯¹`all_logps`è¿›è¡Œæ‹†åˆ†ï¼‰å°±å¯ä»¥å¾—åˆ°æ¥å—å›ç­”çš„å€¼ï¼ˆ`chosen_logps`ï¼‰ï¼Œä»¥åŠæ‹’ç»å›ç­”çš„å€¼ï¼ˆ`rejected_logps`ï¼‰ï¼Œ**æœ€å**åœ¨å¾—åˆ°è¿™ä¸¤éƒ¨åˆ†å€¼ä¹‹åå°±æ˜¯ç›´æ¥å»è®¡ç®—lossã€‚
å¯¹äºlossè®¡ç®—è¿‡ç¨‹ï¼ˆå‡è®¾ä¸ºKLæ•£åº¦ï¼‰ï¼š$\mathrm{loss}=-\frac{1}{N}\sum_{i=1}^{N}\log\sigma\left(\beta\cdot((\log\pi_{\theta}(y_{w}|x)-\log\pi_{\theta}(y_{l}|x))-(\log\pi_{\mathrm{ref}}(y_{w}|x)-\log\pi_{\mathrm{ref}}(y_{l}|x)))\right)$ã€‚å¯¹äºé‡Œé¢ä¸¤é¡¹ç›¸å‡è¿‡ç¨‹ä»£ç ï¼š
```python
chosen_logratios = chosen_logps.to(device) - (not self.reference_free) * ref_chosen_logps.to(device)
rejected_logratios = rejected_logps.to(device) - (not self.reference_free) * ref_rejected_logps.to(device)
```
**åæ€**ï¼šå¦‚æœéœ€è¦æ‰‹æ“ä¸€ä¸ªDPOè®­ç»ƒè¿‡ç¨‹ä»£ç ï¼ˆéœ€è¦å€Ÿé‰´`concatenated_forward`ä»£ç æ¥è¾…åŠ©å®ç°ï¼‰
#### RL-GRPOå¤„ç†ä»£ç 
å®˜æ–¹å®ç°[ä»£ç ](https://github.com/huggingface/trl/blob/main/examples/scripts/grpo_vlm.py)ï¼Œå¯¹äºDPOè¿‡ç¨‹å¾ˆå®¹æ˜“å‘ç°ä¸€ç‚¹åœ¨GRPOä¸­ç›´æ¥ä¸è¦`ref_model` åªæ˜¯ç”¨ä¸€ä¸ªmodelä¸è¿‡è®¾è®¡äº†ä¸€ä¸ª`reward_function`ã€‚
* **æ•°æ®å¤„ç†è¿‡ç¨‹**

ä»¥å®˜æ–¹ä»£ç ä¸ºä¾‹ï¼ˆè®­ç»ƒä¸€ä¸ªå…·æœ‰æ€è€ƒè¿‡ç¨‹çš„å¤šæ¨¡æ€æ¨¡å‹ï¼‰ï¼Œåœ¨æ•°æ®å¤„ç†å±‚é¢ä½¿ç”¨ç±»ä¼¼å¦‚ä¸‹æ•°æ®é›†
![image.png](https://s2.loli.net/2025/09/05/3xYD4jFp5VsyPeI.webp)
ä»¥ä¸ºéœ€è¦è®¾è®¡ä¸€ä¸ªâ€œè¾“å‡ºâ€æ€è€ƒè¿‡ç¨‹çš„æ¨¡å‹å› æ­¤è®¾è®¡è®¾è®¡å…·æœ‰â€œæ€è€ƒâ€è¿‡ç¨‹çš„promptï¼Œæœ€åè¾“å…¥æ¨¡å‹æ•°æ®æ ¼å¼ä¸ºï¼š
```python
# åŸå§‹æ–‡æœ¬
{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=147x86 at 0x7FF65C5776D0>,
 'original_answer': ...,
 'original_question': ...,
 'problem': ...
 'prompt': [{'content': 'system-content',
             'role': 'system'},
            {'content': 'user-content',
             'role': 'user'}],
 'solution': "<think>...</think>'
             '<answer>...</answer>'}
# åˆæ­¥å¤„ç†åæ–‡æœ¬
{'The prompt Text: '
'<|im_start|>system\n systen-content <|im_end|>\n'
'<|im_start|>user\n user-content <|im_end|>\n'
'<|im_start|>assistant\n'}
# æ¨¡å‹æœ€åå¾—åˆ°çš„è¾“å‡º
output = {
            "prompt_ids": prompt_ids,
            "prompt_mask": prompt_mask,
            "completion_ids": completion_ids,
            "completion_mask": completion_mask,
            "advantages": advantages,
            "num_items_in_batch": num_items_in_batch,
        }
```
ä¸è¿‡åœ¨å¾—åˆ°ç±»ä¼¼ä¸Šé¢æ•°æ®é›†ä¹‹åï¼Œä¸æ˜¯ç›´æ¥ä¸¢åˆ°æ¨¡å‹é‡Œé¢è¿›è¡Œå¤„ç†ï¼Œåœ¨DPOTrainerä¸­é¦–å…ˆä¼šå»ç”±`_prepare_inputs`ï¼ˆ[ä»£ç ](https://github.com/huggingface/trl/blob/67991605c0e6aaf1ef3c2bf64e11da914948c4a4/trl/trainer/grpo_trainer.py#L975)ï¼‰å‡½æ•°è¿›è¡Œå¤„ç†ï¼Œå¯¹äºæµ‹è¯•ç›´æ¥é€šè¿‡å‡½æ•° ` self._generate_and_score_completions(...)`å¤„ç†ï¼Œå¯¹äºè®­ç»ƒæ•°æ®é›†
> `_generate_and_score_completions`ï¼š
> **ç¬¬ä¸€æ­¥ã€æ ¼å¼åŒ–æ•°æ®**ã€‚ï¼ˆå¯¹äºå¤šæ¨¡æ€/åªæœ‰æ–‡æœ¬ï¼‰è¿™ä¸ªè¿‡ç¨‹ä¸»è¦æ˜¯äº‰å¯¹æˆ‘ä¸Šé¢æ•°æ®ä¸­çš„`prompt`ç›´æ¥é€šè¿‡æ¨¡æ¿è¿›è¡Œå¤„ç†å¾—åˆ°`prompts_text`ï¼Œè€Œåå°±æ˜¯ç›´æ¥å†å»é€šè¿‡ `processing_claa`ï¼ˆç›´æ¥è°ƒç”¨QwenVLçš„processorï¼‰å¤„ç†å¾—åˆ°`prompt_inputs`ï¼Œè€Œåå°±æ˜¯å¦‚æœ`self.max_prompt_length`é‚£ä¹ˆå°±ä¼šå»å¯¹å¤šæ¨¡æ€ï¼ˆæ–‡å­— + å›¾åƒï¼‰è¾“å…¥æ—¶ï¼Œå¯¹ `prompt_inputs["input_ids"]`è¿˜åŸæ–‡æœ¬ç„¶åå»é™¤ç±»ä¼¼`<pad>`å’Œä¸€äº›é‡å¤/é”™è¯¯çš„ `<image>`å¾—åˆ°å¹²å‡€çš„ `prompts_text`ã€‚
> **ç¬¬äºŒæ­¥ã€ç”Ÿæˆå›ç­”**ã€‚åœ¨`trl`ä¸­ä½¿ç”¨äº†3ç§ç”Ÿæˆæ–¹å¼ï¼š1ã€ç›´æ¥ç”¨æ¨¡å‹ç”Ÿæˆï¼›2ã€ä½¿ç”¨vllmæ–¹å¼ç”Ÿæˆï¼›3ã€ä½¿ç”¨use_transformers_pagedæ–¹å¼ã€‚å¯¹äºç”Ÿæˆï¼ˆç›´æ¥é€šè¿‡æ¨¡å‹ï¼‰è¿‡ç¨‹è€Œè¨€å°±æ¯”è¾ƒç®€å•ç›´æ¥å°†`prompt_inputs["input_ids"]` å’Œ `prompt_inputs["attention_mask"]` ä¸¢åˆ°æ¨¡å‹é‡Œé¢å¾—åˆ°`prompt_completion_ids`å†å»å°† promptå†…å®¹å’Œå›ç­”æˆªå–å‡ºæ¥å¾—åˆ° `prompt_ids` å’Œ `completion_ids`
> **ç¬¬ä¸‰æ­¥ã€è®¡ç®—å¥–åŠ±å€¼**ã€‚è¿™ä¸ªè¿‡ç¨‹å°±æ¯”è¾ƒç®€å•ï¼Œç›´æ¥å°†æ¨¡å‹çš„å›ç­”è¿›è¡Œè§£ç å†å»é€šè¿‡å¥–åŠ±å‡½æ•°è®¡ç®—å›ç­”çš„å¥–åŠ±å€¼ï¼Œè€Œåå½’ä¸€åŒ–æˆä¼˜åŠ¿å‡½æ•°ï¼ˆ`advantages`ï¼‰ï¼ŒæŒ‰ groupï¼ˆä¸€æ¬¡ç”Ÿæˆå¤šä¸ªæ ·æœ¬ï¼‰ç®—å‡å€¼ï¼Œè®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ ç›¸å¯¹ä¼˜åŠ¿ï¼ˆæ¯”å¦‚è¯´ä¸¤ä¸ªå›ç­”æ‰“åˆ†ä¸º [0.8, 0.5]é‚£ä¹ˆå‡å» group å†…å‡å€¼ï¼Œå‡è®¾ä¸º[+0.15, -0.15]ï¼‰
> **æœ€åã€è¿”å›è¾“å‡º**ã€‚
> ![image.png](https://s2.loli.net/2025/09/05/f2loj6LEVUwr7Kg.webp)
> åœ¨æœ€åè¿”å›çš„è¾“å‡ºä¸­ `old_per_token_logps` å’Œ `ref_per_token_logps`å¤„ç†ç›´æ¥é€šè¿‡å‡½æ•°`_get_per_token_logps_and_entropies`ï¼ˆå°±ç›¸å½“äºæŠŠ ç¬¬äºŒæ­¥å¾—åˆ°çš„ `prompt_completion_ids`åœ¨äº¤ç»™æ¨¡å‹é‡Œé¢å»è®¡ç®—æ¯ä¸ªtokençš„æ¦‚ç‡ï¼‰

* **å¥–åŠ±å‡½æ•°è®¾è®¡**

GRPOæ²¡æœ‰ä½¿ç”¨ref_modelè½¬è€Œä½¿ç”¨å¥–åŠ±å‡½æ•°ï¼Œå¯¹äºå¥–åŠ±å‡½æ•°è®¾è®¡ï¼š`think_format_reward`ï¼Œ `accuracy_reward`ã€‚å¯¹äº`accuracy_reward`å¾ˆå®¹æ˜“ç†è§£ä»£ç å°±æ˜¯**ç›´æ¥å¯¹æ¯”æ¨¡å‹è¾“å‡ºå’Œç­”æ¡ˆä¹‹é—´æ˜¯å¦æ­£ç¡®**ï¼ˆé€šè¿‡`parse` [`from math_verify import LatexExtractionConfig, parse, verify`] å»è§£ææœ€åè¾“å‡ºæ‰“ç­”æ¡ˆç„¶åå¯¹æ¯”ä¸¤è€…ä¹‹é—´æ˜¯å¦æ­£ç¡®ï¼‰ã€‚å¯¹äº`think_format_reward`ï¼šè¿™ä¸ªæ›´åŠ ç›´æ¥ï¼Œç›´æ¥å»åˆ¤æ–­è¾“å‡ºæ˜¯ä¸æ˜¯æœ‰ `<think>...</think>` åŒ…è£¹ï¼ˆæœ‰=1ï¼Œæ— /ç¼ºå¤±=0ï¼‰
å½“ç„¶ä¸ä¸€å®šè¦ä½¿ç”¨è‡ªå®šä¹‰çš„ï¼ˆè¿™ä¹ˆç²—ç³™çš„ï¼‰åœ¨DPOTrainerä¸­å¯¹äº`self.reward_funcs`ï¼ˆ[ä»£ç ](https://github.com/huggingface/trl/blob/18633dbb06ff6efc5099779592ba180d8ca767ea/trl/trainer/grpo_trainer.py#L290C9-L302C41)ï¼‰ä¹Ÿå¯ä»¥ç›´æ¥å»åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ `AutoModelForSequenceClassification.from_pretrained(...)`
* **æ¨¡å‹å¤„ç†è¿‡ç¨‹**

ç›´æ¥å»çœ‹lossè®¡ç®—è¿‡ç¨‹ï¼š
```python
def compute_loss(self, model, inputs, return_outputs, num_items_in_batch):
    ...
    if self.use_liger_loss:
        unwrapped_model = self.accelerator.unwrap_model(model)
        return self._forward_redirection(model, unwrapped_model, self.compute_liger_loss, unwrapped_model, inputs)
    else:
        return self._compute_loss(model, inputs)
```
å…¶ä¸­ä½¿ç”¨äº†ä¸¤ç§losså¤„ç†è¿‡ç¨‹ï¼š`_forward_redirection` ä»¥åŠ `_compute_loss`ã€‚
* `self._compute_loss` å¤„ç†è¿‡ç¨‹ï¼ˆ[Github-ä»£ç ](https://github.com/huggingface/trl/blob/67991605c0e6aaf1ef3c2bf64e11da914948c4a4/trl/trainer/grpo_trainer.py#L1626)ï¼‰ï¼ˆå®é™…è§£é‡Šä½¿ç”¨ **trl:0.22.1ç‰ˆæœ¬ä»£ç **å’Œgithubæœ‰å·®å¼‚ï¼‰

é¦–å…ˆæ˜¯å°†è¾“å…¥é—®é¢˜å’Œå›ç­”æ‹¼æ¥èµ·æ¥ï¼Œç„¶åç›´æ¥ä¸¢åˆ°`self._get_per_token_logps_and_entropies`ï¼ˆç›´æ¥å°†æ•°æ®ä¸¢åˆ°æ¨¡å‹ä¸­ï¼Œè€Œåå»æˆªå–æ¨¡å‹è¾“å‡ºä¸­â€œçœŸæ­£å›ç­”â€çš„å†…å®¹ï¼‰ä¸­è¿›è¡Œå¤„ç†å¾—åˆ°`per_token_logps`ï¼ˆæ¯ä¸ªtokençš„æ¦‚ç‡ï¼‰ï¼Œ`entropies`ï¼ˆæ¯ä¸ªtokençš„ä¿¡æ¯ç†µï¼‰ï¼Œè€Œåå°±æ˜¯é€šè¿‡é«˜ç†µå»è¿‡æ»¤tokenåªåœ¨**é«˜ç†µä½ç½®è®¡ç®— loss**ï¼Œè€Œåå°±æ˜¯**è®¡ç®—KLæ•£åº¦**ï¼ˆ`torch.exp(inputs["ref_per_token_logps"] - per_token_logps) - (inputs["ref_per_token_logps"] - per_token_logps) - 1)`ï¼‰ï¼Œé¿å…æ–°ç­–ç•¥æ¼‚ç§»å¤ªè¿œ

> `self._get_per_token_logps_and_entropies`å¤„ç†è¿‡ç¨‹ï¼ˆ[Github-ä»£ç ](https://github.com/huggingface/trl/blob/67991605c0e6aaf1ef3c2bf64e11da914948c4a4/trl/trainer/grpo_trainer.py#L786)ï¼‰ï¼ˆå®é™…è§£é‡Šä½¿ç”¨ **trl:0.22.1ç‰ˆæœ¬ä»£ç **å’Œgithubæœ‰å·®å¼‚ï¼‰
> å…¶å¤„ç†è¿‡ç¨‹æ¯”è¾ƒç®€å•ï¼Œç›´æ¥å°†æ‰€æœ‰çš„æ•°æ®éƒ½å¤„ç†æˆæ¨¡å‹è¾“å…¥ï¼ˆGRPOä¸æƒ³DPOé‚£æ ·éœ€è¦å°†3å…ƒç»„è¿›è¡Œæ‹†å¼€æ‹¼æ¥ï¼‰å¦‚ï¼šinput_idsã€pixel_valuesç­‰ç„¶åç›´æ¥`logits = model(**model_inputs).logits`åœ¨å¾—åˆ°æ¨¡å‹çš„è¾“å‡ºä¹‹ååç»­å°±æ˜¯å¯¹è¾“å‡ºåšä¸€äº›æˆªæ–­å¤„ç†ï¼ˆå¦‚åªéœ€è¦æ¨¡å‹å›ç­”éƒ¨åˆ†çš„è¾“å‡º`logits[:, -logits_to_keep:, :]`ï¼‰è€Œåå»è®¡ç®— `logits / self.temperature`ï¼ˆé€šè¿‡æ¸©åº¦ç³»æ•°æ¥ç¡®å®šè¾“å‡ºå†…å®¹å¤šæ ·åŒ–ï¼‰æœ€åå†å»é€šè¿‡ï¼š`logps = selective_log_softmax(logits, completion_ids)`ï¼ˆselective_log_softmaxåªå»è®¡ç®—completion_idséƒ¨åˆ†çš„log_softmaxå€¼ï¼‰å°±å¯ä»¥å¾—åˆ°æœ€åçš„å€¼ã€‚

#### RL-GRPOå¤„ç†è¿‡ç¨‹æ€»ç»“
![1.png](https://s2.loli.net/2025/09/21/x45DlMb6QVPuh7r.webp)
å¯¹äºä¸Šé¢lossè®¡ç®—å…¬å¼ä¸­ä¸»è¦å°±æ˜¯å¦‚ä¸‹å‡ ä¸ªå€¼éœ€è¦å…³æ³¨ï¼š1ã€advantageå€¼ï¼›2ã€KLæ•£åº¦å€¼ã€‚
å› æ­¤ç®€å•æ€»ç»“ä¸€äº›GRPOä»£ç å¤„ç†è¿‡ç¨‹[^1]ï¼Œ**é¦–å…ˆ**ï¼Œå¯¹äºæ•°æ®å¤„ç†ï¼Œè¿™å—å†…å®¹æ¯”è¾ƒç®€å•ç›´æ¥ **æ¨¡æ¿åŒ–**ã€**ç¼–ç å†…å®¹å³å¯**ï¼Œå› ä¸ºGRPOæ˜¯â€œä¸€ä¸ªé—®é¢˜æŠ›å‡ºå¤šç»„å›ç­”ç„¶åè¯„ä¼°å›ç­”â€ï¼Œå› æ­¤åœ¨æ•°æ®å¤„ç†è¿‡ç¨‹ä¸­é€šè¿‡æ¨¡å‹ç”Ÿæˆå›ç­” `prompt_completion_ids=model.generate(...)`è€Œåéœ€è¦åšçš„å°±æ˜¯å°†ç”Ÿæˆå†…å®¹è¿›è¡Œæ‹†åˆ†å¾—åˆ°`prompt_ids`å’Œ `completion_ids`ï¼ˆå¾—åˆ°è¿™ä¸€éƒ¨åˆ†å€¼ä¹‹åå°±åªéœ€è¦åœ¨å»è¿˜åŸæˆtextæ–‡æœ¬ç„¶åå†å»é€šè¿‡rewardå‡½æ•°å»è®¡ç®—rewardå€¼ä»¥åŠè®¡ç®—æœ€åéœ€è¦çš„ `advantage`å€¼ï¼‰ï¼Œé™¤æ­¤ä¹‹å¤–è¿˜ä¼šå»é€šè¿‡modelå’Œmodel_refåˆ†åˆ«è®¡ç®—å›ç­”ä¸­æ¯ä¸ªtokençš„logitså€¼ï¼š`old_per_token_logps` å’Œ `ref_per_token_logps`
> è¿™ä¸ªè¿‡ç¨‹ç›´æ¥é€šè¿‡å‡½æ•° [_get_per_token_logps_and_entropies](https://github.com/huggingface/trl/blob/67991605c0e6aaf1ef3c2bf64e11da914948c4a4/trl/trainer/grpo_trainer.py#L786)å¤„ç†ï¼Œä»–çš„å¤„ç†æ€è·¯ç®€å•ç›´æ¥å°† modeléœ€è¦çš„å†…å®¹å†ä¸¢åˆ°modelé‡Œé¢å¾—åˆ°æ¯ä¸ªtokençš„logitsç„¶åå†å»è®¡ç®—softmaxå€¼

æœ€åå¾—åˆ°ä¸€ä¸ªå®Œæ•´çš„outputå¦‚ä¸‹ï¼š
```python
output = {
    "prompt_ids": prompt_ids,    # é—®é¢˜token
    "prompt_mask": prompt_mask,
    "completion_ids": completion_ids,    # é—®é¢˜çš„å›ç­”token
    "completion_mask": completion_mask,
    "advantages": advantages,
    "num_items_in_batch": num_items_in_batch,
    "old_per_token_logps": old_per_token_logps  
    "importance_sampling_ratio": importance_sampling_ratio  
    "ref_per_token_logps": ref_per_token_logps   
    "pixel_values": prompt_inputs["pixel_values"]   
    "image_grid_thw": prompt_inputs["image_grid_thw"]   
    "pixel_attention_mask": prompt_inputs["pixel_attention_mask"]   
    "image_sizes": prompt_inputs["image_sizes"]   
}

```

**è€Œå**ï¼Œå¯¹äºlossè®¡ç®—è¿‡ç¨‹é¦–å…ˆå°†ä¸Šé¢outputä¸­çš„ é—®é¢˜+å›ç­”è¿›è¡Œç»„åˆå†ä¸¢åˆ°`_get_per_token_logps_and_entropies`ä¸­å¾—åˆ°æ¯ä¸ªtokenæ¦‚ç‡ä»¥åŠç†µçš„å€¼ï¼š`per_token_logps`ï¼Œ`entropies`ï¼Œè€Œåå°±æ˜¯ï¼š1ã€**é€‰æ‹©å‡ºé«˜ç†µå€¼çš„token**ï¼ˆ`entropy_mask`ï¼‰ï¼›2ã€**è®¡ç®—KLæ•£åº¦**ï¼ˆ`torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1`ï¼‰ï¼›3ã€**é‡è¦æ€§é‡‡æ ·æƒé‡**ï¼šæ¯”è¾ƒå½“å‰ log æ¦‚ç‡å’Œæ—§ç­–ç•¥ï¼ˆ`per_token_logps - old_per_token_logps`ï¼‰ï¼Œå¾—åˆ° importance weightï¼Œåš clipping é™åˆ¶ã€‚æ„é€ ä¸¤ä¸ªå€™é€‰ lossï¼ˆä¸è£å‰ªå’Œè£å‰ªï¼‰ï¼Œå–æœ€å°å€¼ï¼Œå½¢æˆ `per_token_loss`å†å»ä¹˜ä¸Š entropy_maskå’ŒåŠ ä¸Š KL æƒ©ç½šé¡¹å°±å¯ä»¥å¾—åˆ°æœ€åçš„losså€¼ã€‚
#### RL-PPOå¤„ç†ä»£ç 
å€Ÿç”¨huggingfaceä¸­å¯¹äºPPOè¿‡ç¨‹æè¿°å›¾ï¼š
![image.png](https://s2.loli.net/2025/09/05/AvLeinFOo5lPV6z.webp)
å¯¹äº[ä»£ç ](https://github.com/huggingface/trl/blob/1d06757e57723e85048ab7b061b12aac8895ca89/trl/trainer/ppo_trainer.py#L100)ä½¿ç”¨ï¼Œç›¸æ¯”è¾ƒGRPOå’ŒDPOè¦ç®€å•å¾ˆå¤šï¼ˆä¸è¿‡åœ¨ä½¿ç”¨æ¨¡å‹ä¸Šï¼ŒDPOå’ŒPPOéƒ½éœ€è¦åŠ è½½modelå’Œref_modelè€ŒGRPOåªéœ€è¦åŠ è½½ä¸€ä¸ªmodelï¼‰ï¼ŒæŒ‰ç…§ä¸Šé¢çš„å¤„ç†è¿‡ç¨‹ï¼š
**é¦–å…ˆ**è®¡ç®—rolloutè¾“å‡ºï¼Œç›´æ¥é€šè¿‡åŠ è½½çš„æ¨¡å‹ç„¶åæ¨¡å‹å¯¹äºâ€œé—®é¢˜â€å»å¾—åˆ°â€œå›ç­”â€`query_responses`ï¼ˆ**å®Œæ•´çš„æ¨¡å‹ç”Ÿæˆå†…å®¹**ï¼šprompt+æ¨¡å‹çš„å›ç­”ï¼‰ï¼Œ`logitss`ï¼Œæ¥ä¸‹æ¥ï¼ˆ[ä»£ç ](https://github.com/huggingface/trl/blob/9955ee7eaa7e361ef46f7ac26b5ddc79199811f8/trl/trainer/ppo_trainer.py#L473C21-L490C34)ï¼‰å»è®¡ç®—modelå’Œref_modelä¸­æ¯ä¸ªtokençš„logæ¦‚ç‡å€¼ï¼ˆè¿™ä¸ªè¿‡ç¨‹å’ŒGRPOå¤„ç†æ˜¯ä¸€æ ·çš„ï¼Œå°†é—®é¢˜+å›ç­”æ‹¼æ¥èµ·æ¥è€Œåä¸¢åˆ°æ¨¡å‹ä¸­è®¡ç®—æ¯ä¸ªtokençš„logæ¦‚ç‡å€¼ï¼‰æœ€ååˆ†åˆ«å¾—åˆ°æ¨¡å‹çš„è¾“å‡ºç»“æœï¼š`logprob` `response`ï¼ˆæˆªå–modelå›ç­”å†…å®¹ï¼‰ å’Œ `ref_logprob`ã€‚åé¢éƒ¨åˆ†ï¼ˆ[ä»£ç ](https://github.com/huggingface/trl/blob/9955ee7eaa7e361ef46f7ac26b5ddc79199811f8/trl/trainer/ppo_trainer.py#L492C21-L509C22)ï¼‰å°±æ˜¯ç›´æ¥æ ¹æ® `response`ï¼ˆmodelçš„å›ç­”ï¼‰ ä»¥åŠ `query`ï¼ˆå°±æ˜¯æˆ‘ä»¬çš„é—®é¢˜ï¼‰å»è®¡ç®—rewardçš„å€¼`scores`ã€‚
æ¥ä¸‹æ¥å¤„ç†è¿‡ç¨‹ï¼š1ã€å¤„ç† EOS ç¼ºå¤±æƒ©ç½šï¼šå°†socresä¸­å¦‚æœç”Ÿæˆå†…å®¹ä¸å«ç»“æŸæ ‡è®°å°±ä»`scores`ä¸­å‡å»æ•°å€¼ï¼›2ã€è®¡ç®—klä»¥åŠæœ€åçš„rewardså€¼ï¼Œå¯¹klç›´æ¥é¦–å…ˆé€šè¿‡maskå»æ©ç›–éƒ¨åˆ†logprobsï¼ˆref_logprobsï¼‰ç„¶åç›´æ¥é€šè¿‡ `kl = -(ref_logprobs - logprobs) if args.kl_estimator == "k1" else ((ref_logprobs - logprobs).exp() - 1) - logr`å¾—åˆ°klå€¼ï¼›3ã€è®¡ç®—advantageå€¼ï¼ˆ[ä»£ç ](https://github.com/huggingface/trl/blob/9955ee7eaa7e361ef46f7ac26b5ddc79199811f8/trl/trainer/ppo_trainer.py#L561)ï¼‰
æœ€åå°±æ˜¯è¿­ä»£ä¼˜åŒ–æ¨¡å‹å‚æ•°ï¼ˆ[ä»£ç ](https://github.com/huggingface/trl/blob/9955ee7eaa7e361ef46f7ac26b5ddc79199811f8/trl/trainer/ppo_trainer.py#L576C13-L654C34)ï¼‰è¿™ä¸ªè¿‡ç¨‹ï¼ˆå¯¹é‡‡æ ·å¾—åˆ°çš„ä¸€æ‰¹åºåˆ—æ•°æ®åšå¤šè½®ï¼ˆnum_ppo_epochsï¼‰å°æ‰¹æ¬¡æ›´æ–°ï¼Œé€šè¿‡ ratio = Ï€Î¸/Ï€_old å’Œè£å‰ªï¼ˆclipï¼‰æ¥æ„é€ ç­–ç•¥æŸå¤±ï¼ŒåŒæ—¶å¯¹ä»·å€¼å‡½æ•°åšè£å‰ªçš„ value lossï¼‰ä¸»è¦æ˜¯è¿›è¡Œå¦‚ä¸‹å¤„ç†æµç¨‹ï¼šé¦–å…ˆæ˜¯ç›´æ¥å°†æœ€ä¸Šé¢å¾—åˆ°çš„`query_responses`ä¸­é€‰æ‹©éƒ¨åˆ†ä¾‹å­ä¸¢åˆ°æ¨¡å‹ä¸­å»è®¡ç®—æ¯ä¸€ä¸ªtokençš„logitsï¼ˆ `new_logprobs = selective_log_softmax(logits, mb_responses)
`ï¼‰ è€Œåè®¡ç®—ç­–ç•¥æŸå¤±å€¼ï¼ˆ`pg_loss`ï¼‰ä»¥åŠvf_loss 
> å›é¡¾ä¸€ä¸‹ï¼Œå¯¹äºåŠ è½½çš„**llmåœ¨ä½¿ç”¨generate**æ—¶ä¸€èˆ¬è¿”å›å¦‚ä¸‹4ä¸ªå€¼ï¼š
> `sequences`ï¼šç”Ÿæˆçš„ token idsï¼ˆè·Ÿé»˜è®¤è¿”å›ä¸€æ ·ï¼‰ï¼›
> `scores`ï¼šæ¯ä¸€æ­¥çš„ logitsï¼ˆå¦‚æœ output_scores=Trueï¼‰
> `attentions`ï¼šæ³¨æ„åŠ›çŸ©é˜µï¼ˆå¦‚æœ output_attentions=Trueï¼‰
> `hidden_states`ï¼šéšè—å±‚è¡¨ç¤ºï¼ˆå¦‚æœ output_hidden_states=Trueï¼‰
> ä¸€èˆ¬è€Œè¨€ä½¿ç”¨åˆ°çš„ä¸»è¦æ˜¯ä¸Šé¢ä¸¤é¡¹ï¼Œå¯¹äº**ç¬¬ä¸€é¡¹**`sequences`ä¸€èˆ¬å¾—åˆ°çš„å®Œæ•´çš„å›ç­”ï¼ˆprompt+æ¨¡å‹ç”Ÿæˆçš„å†…å®¹ï¼‰ï¼Œæ‰€ä»¥ä¸€èˆ¬ä¼šæœ‰ä¸€ä¸ªæˆªå–å¤„ç†ï¼ˆåªéœ€è¦è®°å½•`inputs['input_ids'].shape[1]`ç„¶åå»æˆªå–å³å¯ï¼‰ï¼›å¯¹äº**ç¬¬äºŒé¡¹**`scores`ä¸€èˆ¬å¾—åˆ°çš„æ˜¯é€šå¸¸æ˜¯logitsï¼ˆéœ€è¦å»é€šè¿‡softmaxè®¡ç®—æ‰èƒ½å¾—åˆ°tokenæ¦‚ç‡ï¼‰ï¼›å› æ­¤åœ¨GRPOå’ŒPPOä¸­ä¸ºäº†**å¾—åˆ°æ¯ä¸€ä¸ªtokençš„logæ¦‚ç‡å€¼**ï¼Œ`logprob = selective_log_softmax(logits, response)`ç›´æ¥é€šè¿‡è¿™ç§æ–¹å¼å»è®¡ç®—æ¥èŠ‚çº¦æ˜¾å­˜ã€‚
> é™¤æ­¤ä¹‹å¤–ä¹Ÿæœ‰ç›´æ¥é€šè¿‡ `model(**model_inputs)`è¿™æ ·å¤„ç†ä¸€èˆ¬å¾—åˆ°çš„æ˜¯

#### RL-PPOå¤„ç†è¿‡ç¨‹æ€»ç»“
**ç¬¬ä¸€é˜¶æ®µ**ï¼šé¦–å…ˆæ˜¯å¯¹äºé—®é¢˜ï¼ˆ`query`ï¼‰é€šè¿‡ä¸¢åˆ°æ¨¡å‹`batch_generation`ä¸­å¤„ç†å¾—åˆ°`query_responses`ï¼ˆå®Œæ•´é—®é¢˜+æ¨¡å‹å›ç­”ï¼‰ å’Œ`logitss`ï¼ˆæ¯ä¸ªtokenå¯¹åº”çš„æ¦‚ç‡ï¼‰ï¼Œè¿›ä¸€æ­¥å°†å…¶å¾—åˆ°å›ç­”tokençš„æ¦‚ç‡å€¼`logprob`ï¼ˆ`selective_log_softmax`ï¼‰åŒæ ·çš„å¤„ç†è¿‡ç¨‹é€šè¿‡policy_modelå°†`query_response`ï¼ˆä» `query_responses`æŒ‘é€‰çš„ï¼‰è¾“å…¥åˆ°æ¨¡å‹è¿›è¡Œå¤„ç†åŒæ ·çš„å¤„ç†å¾—åˆ°`ref_logprob`ï¼Œæœ€åå°±æ˜¯é€šè¿‡`reward_model`å»è®¡ç®—ï¼ˆ`torch.cat((query, postprocessed_response), 1)`ï¼‰å¾—åˆ°å¥–åŠ±å€¼ã€‚
**ç¬¬äºŒé˜¶æ®µ**ï¼š**klå€¼**ï¼šç›´æ¥è®¡ç®—`ref_logprobs - logprobs`ï¼ˆä¹Ÿå°±æ˜¯è®¡ç®—ä¸Šé¢é˜¶æ®µçš„ref_logprobå’Œ logprobä¹‹é—´å·®å€¼ï¼‰ï¼›**rewardså€¼**ï¼šç›´æ¥copyè®¡ç®—çš„klç»“æœç„¶åå†åºåˆ—çš„ç»“å°¾è¡¥å……ä¸Šscoresï¼›**advantageå€¼**ï¼šæ ¹æ® reward å’Œ valueï¼Œç”¨ GAE ç®— advantageã€‚GAEè®¡ç®—è¿‡ç¨‹ï¼š$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$  å’Œ$A_t = \delta_t + \gamma \lambda A_{t+1}$æœ€åè®¡ç®—`advantages + values`ä¹Ÿå°±æ˜¯ $R_t=A_t+V(s_t)$
**ç¬¬ä¸‰é˜¶æ®µ**ï¼šè¿›è¡Œè¿­ä»£ä¼˜åŒ–æ¨¡å‹å‚æ•°è¿‡ç¨‹ï¼Œä¼˜åŒ–è¿‡ç¨‹é¦–å…ˆæ˜¯ç›´æ¥å°†å°æ‰¹æ¬¡çš„`query_responses` è¾“å…¥åˆ°æ¨¡å‹ä¸­è®¡ç®—å¾—åˆ°`output, vpred_temp`ç„¶åå°±æ˜¯è€æ“çºµå¾—åˆ°æ¯ä¸ªtokençš„logitså€¼`new_logprobs`ï¼Œç„¶åè®¡ç®—å»è®¡ç®—`vf_loss`ï¼šè®¡ç®—loss1ï¼ˆ`torch.square(vpred - mb_return)`ï¼‰å’Œloss2ï¼ˆ`torch.square(vpredclipped - mb_return)`ï¼‰çš„æœ€å¤§å€¼ã€‚`pg_loss`ï¼šè®¡ç®—loss1ï¼ˆ`-mb_advantage * ratio`ï¼‰å’Œloss2ï¼ˆ`-mb_advantage * torch.clamp(ratio, 1.0 - args.cliprange, 1.0 + args.cliprange)`ï¼‰çš„æœ€å¤§å€¼ç„¶åå–meanã€‚æœ€åå¾—åˆ°lossä¸º`pg_loss + args.vf_coef * vf_loss`
> vpredã€vpredclippedã€mb_returnåˆ†åˆ«é€šè¿‡ä»vpred_tempé€‰æ‹©å›ç­”tokenã€å¯¹vpredè¿›è¡Œclampè£å‰ªã€advantages + values

### RLç®—æ³•å¯¹æ¯”
#### å¯¹æ¯”ä¸€ä¸‹GRPOå’ŒDPOçš„å¤„ç†è¿‡ç¨‹
**DPOçº¯æ•°æ®é©±åŠ¨è¿‡ç¨‹**ï¼Œæ•°æ®é©±åŠ¨ï¼šè®­ç»ƒæ—¶éœ€è¦æ ‡æ³¨å¥½çš„åå¥½å¯¹ï¼š$[q, y^+], [q, y^-]$ã€‚è®¡ç®—æµç¨‹ï¼š1. è¾“å…¥åŒä¸€ä¸ªé—®é¢˜ $q$ï¼Œåˆ†åˆ«æ‹¼æ¥ä¸Šæ­£æ ·æœ¬å›ç­” $y^+$ å’Œè´Ÿæ ·æœ¬å›ç­” $y^-$ã€‚2. ç”¨å½“å‰æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹åˆ†åˆ«è®¡ç®— $\log \pi_\theta(y^+|q), \log \pi_\theta(y^-|q), \log \pi_{\text{ref}}(y^+|q), \log \pi_{\text{ref}}(y^-|q)$ã€‚3. åŸºäºè¿™ 4 ä¸ª log-probï¼Œç›´æ¥è®¡ç®—ä¸€ä¸ª logistic å›å½’å¼çš„ lossï¼Œå¼ºåˆ¶æ¨¡å‹åœ¨æ­£æ ·æœ¬ä¸Šæ¯”åˆ†æ•°æ›´é«˜ï¼Œåœ¨è´Ÿæ ·æœ¬ä¸Šæ¯”åˆ†æ•°æ›´ä½ã€‚
**GRPOç”Ÿæˆé©±åŠ¨è¿‡ç¨‹**ï¼Œç”Ÿæˆé©±åŠ¨ï¼šè®­ç»ƒæ—¶åªç»™å®šé—®é¢˜ promptï¼Œæ¨¡å‹è‡ªå·± roll-out å¤šä¸ªå›ç­”ã€‚è®¡ç®—æµç¨‹ï¼š1. å¯¹æ¯ä¸ªé—®é¢˜ç”Ÿæˆ $G$ ä¸ªå›ç­”ã€‚2. é€šè¿‡å¥–åŠ±å‡½æ•°ï¼ˆæˆ–æ‰“åˆ†å™¨ï¼‰ç»™æ¯ä¸ªå›ç­”æ‰“åˆ† $r_i$ã€‚3. ç»„å†…å½’ä¸€åŒ–å¥–åŠ± â†’ å¾—åˆ° advantage å€¼ $A_i$ï¼ˆæ¯”ç»„å†…å¹³å‡å¥½/å·®å¤šå°‘ï¼‰ã€‚4. ç”¨å‚è€ƒæ¨¡å‹è®¡ç®— ref_per_token_logpsï¼ˆä½¿ç”¨ref_modelç”Ÿæˆæ²¡æœ‰çš„è¯ç›´æ¥ç”¨modelä»£æ›¿ref_modelï¼‰ã€‚5. ç”¨æ—§ç­–ç•¥ï¼ˆå†»ç»“ä¸€å¸§çš„å½“å‰æ¨¡å‹ï¼‰å¾—åˆ° old_per_token_logpsï¼ˆç›´æ¥é€šè¿‡modelç”Ÿæˆï¼‰ã€‚6. ç”¨å½“å‰æ¨¡å‹å¾—åˆ° per_token_logpsã€‚7. è®¡ç®—é‡è¦æ€§æ¯”ç‡å’Œ KL æ•£åº¦ï¼ˆä½¿ç”¨per_token_logpså’Œref_per_token_logpsè®¡ç®—ï¼‰è¿‘ä¼¼ï¼Œå†å¥— PPO é£æ ¼çš„å‰ªåˆ‡ç›®æ ‡ï¼ˆä½¿ç”¨old_per_token_logpså’Œper_token_logpï¼‰ â†’ æœ€ç»ˆ lossã€‚
#### å¯¹äºDPOã€GRPOã€PPOä¸­KLè®¡ç®—å·®å¼‚
> $KL(p||q)=\sum_x p(x)\log\frac{p(x)}{q(x)}=H(p,q)-H(q)$ï¼Œäº¤å‰ç†µ-ç†µ
> è®¡ç®—äº¤å‰ç†µçš„ç›®çš„åœ¨äº**çº¦æŸæ–°ç­–ç•¥ä¸è¦åç¦»å‚è€ƒç­–ç•¥å¤ªå¤š**ï¼Œç±»ä¼¼çš„å¯¹äºäº¤å‰ç†µæŸå¤±ï¼ˆ$H(p,q)=-\sum_x p(x)\log q(x)$ï¼‰ä¸¤è€…ä¹‹é—´å·®å¼‚æ˜¯äº¤å‰ç†µæ˜¯è®©â€œqå»æ‹Ÿåˆpâ€ï¼Œè€ŒKLåˆ™æ˜¯åº¦é‡â€œqå’Œpä¹‹é—´è·ç¦»â€

**1ã€DPOä¸­è®¡ç®—KL**ï¼šåœ¨model_refä»¥åŠmodelåˆ†åˆ«è¾“å…¥â€œ3å…ƒç»„â€æ•°æ®ä¹‹åä¼šå»è®¡ç®—ä¸åŒtokençš„æ¦‚ç‡å€¼ï¼Œä¹Ÿå°±æ˜¯modelå’Œreféƒ½ä¼šç”Ÿæˆ rejectå’Œchooseçš„æ¦‚ç‡å€¼ï¼Œç„¶åå»è®¡ç®—ï¼š$\mathrm{loss}=-\frac{1}{N}\sum_{i=1}^{N}\log\sigma\left(\beta\cdot((\log\pi_{\theta}(y_{w}|x)-\log\pi_{\theta}(y_{l}|x))-(\log\pi_{\mathrm{ref}}(y_{w}|x)-\log\pi_{\mathrm{ref}}(y_{l}|x)))\right)$ çš„sigmoid æŸå¤±ä¼˜åŒ–ç›¸å¯¹åå¥½
**2ã€GRPOä¸­è®¡ç®—KL**ï¼šé€šè¿‡model_refå¯¹äºé—®é¢˜Qä»¥åŠæ¨¡å‹ç”Ÿæˆçš„å¤šç»„å›ç­”è¿›è€Œå¯ä»¥å¾—åˆ°æ¯ç»„å›ç­”çš„tokenæ¦‚ç‡ï¼š`ref_per_token_logps` è€Œåæˆ‘åˆé€šè¿‡modelå»ç”Ÿæˆå¤šç»„å›ç­”ä»¥åŠtokenæ¦‚ç‡ï¼š`per_token_logps`æ¥ä¸‹æ¥å°±æ˜¯ç›´æ¥ä»–ä»¬ä¹‹é—´KLæ•£åº¦ï¼š
![](https://s2.loli.net/2025/09/21/UwmkqNA42lgvzWy.webp)
**3ã€PPOä¸­è®¡ç®—KL**ï¼šé€šè¿‡modelå¾—åˆ°å›ç­”ä¸­çš„æ¯ä¸€ä¸ªtokençš„æ¦‚ç‡`logprobs`ï¼ŒåŒæ ·çš„å†å»é€šè¿‡model_rfä¹Ÿå»è®¡ç®—æ¯ä¸€ä¸ªtokençš„æ¦‚ç‡`ref_logprobs`ç„¶åå»è®¡ç®—KL
![](https://s2.loli.net/2025/09/21/EsyjUOIolMTDJHm.webp)
DPOï¼šé€šè¿‡â€œåå¥½å·®å€¼â€é—´æ¥å¼•å…¥ KL çº¦æŸï¼Œåé‡äº å¯¹æ¯”å­¦ä¹ ã€‚
GRPOï¼šæ˜¾å¼è®¡ç®— ç”Ÿæˆå€™é€‰ç»„çš„ token çº§ KLï¼Œä½œä¸ºæ­£åˆ™é¡¹ï¼Œä¿è¯æ¨¡å‹ä¸åç¦»å‚è€ƒç­–ç•¥ã€‚
PPOï¼šåŸºäºå½“å‰ç­–ç•¥ä¸å‚è€ƒç­–ç•¥ï¼ˆæˆ–æ—§ç­–ç•¥ï¼‰çš„ KLï¼Œå¸¸ä½œä¸º æ­£åˆ™æˆ– early stopping ä¿¡å·
#### å¯¹äºGRPOä»¥åŠPPOä¸­ä¼˜åŠ¿å€¼è®¡ç®—è¿‡ç¨‹
**GRPOä¼˜åŠ¿å€¼è®¡ç®—è¿‡ç¨‹**ï¼šå¯¹äºç»™å‡ºå¤šç»„å›ç­”ç›´æ¥é€šè¿‡å¥–åŠ±å‡½æ•°å»è®¡ç®—æ¯ç»„å›ç­”çš„å¥–åŠ±å€¼è€Œåå»ä¸Šè®¡ç®—ï¼š$A_i = \frac{r_i- mean(r)}{std(r)}$
**PPOä¼˜åŠ¿å€¼è®¡ç®—è¿‡ç¨‹**ï¼šä¸€èˆ¬ç›´æ¥é€šè¿‡å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡æ–¹æ³•GAEæ¥è®¡ç®—ä¼˜åŠ¿å€¼ï¼Œé¦–å…ˆé€šè¿‡å¥–[åŠ±å‡½æ•°è¯„ä¼°æ¨¡å‹è¾“å‡º](https://github.com/huggingface/trl/blob/9955ee7eaa7e361ef46f7ac26b5ddc79199811f8/trl/trainer/ppo_trainer.py#L503)ï¼ˆé—®é¢˜+å›ç­”ï¼‰ï¼Œè€Œå[è®¡ç®—GAE](https://github.com/huggingface/trl/blob/9955ee7eaa7e361ef46f7ac26b5ddc79199811f8/trl/trainer/ppo_trainer.py#L561C17-L569C76)

#### å¯¹æ¯”DPOã€GRPOã€PPOä¸­lossè®¡ç®—å·®å¼‚
DPOçš„lossè®¡ç®—ï¼š
$$
\mathcal{L}_{\text{DPO}} = -\frac{1}{N} \sum_{i=1}^{N} \log \sigma\left( \beta \underbrace{\left[ \log \pi_\theta(y_w|x) - \log \pi_\theta(y_l|x) \right]}_{\text{model ä¹‹é—´å·®å¼‚}} - \underbrace{\left( \log \pi_{\text{ref}}(y_w|x) - \log \pi_{\text{ref}}(y_l|x) \right)}_{\text{éšå« KL åŸºå‡†}} \right)
$$
GRPOçš„lossè®¡ç®—ï¼š
$$
\mathcal{L}_{\text{GRPO}} = -\mathbb{E}\left[ \frac{\pi_\theta(y|x)}{\pi_{\text{ref}}(y|x)} A(y) \right] + \lambda \, \mathrm{KL}\left( \pi_\theta \parallel \pi_{\text{ref}} \right)
$$
PPOçš„lossè®¡ç®—ï¼š
$$
r_t(\theta) = \exp\left( \log \pi_\theta(a_t|s_t) - \log \pi_{\text{ref}}(a_t|s_t) \right)
$$


$$
\mathcal{L}_{\text{PPO}} = -\mathbb{E}\left[ \min\left( r_t(\theta) A_t, \, \mathrm{clip}\left(r_t(\theta), \, 1 - \epsilon, \, 1 + \epsilon\right) A_t \right) \right] + \lambda \, \mathrm{KL}\left( \pi_\theta \parallel \pi_{\text{ref}} \right)
$$

## å‚è€ƒ
[^1]: https://huggingface.co/docs/trl/main/en/grpo_trainer