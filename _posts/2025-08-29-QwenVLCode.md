---
layout: mypost
title: å¤šæ¨¡æ€æ¨¡å‹â€”â€”QwenVL2.5çš„å¾®è°ƒä»¥åŠå¼ºåŒ–å­¦ä¹ ä»£ç æ“ä½œ
categories: å¤šæ¨¡æ€
address: æ­¦æ±‰ğŸ¯
extMath: true
show_footer_image: true
tags:
- å¤šæ¨¡æ€
- QwenVL
description: ä»ä»£ç è§’åº¦è§£æQwenVL2.5æ¨¡å‹å¤„ç†æµç¨‹ï¼ŒåŒ…æ‹¬æ¨¡æ¿åŒ–è¾“å…¥ï¼ˆé€šè¿‡processor.apply_chat_templateå¤„ç†å¯¹è¯messagesï¼Œdata_loaderé¢„å¤„ç†ï¼‰ã€ç¼–ç ï¼ˆsmart_resizeåŠ¨æ€è°ƒæ•´å›¾åƒåˆ†è¾¨ç‡ï¼ŒQwen2VLImageProcessorå®Œæˆå½’ä¸€åŒ–åŠpatchåˆ‡å‰²ï¼‰ã€æ¨¡å‹å¤„ç†ï¼ˆVisionTransformerå¤„ç†pixel_valuesï¼Œç»Conv3dä¸window-attentionè®¡ç®—ï¼‰ï¼Œå¹¶ä»‹ç»å…¶SFTå’Œå¼ºåŒ–å­¦ä¹ å¾®è°ƒæ–¹æ³•ã€‚
---

ä»ä»£ç è§’åº¦å»ç†è§£QwenVL2.5æ˜¯å¦‚ä½•å¤„ç†ï¼Œä»¥åŠç»“åˆå®é™…æ“ä½œç†è§£å¦‚ä½•å»å¯¹ä¸€ä¸ªQwenVL2.5-3Bè¿›è¡ŒSFTå’Œå¼ºåŒ–å­¦ä¹ å¤„ç†ï¼Œé¦–å…ˆç›´æ¥é€šè¿‡å®˜æ–¹æä¾›ä¾‹å­äº†è§£QwenVLæ˜¯æ€ä¹ˆä½¿ç”¨çš„ï¼š
```python
from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor
from qwen_vl_utils import process_vision_info

model = Qwen2_5_VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2.5-VL-3B-Instruct", torch_dtype="auto", device_map="auto"
)

min_pixels = 256*28*28
max_pixels = 1280*28*28
processor = AutoProcessor.from_pretrained("Qwen/Qwen2.5-VL-3B-Instruct", min_pixels=min_pixels, max_pixels=max_pixels)

messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

# Preparation for inference
text = processor.apply_chat_template(
    messages, tokenize=False, add_generation_prompt=True
)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)
inputs = inputs.to(model.device)

# Inference: Generation of the output
generated_ids = model.generate(**inputs, max_new_tokens=128)
generated_ids_trimmed = [
    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
]
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)
print(output_text)
```

é€ä¸€äº†è§£ä¸€ä¸‹QwenVL2.5æ¨¡å‹çš„æ•´ä¸ªå¤„ç†è¿‡ç¨‹ï¼Œæ¨¡å‹æ•´ä½“è¿‡ç¨‹å¤§è‡´ä¸ºï¼š1ã€é¦–å…ˆæ˜¯é€šè¿‡æ¨¡æ¿åŒ–å¤„ç†æˆ‘çš„æ¨¡å‹çš„è¾“å…¥ï¼ˆimage+textï¼‰ï¼›2ã€å°†è¾“å…¥è½¬åŒ–ä¸ºç¼–ç å½¢å¼ï¼ˆæ¯”å¦‚æ–‡æœ¬tokenizerå¤„ç†ç­‰ï¼‰ï¼›3ã€å‡ºå…¥æ¨¡å‹å¤„ç†è¾“å…¥ç„¶åæ¨¡å‹è¾“å‡ºï¼›4ã€è§£ç è¾“å‡ºå†…å®¹ã€‚æ•´ä½“ä¸»è¦æ˜¯ä¸Šè¿°4ä¸ªè¿‡ç¨‹ï¼Œå› æ­¤ä¸‹é¢é€ä¸€äº†è§£ä¸€ä¸‹æ¨¡å‹åˆ°åº•åœ¨åšä»€ä¹ˆã€‚
## QwenVLçš„åŸºæœ¬ä½¿ç”¨
### 1ã€æ¨¡æ¿åŒ–æ¨¡å‹è¾“å…¥
```python
messages = [
    {
        "role": "user",
        "content": [
            {
                "type": "image",
                "image": "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg",
            },
            {"type": "text", "text": "Describe this image."},
        ],
    }
]

text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
```
æ‰€è°“æ¨¡æ¿åŒ–æ¨¡å‹çš„è¾“å…¥ï¼Œå¾ˆå®¹æ˜“ç†è§£ï¼ˆé€šè¿‡`processor.apply_chat_template`æŠŠ**å¯¹è¯ messages è½¬æˆæ¨¡å‹èƒ½ç†è§£çš„ prompt**ï¼Œä¸è¿‡å€¼å¾—æ³¨æ„çš„æ˜¯ä¸åŒæ¨¡å‹å¯èƒ½å¤„ç†çš„æ–¹å¼ä¸åŒï¼‰ï¼Œå°±æ˜¯å°†æˆ‘çš„å†…å®¹â€œå¡«å……â€åˆ°æ¨¡æ¿ä¸­æ¨¡æ‹Ÿå¯¹è¯å†…å®¹ï¼Œæ¯”å¦‚è¯´ä¸Šé¢å¤„ç†å¾—åˆ°çš„ä¸€ä¸ªç®€å•ç»“æœå°±æ˜¯ï¼š
```python
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>
<|im_start|>assistant
```
ä¸€èˆ¬åœ¨**data_loaderé‡Œé¢å°±ä¼šæå‰å°†æˆ‘ä»¬çš„æ¨¡å‹éœ€è¦çš„è¾“å…¥å¤„ç†å¥½**ï¼Œæ¯”å¦‚è¯´æˆ‘ä»¬å®šä¹‰å¦‚ä¸‹çš„æ¨¡æ¿
```python
def format_data(self, image, text, prompt):
    # self.SYSTEM_MESSAGE = """You are a helpful assistant."""
    return [
        {
            "role": "system",
            "content": [{"type": "text", "text": self.SYSTEM_MESSAGE}],
        },
        {
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "image": image,
                },
                {
                    "type": "text",
                    "text": prompt
                },
            ],
        },
        {
            "role": "assistant",
            "content": [{"type": "text", "text": text}],
        },
    ]
""" 
<|im_start|>system
You are a helpful assistant.<|im_end|>
<|im_start|>user
<|vision_start|><|image_pad|><|vision_end|>This is a prompt<|im_end|>
<|im_start|>assistant
This is a text<|im_end|>
<|im_start|>assistant
"""

```
å¯¹äºä¸Šé¢å†…å®¹è¾“å‡ºç†è§£ï¼Œé¦–å…ˆ `<|im_start|>....<|im_end|>`ä¸€èˆ¬æ˜¯ä¸€ç»„â€œå‘è¨€â€çš„å¼€å§‹å’Œç»“æŸæ ‡è®°ï¼Œè€Œåé‡Œé¢å†…å®¹å°±æ˜¯æˆ‘ä»¬çš„æ–‡æœ¬/å›¾åƒå†…å®¹ï¼Œ`user`/ `assistant`/ `system` åˆ™æ˜¯åˆ†åˆ«ä»£è¡¨ï¼šç”¨æˆ·ã€æ¨¡å‹ã€è§’è‰²ï¼ˆå‘Šè¯‰æ¨¡å‹ä»Šå¤©æ‰®ä»€ä¹ˆè§’è‰²ï¼‰ã€‚`<|vision_start|>...<|vision_end|>`ï¼šè¡¨ç¤ºå›¾åƒè¾“å…¥çš„å ä½ç¬¦ï¼Œå‘Šè¯‰æ¨¡å‹è¿™é‡Œæœ‰ä¸€æ®µè§†è§‰ä¿¡æ¯ã€‚`<|image_pad|>`ï¼šå›¾åƒå®é™…çš„ embedding ä¼šåœ¨è¿™é‡Œæ›¿æ¢ï¼ˆå¡«å……ï¼‰ï¼Œä¸æ˜¯æ–‡å­—ï¼Œè€Œæ˜¯å›¾åƒç¼–ç åçš„å‘é‡ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ `assistant`åé¢çš„å†…å®¹å°±æ˜¯ **æ¨¡å‹éœ€è¦è¾“å‡ºçš„æ–‡æœ¬å†…å®¹**ã€‚ä¸Šé¢è¿‡ç¨‹å¾ˆå®¹æ˜“ç†è§£ï¼Œåªä¸è¿‡éœ€è¦æ³¨æ„å¦‚ä¸‹é—®é¢˜ï¼Œå› ä¸ºQwenVL2.5å¯¹äºåˆ†è¾¨ç‡æ˜¯å­˜åœ¨å¤„ç†ï¼ˆä¸€èˆ¬ç›´æ¥é€šè¿‡`smart_resize`å¤„ç†ï¼Œåç»­æœ‰ä»‹ç»ï¼‰ï¼Œå› æ­¤å¦‚æœæ¶‰åŠåˆ°ç›®æ ‡è¯†åˆ«ï¼Œå¯èƒ½éœ€è¦æå‰å°†åæ ‡è¿›è¡Œè½¬æ¢é¿å…åˆ†è¾¨ç‡ä¸åŒå¯¼è‡´bboxå¯¹åº”ä¸ä¸Šçš„é—®é¢˜

### 2ã€ç¼–ç æ¨¡æ¿è¾“å…¥
```python
image_inputs, video_inputs = process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)
```
ç¼–ç æ¨¡æ¿è¾“å…¥å°±æ¯”è¾ƒç®€å•ï¼Œå› ä¸ºæˆ‘çš„è¾“å…¥éƒ½æ˜¯æ–‡æœ¬/å›¾ç‰‡ï¼Œæ­¤è¿‡ç¨‹å°±æ˜¯éœ€è¦å°†è¿™äº›å†…å®¹è½¬åŒ–ä¸ºç¼–ç å½¢å¼ï¼ˆæ¯”å¦‚tokenizerå¤„ç†ç­‰ï¼‰ï¼Œå¤„ç†æ–¹å¼å¦‚ä¸‹ï¼š
* 1ã€[process_vision_info](https://github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L352):è¿”å›æˆ‘çš„å›¾åƒ/è§†é¢‘è¾“å‡ºï¼ˆéƒ½å­˜å‚¨åœ¨listä¸­ï¼‰

é¦–å…ˆæ˜¯è¿‡[extract_vision_info](https://github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L334)ä»æˆ‘ä¸Šé¢çš„å†…å®¹ä¸­æå–å‡ºå›¾ç‰‡/è§†é¢‘ï¼ˆ`[{'type': 'image', 'image': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'}]`ï¼‰æå–å®Œæ¯•ä¹‹åå°±æ˜¯äº¤ç»™å¤„ç†å›¾ç‰‡/è§†é¢‘çš„å‡½æ•°è¿›è¡Œå¤„ç†
**å›¾ç‰‡å¤„ç†è¿‡ç¨‹**ï¼ˆ[`fetch_image`](https://github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L97)ï¼‰æ­¤è¿‡ç¨‹ä¹Ÿä¼šæ¯”è¾ƒç®€å•ï¼Œé¦–å…ˆå»åˆ¤æ–­ç±»å‹ï¼ˆæ˜¯`Image.Image`å¯¹è±¡/å›¾ç‰‡é“¾æ¥ç­‰ï¼‰ç„¶åæ‰“å¼€å›¾ç‰‡ï¼Œè€Œåå°±æ˜¯**ç¡®å®šå›¾ç‰‡åˆ†è¾¨ç‡å°ºå¯¸**ï¼Œæœ‰ä¸¤ç§`smart_resize`å¤„ç†æ–¹å¼ï¼Œç¬¬ä¸€ç§æ˜¯ç›´æ¥é€šè¿‡ï¼š`resized_height` å’Œ `resized_width`æ¥ç¡®å®šæ”¹å˜ï¼Œå¦å¤–ä¸€ç§ç›´æ¥é€šè¿‡ `min_pixels` å’Œ `max_pixels` æ¥å¤„ç†å›¾åƒå°ºå¯¸ã€‚å¯¹äº`smart_rezie`å‡½æ•°å¤„ç†è¿‡ç¨‹ä¸ºï¼š
```python
def smart_resize(
    height: int, width: int, factor: int = IMAGE_FACTOR, min_pixels: int = MIN_PIXELS, max_pixels: int = MAX_PIXELS)
    # IMAGE_FACTOR= 28
    if max(height, width) / min(height, width) > MAX_RATIO:
        ...
    h_bar = max(factor, round_by_factor(height, factor)) # round(number / factor) * factor
    w_bar = max(factor, round_by_factor(width, factor))
    if h_bar * w_bar > max_pixels:
        beta = math.sqrt((height * width) / max_pixels)
        h_bar = floor_by_factor(height / beta, factor) # æŒ‰æ¯”ä¾‹ç¼©å°å¹¶å‘ä¸‹å–æ•´  math.floor(number / factor) * factor
        w_bar = floor_by_factor(width / beta, factor)
    elif h_bar * w_bar < min_pixels:
        beta = math.sqrt(min_pixels / (height * width))
        h_bar = ceil_by_factor(height * beta, factor) # æŒ‰æ¯”ä¾‹æ”¾å¤§å¹¶å‘ä¸Šå–æ•´ math.ceil(number / factor) * factor
        w_bar = ceil_by_factor(width * beta, factor)
    return h_bar, w_bar
```
ä¸Šé¢3ä¸ªå°çš„å­å‡½æ•°è¡¨ç¤ºï¼šè®¡ç®—factorå€æ•°ã€å‘ä¸Šå–æ•´è®¡ç®—å€æ•°ã€å‘ä¸‹å–æ•´è®¡ç®—å€æ•°ï¼Œå¯¹äºsmart_resizeï¼ˆå»å®ç°åŠ¨æ€åˆ†è¾¨ç‡ï¼‰å‡½æ•°ï¼š**é€šè¿‡å››èˆäº”å…¥çš„æ–¹å¼ï¼Œé‡æ–°è®¾ç½®å›¾ç‰‡çš„ h å’Œ w å€¼ï¼Œç¡®ä¿å®ƒä»¬å¯ä»¥è¢«28æ•´é™¤**ï¼Œè¿™æ ·ä¸€æ¥å°±å¾—åˆ°äº†å›¾åƒçš„éœ€è¦ä¿®æ”¹çš„å°ºå¯¸äº†ï¼Œæ¯”å¦‚è¯´ï¼š
è¾“å…¥: ä¸€å¼  1000x500 çš„å›¾åƒ
è®¡ç®—åŸºç¡€å°ºå¯¸ï¼šround(1000/28)=36, round(500/28)=18 â†’ 1008x504
æ£€æŸ¥åƒç´ æ•°ï¼š1008*504 = 508,032 > MAX_PIXELS(200,704)
è®¡ç®—ç¼©æ”¾ç³»æ•°ï¼šbeta = sqrt(1000*500/200704) â‰ˆ 1.58
æœ€ç»ˆå°ºå¯¸ï¼šfloor(1000/1.58)=632, floor(500/1.58)=316 â†’ 616x308ï¼ˆ28çš„å€æ•°ï¼‰
**è§†é¢‘å¤„ç†è¿‡ç¨‹**ï¼ˆ[fetch_video](https://github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L277)ï¼‰å¯¹äºè§†é¢‘å¤„ç†å’Œå›¾åƒå¤„ç†ç›¸ç±»ä¼¼æ‰“å¼€-->æ”¹å˜å°ºå¯¸ã€‚åªä¸è¿‡åœ¨æ‰“å¼€è¿‡ç¨‹ä¸­QwenLV2.5å¤„ç†è¿‡ç¨‹ä¸ºï¼š
```python
def fetch_video(ele: dict, image_factor: int = IMAGE_FACTOR, return_video_sample_fps: bool = False):
    if isinstance(ele["video"], str):
        video_reader_backend = get_video_reader_backend()
        try:
            video, sample_fps = VIDEO_READER_BACKENDS[video_reader_backend](ele)
        except Exception as e:
            logger.warning(f"video_reader_backend {video_reader_backend} error, use torchvision as default, msg: {e}")
            video, sample_fps = VIDEO_READER_BACKENDS["torchvision"](ele)
    ...
```
å¯¹äº`VIDEO_READER_BACKENDS`è®¾è®¡äº†3ä¸­ä¸åŒèŒƒå¼ï¼š1ã€[_read_video_decord](https://github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L226)ï¼›2ã€[_read_video_torchvision](https://github.com/QwenLM/Qwen2.5-VL/blob/c15045f8829fee29d4b3996e068775fe6a5855db/qwen-vl-utils/src/qwen_vl_utils/vision_process.py#L183)ï¼›3ã€_read_video_torchcodecã€‚
* `_read_video_decord`

```python
def _read_video_decord(
    ele: dict,
) -> (torch.Tensor, float):
    """read video using decord.VideoReader

    Args:
        ele (dict): a dict contains the configuration of video.
        support keys:
            - video: the path of video. support "file://", "http://", "https://" and local path.
            - video_start: the start time of video.
            - video_end: the end time of video.
    Returns:
        torch.Tensor: the video tensor with shape (T, C, H, W).
    """
    import decord
    video_path = ele["video"]
    st = time.time()
    vr = decord.VideoReader(video_path)
    total_frames, video_fps = len(vr), vr.get_avg_fps()
    start_frame, end_frame, total_frames = calculate_video_frame_range(
        ele,
        total_frames,
        video_fps,
    ) # å¾—åˆ°è§†é¢‘çš„å¼€å§‹ ç»“æŸ æ€»ç»“å¤šå°‘å¸§
    nframes = smart_nframes(ele, total_frames=total_frames, video_fps=video_fps)
    idx = torch.linspace(start_frame, end_frame, nframes).round().long().tolist()
    video = vr.get_batch(idx).asnumpy()
    video = torch.tensor(video).permute(0, 3, 1, 2)  # Convert to TCHW format
    ...
    sample_fps = nframes / max(total_frames, 1e-6) * video_fps
    return video, sample_fps
```
å¯¹äºå…¶ä¸­çš„ `calculate_video_frame_range`å‡½æ•°å¤„ç†è¿‡ç¨‹ä¹Ÿå¾ˆç®€å•ï¼ˆç›´æ¥å»è®¡ç®—è§†é¢‘å¼€å§‹ã€ç»“æŸã€æ€»å…±å¤šå°‘å¸§ï¼‰ï¼Œè€Œåç±»ä¼¼åŠ¨æ€åˆ†è¾¨ç‡ï¼ˆsmart_resizeä¸­æˆç«‹ç›¸ç±»ä¼¼çš„ï¼‰å¯¹äºè§†é¢‘ä¼šé€šè¿‡æ™ºèƒ½è§†é¢‘å¸§æ•°è®¡ç®—ç®—æ³•ï¼ˆsmart_nframesï¼‰ï¼Œç”¨äº**ç¡®å®šä»è§†é¢‘ä¸­æå–å¤šå°‘å¸§ä½œä¸ºæ¨¡å‹è¾“å…¥**ï¼Œå¤„ç†è¿‡ç¨‹ä¸ºï¼šç¬¬ä¸€ç§ç›´æ¥é€šè¿‡`round_by_factor(ele["nframes"], FRAME_FACTOR)`æ¥å¾—åˆ°å¸§æ•°ï¼›ç¬¬äºŒç§å¤„ç†æ–¹å¼ä¸ºï¼ˆFPS_MIN_FRAMES = 4ã€FRAME_FACTOR = 2ã€FPS_MAX_FRAMES = 768ã€FPS = 2.0ï¼‰ï¼š
```python
fps = ele.get("fps", FPS)
min_frames = ceil_by_factor(ele.get("min_frames", FPS_MIN_FRAMES), FRAME_FACTOR)
max_frames = floor_by_factor(ele.get("max_frames", min(FPS_MAX_FRAMES, total_frames)), FRAME_FACTOR)
nframes = total_frames / video_fps * fps
nframes = min(min(max(nframes, min_frames), max_frames), total_frames)
nframes = floor_by_factor(nframes, FRAME_FACTOR)

"""
config = {"nframes": 24}
result = smart_nframes(config, total_frames=100, video_fps=30)
# è¾“å‡ºï¼š24ï¼ˆç›´æ¥ä½¿ç”¨é…ç½®å€¼ï¼‰

config = {"fps": 10, "min_frames": 16, "max_frames": 32}
result = smart_nframes(config, total_frames=100, video_fps=30)
# è®¡ç®—ï¼š100/30*10 â‰ˆ 33.33 â†’ çº¦æŸåˆ°32 â†’ å¯¹é½åˆ°32ï¼ˆFRAME_FACTOR=8çš„å€æ•°ï¼‰
"""
```
* 2ã€[processor](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py#L48)ï¼šå»å°†å›¾ç‰‡/æ–‡æœ¬è¿›è¡Œç¼–ç 

å…¶ä¸­å¯¹äºæ–‡æœ¬ç¼–ç ç›´æ¥é€šè¿‡ `self.tokenizer` æ¥å¤„ç†ï¼Œè€Œå¯¹äºå›¾åƒç›´æ¥é€šè¿‡ `self.image_processor`æ¥å¤„ç†ã€‚é¦–å…ˆåœ¨ [ä»£ç ](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py#L48)ä¸­å¾ˆå®¹æ˜“çœ‹åˆ°ä½¿ç”¨çš„å›¾åƒ/æ–‡æœ¬å¤„ç†æ–¹å¼`image_processor_class = "AutoImageProcessor"` å¯¹äºæ–‡æœ¬å¤„ç†æ–¹å¼ `tokenizer_class = ("Qwen2Tokenizer", "Qwen2TokenizerFast")`ã€‚
å¯¹äº**å›¾ç‰‡å¤„ç†æ–¹å¼**çš„ `Qwen2VLImageProcessor`ï¼ˆ[ä»£ç ](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py#L87)ï¼‰çš„å¤„ç†æ€è·¯ï¼š
```python
class Qwen2VLImageProcessor(BaseImageProcessor):
    def __init(...):
        ...
    def _preprocess(self, images, ...):
        ...
        height, width = get_image_size(images[0], channel_dim=input_data_format)
        resized_height, resized_width = height, width
        processed_images = []
        # Step-1
        for image in images:
            if do_resize:
                resized_height, resized_width = smart_resize(
                    height,
                    width,
                    factor=self.patch_size * self.merge_size,
                    min_pixels=self.min_pixels,
                    max_pixels=self.max_pixels,
                )
                image = resize(
                    image, size=(resized_height, resized_width), resample=resample, input_data_format=input_data_format
                )
            if do_rescale:
                image = self.rescale(image,...)
            if do_normalize:
                image = self.normalize(image,...)
        # Step-2
        patches = np.array(processed_images)
        if data_format == ChannelDimension.LAST:
            patches = patches.transpose(0, 3, 1, 2)
        if patches.shape[0] % self.temporal_patch_size != 0:
            # è§†é¢‘è¡¥å¸§å¤„ç†
            repeats = np.repeat(patches[-1][np.newaxis], self.temporal_patch_size - 1, axis=0)
            patches = np.concatenate([patches, repeats], axis=0)
        # è®¡ç®—ä¸åŒ patch ç½‘æ ¼å¤§å°
        channel = patches.shape[1]
        grid_t = patches.shape[0] // self.temporal_patch_size
        grid_h, grid_w = resized_height // self.patch_size, resized_width // self.patch_size

        patches = patches.reshape(
            grid_t,
            self.temporal_patch_size,
            channel,
            grid_h // self.merge_size,
            self.merge_size,
            self.patch_size,
            grid_w // self.merge_size,
            self.merge_size,
            self.patch_size,
        )
        patches = patches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)
        flatten_patches = patches.reshape(
            grid_t * grid_h * grid_w, channel * self.temporal_patch_size * self.patch_size * self.patch_size
        )

        return flatten_patches, (grid_t, grid_h, grid_w)
```
å¯¹äºä¸Šé¢å¤„ç†è¿‡ç¨‹ä¸­ï¼Œ**é¦–å…ˆ**å¯¹äº `_preprocess`ä¸»è¦æ˜¯å¯¹å›¾åƒè¿›è¡Œä¸€äº›é¢„å¤„ç†ï¼š1ã€do_resizeï¼šæ”¹å˜å›¾ç‰‡å¤§å°ï¼ˆç›´æ¥é€šè¿‡`smrt_resize`è¿›è¡Œå¤„ç†ï¼‰2ã€do_rescaleï¼šåƒç´ ç¼©å‡åˆ°0-1ä¹‹é—´ï¼›3ã€do_normalizeï¼šå¯¹å›¾ç‰‡è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼ˆé€šé“ç»´åº¦ï¼‰ï¼›**è€Œå**ç›´æ¥å¯¹äºé¢„å¤„ç†åçš„å›¾åƒç›´æ¥è¿›è¡Œåˆ‡å‰²å¤„ç†ä¸ºä¸åŒçš„patchè¾“å…¥åˆ°Vitä¸­ï¼Œæ¯”å¦‚å‡è®¾æˆ‘ä»¬çš„ `patches=(1,3,1024,1024)` é‚£ä¹ˆé¦–å…ˆè®¡ç®—ä¸åŒ patchç½‘æ ¼å¤§å°ï¼ˆtemporal_patch_size=2ï¼Œpatch_size=16ï¼Œmerge_size=2ï¼Œresized_height=1024 å€¼å¾—æ³¨æ„çš„æ˜¯temporal_patch_size=2ä¼šå°†å›¾ç‰‡å¤„ç†ä¸º 2 3 1024 1024ï¼‰é‚£ä¹ˆè®¡ç®—å¾—åˆ°ç½‘ç»œpatchå¤§å°ä¸ºï¼ˆgrid_h= grid_w= 64ï¼‰ï¼š1x64x64ï¼Œè€Œååˆ†åˆ«å°†ä¸åŒç»´åº¦ä¿¡æ¯ï¼ˆt h wï¼‰è¿›è¡Œåˆ’åˆ†ï¼Œä¹Ÿå°±æ˜¯å°† ï¼ˆ2ï¼Œ3ï¼Œ1024ï¼Œ1024ï¼‰-->ï¼ˆ1ï¼Œ2ï¼Œ3ï¼Œ32ï¼ˆ64//2ï¼‰ï¼Œ2ï¼Œ16ï¼Œ32ï¼ˆ64//2ï¼‰ï¼Œ2ï¼Œ16ï¼‰æœ€åå†å»äº¤æ¢ç»´åº¦å¹¶ä¸”è¿›è¡Œåˆå¹¶å³å¯ã€‚
**å›é¡¾ä¸€ä¸‹QwenVL2.5çš„å›¾ç‰‡å¤„ç†è¿‡ç¨‹**ï¼šé¦–å…ˆæ˜¯å»å¯¹å›¾ç‰‡è¿›è¡Œæ”¹å˜å°ºå¯¸ï¼ˆä¿è¯å›¾ç‰‡æœ€åå¯ä»¥æ•´é™¤patch_sizeï¼‰/ç¼©æ”¾/å½’ä¸€åŒ–ã€‚è€Œåå°±æ˜¯ç›´æ¥å°†å›¾ç‰‡å¤„ç†ä¸ºvitèƒ½å¤Ÿå¤„ç†çš„â€œåºåˆ—è¾“å…¥â€å¾—åˆ°çš„ç»´åº¦ä¸ºï¼š`[grid_t * grid_h * grid_w, channel * temporal_patch_size(2) * patch_size(14) * patch_size(14)]`ã€‚
> **è¡¥å……ä¸€**ï¼šå›¾ç‰‡è¾“å…¥å…·ä½“ä¾‹å­è¯´æ˜
> å‡è®¾é»˜è®¤å‚æ•°ä¸ºï¼špatch_size= 14, temporal_patch_size= 2, merge_size= 2
> å›¾åƒè¾“å…¥ä¸ºï¼ˆé€šè¿‡process_vision_infoæå‰å¤„ç†ä¹‹åçš„ç»´åº¦ï¼‰ï¼š(1092, 1568) 
> é¦–å…ˆè®¡ç®— `resized_height, resized_width = smart_resize`å¾—åˆ° 812 1176
> é¦–å…ˆè®¡ç®—ï¼šgrid_t=1ï¼Œgrit_h=812//14=58ï¼Œgrid_w=1176//14=84é‚£ä¹ˆè®¡ç®—å¾—åˆ°ä¸º 4872å¦å¤–ä¸€é¡¹ä¸º 1176ä¹Ÿå°±æ˜¯æœ€åå›¾åƒå¤„ç†å¾—åˆ°çš„è¾“å‡ºä¸ºï¼š`(1*58*84, 14*14*2*3)=(4872,1176)`
> **è¡¥å……äºŒ**ï¼šå¯¹äº smart_resizeå¿«é€Ÿä¼°ç®—æœ€åå¤§å°ï¼š
> å…ˆ round åˆ° factor çš„å€æ•°
> å¦‚æœè¶…å‡º max_pixels â†’ é™¤ä»¥ sqrt(HW/max_pixels)ï¼Œfloor â†’ factor å€æ•°
> å¦‚æœå°äº min_pixels â†’ ä¹˜ä»¥ sqrt(min_pixels/HW)ï¼Œceil â†’ factor å€æ•°
> å…¶å®ä¹Ÿå°±æ˜¯ï¼š**é¦–å…ˆå°†å›¾åƒå¤„ç†åˆ°ä¸ºfactorå€æ•°çš„åˆ†è¾¨ç‡ï¼Œè€Œåå»åˆ¤æ–­å’Œmax_pixelså’Œmin_pixelsä¹‹é—´å¤§å°ï¼Œå¤§äºå‰è€…å°±ç¼©å°ï¼Œå°äºå‰è€…å°±æ”¾å¤§**

æœ€åé€šè¿‡ä¸€ç³»åˆ—ç¼–ç ä¹‹åå¾—åˆ°è¾“å‡ºï¼š
```python
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt",
)
"""
input_ids: torch.Size([1, 1243])
attention_mask: torch.Size([1, 1243])
pixel_values: torch.Size([4872, 1176])
image_grid_thw: torch.Size([1, 3])
"""
```

### 3ã€æ¨¡å‹è¾“å…¥å¤„ç†
```python
generated_ids = model.generate(**inputs, max_new_tokens=128)
```
æ•´ä½“[æ¨¡å‹](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L1724)è¾“å…¥å¤„ç†ï¼Œè¾“å…¥æ¨¡å‹ä¹Ÿå°±æ˜¯ä¸Šé¢ç¼–ç æ¨¡æ¿è¾“å…¥å‡ ä¸ªéƒ¨åˆ†ï¼Œåªä¸è¿‡ä¸»è¦å°±æ˜¯å¦‚ä¸‹å‡ ä¸ªå¤„ç†ï¼šé¦–å…ˆæ˜¯æ¨¡å‹å¤„ç†è¾“å…¥ `input_ids` ä»¥åŠæˆ‘çš„å›¾åƒ `pixel_values`ï¼ˆ`inputs_embeds = self.model.embed_tokens(input_ids)` [ä»£ç ](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L1790)ï¼‰ï¼Œè€Œåå°†è¾“å…¥è¿›è¡Œä½ç½®ç¼–ç å¤„ç†ï¼ˆ[ä»£ç ](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L1838)ï¼‰ï¼Œæœ€åè¾“å‡ºæ¨¡å‹ç»“æœï¼ˆ[ä»£ç ](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L1861)ï¼‰ï¼Œå¯¹äºQwenVL2.5å®Œæ•´æ¨¡å‹ç»“æ„ï¼š
```python
Qwen2_5_VLForConditionalGeneration(
  (model): Qwen2_5_VLModel(
    (visual): Qwen2_5_VisionTransformerPretrainedModel(
      (patch_embed): Qwen2_5_VisionPatchEmbed(
        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
      )
      (rotary_pos_emb): Qwen2_5_VisionRotaryEmbedding()
      (blocks): ModuleList(
        (0-31): 32 x Qwen2_5_VLVisionBlock(
          (norm1): Qwen2RMSNorm((1280,), eps=1e-06)
          (norm2): Qwen2RMSNorm((1280,), eps=1e-06)
          (attn): Qwen2_5_VLVisionAttention(
            (qkv): Linear(in_features=1280, out_features=3840, bias=True)
            (proj): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (mlp): Qwen2_5_VLMLP(
            (gate_proj): Linear(in_features=1280, out_features=3420, bias=True)
            (up_proj): Linear(in_features=1280, out_features=3420, bias=True)
            (down_proj): Linear(in_features=3420, out_features=1280, bias=True)
            (act_fn): SiLU()
          )
        )
      )
      (merger): Qwen2_5_VLPatchMerger(
        (ln_q): Qwen2RMSNorm((1280,), eps=1e-06)
        (mlp): Sequential(
          (0): Linear(in_features=5120, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=5120, out_features=2048, bias=True)
        )
      )
    )
    (language_model): Qwen2_5_VLTextModel(
      (embed_tokens): Embedding(151936, 2048)
      (layers): ModuleList(
        (0-35): 36 x Qwen2_5_VLDecoderLayer(
          (self_attn): Qwen2_5_VLAttention(
            (q_proj): Linear(in_features=2048, out_features=2048, bias=True)
            (k_proj): Linear(in_features=2048, out_features=256, bias=True)
            (v_proj): Linear(in_features=2048, out_features=256, bias=True)
            (o_proj): Linear(in_features=2048, out_features=2048, bias=False)
            (rotary_emb): Qwen2_5_VLRotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=2048, out_features=11008, bias=False)
            (up_proj): Linear(in_features=2048, out_features=11008, bias=False)
            (down_proj): Linear(in_features=11008, out_features=2048, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((2048,), eps=1e-06)
      (rotary_emb): Qwen2_5_VLRotaryEmbedding()
    )
  )
  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)
)
```
* **é¦–å…ˆ**ï¼šå¯¹äºè§†è§‰éƒ¨åˆ†å¤„ç†ï¼ˆ`Qwen2_5_VisionTransformerPretrainedModel`ï¼‰

> å¯¹äºè§†è§‰æ¨¡å‹ä¸»è¦éœ€è¦å¤„ç†çš„å°±æ˜¯ `pixel_values`ï¼Œå‡è®¾è¾“å…¥çš„ `pixel_values`ä¿¡æ¯ä¸ºï¼š`[4872, 1176]`ï¼Œimage_grid_thwä¸ºï¼š [1, 84, 58]ï¼ˆå°±æ˜¯å¯¹åº”grid_tã€grid_hã€grid_wè¿™ä¸‰ä¸ªæ•°å€¼ï¼‰

ä¸»è¦åŒ…æ‹¬å¦‚ä¸‹å‡ ä¸ªæ¨¡å—ï¼š
1ã€[Qwen2_5_VisionPatchEmbed](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L88)ï¼šä¸»è¦è¿›è¡Œå¤„ç†é€šè¿‡ä¸€ä¸ª `Conv3d`å¤„ç†ï¼Œå¤„ç†[è¿‡ç¨‹](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L105C4-L111C29)ä¹Ÿå°±æ˜¯è¯´é¦–å…ˆå°†è¾“å…¥çš„ç»´åº¦è¿›è¡Œä¿®æ”¹å¾—åˆ°ï¼š`view(-1, self.in_channels, self.temporal_patch_size, self.patch_size, self.patch_size)` --> (4872,1176)-->(4872,3,2,14,14)è€Œåå†å»é€šè¿‡å·ç§¯å¤„ç†å¾—åˆ° (4872,1280,1,1,1)æœ€åå¾—åˆ°ï¼š**(4872,1280)**ï¼Œä¹Ÿå°±å¯¹åº”ç€ï¼š`(grid_t*grid_h*grid_w, hiddend_size)`ï¼›
2ã€Qwen2_5_VisionRotaryEmbeddingï¼›
3ã€[Qwen2_5_VLVisionAttention](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L233)ï¼šé¦–å…ˆå»åˆ’åˆ†[window_size](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L465)è¿™ä¸€æ­¥ç›´æ¥æ ¹æ®è®¡ç®—å¾—åˆ°çš„ï¼š`[grid_t, grid_h, grid_w]`å»åˆ’åˆ†windowsï¼Œæ¯”å¦‚è¯´åœ¨ä¸Šè¿°ä¾‹å­ä¸­ï¼Œå¾—åˆ°çš„cu_seqlens = [0,64,128,...,4872]ï¼Œè€Œåå†å»é€šè¿‡å¦‚ä¸‹å¤„ç†ï¼š
```python
lengths = cu_seqlens[1:] - cu_seqlens[:-1]
splits = [
    torch.split(tensor, lengths.tolist(), dim=2) for tensor in (query_states, key_states, value_states)
]
```
å»åˆ’åˆ†qã€kã€vï¼ˆå½¢çŠ¶éƒ½ä¸ºï¼š[1, 16, 4872, 80]ï¼‰ç„¶åè®¡ç®—æ³¨æ„åŠ›ï¼Œè€Œåé€šè¿‡[Qwen2_5_VLPatchMerger](https://github.com/huggingface/transformers/blob/41925e42135257361b7f02aa20e3bbdab3f7b923/src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py#L146)å°†ç»“æœåˆå¹¶èµ·æ¥ã€‚
**å…·ä½“è®¡ç®—è¿‡ç¨‹**ï¼Œé¦–å…ˆæ˜¯å¦‚ä½•å¾—åˆ°cu_seqlensï¼Œå› ä¸ºæˆ‘ä»¬å¾—åˆ°çš„gird_thw=(1, 84, 58)ä¹Ÿå°±æ˜¯è¯´æ€»å…±æœ‰84*58=4872ä¸ªtokenå»è®¡ç®—å…¨å±€æ³¨æ„åŠ›ï¼Œé‚£ä¹ˆè¿™å°±ä¼šå¯¼è‡´è®¡ç®—æ³¨æ„åŠ›çš„æ¶ˆè€—è¿‡å¤§ï¼Œå› æ­¤å¯ä»¥å…ˆå»åˆ‡åˆ†æˆå°çš„windowç„¶åå°å—å†…éƒ¨æ³¨æ„åŠ›è®¡ç®—ã€‚å› æ­¤é¦–å…ˆè®¡ç®—â€œå—â€çš„å¤§å°ï¼š`vit_merger_window_size = self.window_size // self.spatial_merge_size // self.patch_size`å¾—åˆ°ç»“æœä¸º: 4ï¼ˆ112/2/14ï¼‰ä¹Ÿå°±æ˜¯è¯´æ¯å—å¤§å°ä¸ºï¼š4x4=16ï¼Œä½†æ˜¯ä¸ä¸€å®šæˆ‘çš„grid_hå’Œgrid_wå¯èƒ½æ•´é™¤4ï¼Œå› æ­¤å°±éœ€è¦å»è®¡ç®—å¡«å……æ•°é‡ `vit_merger_window_size - llm_grid_h % vit_merger_window_size` åˆ†åˆ«å¾—åˆ° 4å’Œ2å› æ­¤å¡«å……åçš„hå’Œwä¸ºï¼š88,60è¿™æ ·ä¸€æ¥è®¡ç®—å¾—åˆ°windowæ•°é‡ä¸ºï¼š88//4 * 60//4=330æ¯ä¸ªçª—å£çš„tokensæ•°é‡ï¼š16

### 4ã€å›¾åƒå¤„ç†è¿‡ç¨‹æ€»ç»“
**æ€»ç»“ä¸Šè¿°å›¾åƒå¤„ç†è¿‡ç¨‹**ï¼šå¯¹äºä»»æ„è¾“å…¥å›¾åƒé¦–å…ˆé€šè¿‡smart_resizeï¼ˆé¦–å…ˆå°†å›¾åƒæ”¹å˜åˆ° factorçš„å€æ•°ï¼Œç„¶åå»åˆ¤æ–­å’Œmin_pixelså’Œmax_pixelsä¹‹é—´å¤§å°ï¼Œç„¶åè¿›è¡Œæ‰©å¤§ï¼Œç¼©å°ï¼‰è¿›è¡Œå¤„ç†ä¿è¯éƒ½å¯ä»¥æ•´é™¤patch_sizeï¼ˆ14ï¼‰ç„¶åä¸¢åˆ° `processor`ä¸­è¿›è¡Œå¤„ç†ä¸»è¦æ˜¯å¯¹å›¾åƒå½’ä¸€åŒ–ã€æ­£åˆ™åŒ–ã€æ”¹å˜ç»´åº¦ï¼ˆè¿˜ä¼šé€šè¿‡smart_resizeåœ¨å¤„ç†ä¸€æ¬¡ï¼‰ï¼Œå¤„ç†ä¹‹åå†å»ç¡®å®šä»–çš„ `grid_t, grid_h, grid_w`ï¼ˆå¯¹äºè¿™3ä¸ªå‚æ•°ç¡®å®šï¼šç›´æ¥é€šè¿‡ ç¬¬äºŒæ¬¡smart_resizeå¤„ç†ä¹‹åçš„ç»“æœé™¤ patch_sizeå³å¯ï¼‰ä¹Ÿå°±æ˜¯tokensæ•°é‡ï¼Œè€Œåå°†å›¾åƒå†…å®¹é€šè¿‡ conv3då¤„ç†å¾—åˆ°ï¼š`(grid_t* grid_h* grid_w, hidden_size)`ï¼Œæœ€åå°±æ˜¯è®¡ç®—window_attentionï¼ˆé¦–å…ˆç¡®å®šwidow_sizeç´¢å¼•ï¼Œé€šè¿‡ç´¢å¼•è¿›è¡Œåˆ‡åˆ†ï¼Œæœ€åè®¡ç®—æ³¨æ„åŠ›ï¼‰
> è¡¥å……ï¼šå¯¹äºwindow-attentionå¯ä»¥ç”¨å·ç§¯çš„æ€è·¯å»ç†è§£ï¼Œæ¯”å¦‚è¯´æˆ‘å¾—åˆ°â€œå›¾åƒâ€ï¼š`(grid_t, grid_h, grid_w)` æˆ‘æå‰è®¡ç®—æˆ‘çš„â€œå·ç§¯æ ¸â€å¤§å°ï¼ˆ`vit_merger_window_size = self.window_size // self.spatial_merge_size // self.patch_size`ï¼‰ä¸ºäº†ä¿è¯æˆ‘çš„ â€œå›¾åƒâ€å¯ä»¥è¢«å·ç§¯æ ¸å¤„ç†å°±éœ€è¦åšä¸€éƒ¨åˆ†å¡«å……ï¼Œè€Œåç”¨è¿™ä¸ªâ€œå·ç§¯æ ¸â€å»åˆ’åˆ†æˆä¸åŒâ€œå°å—â€åœ¨åˆ°è¿™ä¸ªå°å—é‡Œé¢è®¡ç®—æ³¨æ„åŠ›ã€‚

### 5ã€ä½ç½®ç¼–ç 

## QwenVLçš„å¾®è°ƒè¿‡ç¨‹
### SFT å¤„ç†
https://www.f22labs.com/blogs/complete-guide-to-fine-tuning-qwen2-5-vl-model/

### DL å¤„ç†